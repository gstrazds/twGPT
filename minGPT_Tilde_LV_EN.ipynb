{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a08d5f",
   "metadata": {
    "id": "varying-atlas"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cab3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "#from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.pre_tokenizers import Punctuation\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, Strip, Replace, Sequence\n",
    "from tokenizers.trainers import UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0f22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c5abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILDE_DATA = './data/tilde'\n",
    "!mkdir -p $TILDE_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a2cb45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALC2-FQxnZ1l",
    "outputId": "3e4d0bbd-8b0d-4b9d-884a-eb88c0d3c556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sacrebleu\r\n",
      "Version: 1.5.1\r\n",
      "Summary: Hassle-free computation of shareable, comparable, and reproducible BLEU, chrF, and TER scores\r\n",
      "Home-page: https://github.com/mjpost/sacrebleu\r\n",
      "Author: Matt Post\r\n",
      "Author-email: post@cs.jhu.edu\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: portalocker\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install sacrebleu\n",
    "!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b0a5929",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A8WZUBGiyv7",
    "outputId": "cbe405e8-f665-4298-d4ae-44c533bf0711"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57b506",
   "metadata": {
    "id": "Ex11yfCVaueI"
   },
   "source": [
    "#Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d7f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE_ops=10000 vocab_size=5500 joint_vocab_size=11000\r\n"
     ]
    }
   ],
   "source": [
    "num_bpe_merges = 10000\n",
    "vocab_size = 5500\n",
    "joint_vocab_size = 2*vocab_size\n",
    "\n",
    "!echo BPE_ops=$num_bpe_merges vocab_size=$vocab_size joint_vocab_size=$joint_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fb5e4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTfvBWhIaHTh",
    "outputId": "e75b4122-7e6e-4abc-827a-134aec8cf165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: subword-nmt\r\n",
      "Version: 0.3.7\r\n",
      "Summary: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation\r\n",
      "Home-page: https://github.com/rsennrich/subword-nmt\r\n",
      "Author: Rico Sennrich\r\n",
      "Author-email: None\r\n",
      "License: MIT\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install subword-nmt\n",
    "!pip show subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bfbbe2",
   "metadata": {
    "id": "historic-thermal"
   },
   "outputs": [],
   "source": [
    "# # Read Hemingway texts from URL. There are Hemingway's \"A Farewell to arms\"\n",
    "# text_en = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_en_small.txt').read().decode(\"utf-8\", \"ignore\")\n",
    "# text_lv = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_lv_small.txt').read().decode(\"utf-8-sig\", \"ignore\")\n",
    "\n",
    "# HEMINGWAY_SRC_EN = f'{HEMINGWAY_DATA}/hemingway.en.txt'\n",
    "# HEMINGWAY_SRC_LV = f'{HEMINGWAY_DATA}/hemingway.lv.txt'\n",
    "\n",
    "# with open(HEMINGWAY_SRC_EN, 'w') as f:\n",
    "#     f.write(text_en)\n",
    "\n",
    "# with open(HEMINGWAY_SRC_LV, 'w') as f:\n",
    "#     f.write(text_lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tilde/combined.lv.tok.txt ./data/tilde/combined.en.tok.txt\r\n"
     ]
    }
   ],
   "source": [
    "TILDE_ALL_EN = f'{TILDE_DATA}/all.norm2.en'\n",
    "TILDE_ALL_LV = f'{TILDE_DATA}/all.norm2.lv'\n",
    "\n",
    "TILDE_TOK_EN = f'{TILDE_DATA}/combined.en.tok.txt'\n",
    "TILDE_TOK_LV = f'{TILDE_DATA}/combined.lv.tok.txt'\n",
    "\n",
    "!echo $TILDE_DATA/combined.lv.tok.txt $TILDE_TOK_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9bf37b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzgEuTRkV4kK",
    "outputId": "6ac1bf6e-9a2a-4cf7-c6cc-e3c76ec72522"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/moses-smt/mosesdecoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272baa8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_qVpRZMGJDl",
    "outputId": "d51d2bb1-1337-4e36-eb1f-28f67fdfbd29"
   },
   "outputs": [],
   "source": [
    "# Normalize and tokenize texts\n",
    "cmd1 = f\"cat {TILDE_ALL_EN} | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en > {TILDE_TOK_EN}\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"cat {TILDE_ALL_LV} | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l lv | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l lv > {TILDE_TOK_LV}\"\n",
    "#! $cmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857c8530",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_qVpRZMGJDl",
    "outputId": "d51d2bb1-1337-4e36-eb1f-28f67fdfbd29"
   },
   "outputs": [],
   "source": [
    "# # Normalize and tokenize texts\n",
    "\n",
    "# #!cat hemingway.en.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en \\\n",
    "# !cat hemingway.en.txt \\\n",
    "#   | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en > hemingway.en.tok.txt\n",
    "\n",
    "# # !cat hemingway.lv.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l lv \\\n",
    "# !cat hemingway.lv.txt \\\n",
    "#   | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l lv > hemingway.lv.tok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf6237fb",
   "metadata": {
    "id": "YlDFOYvOIb2c"
   },
   "outputs": [],
   "source": [
    "#!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus $TILDE_DATA/combined.en.tok.txt -model $TILDE_DATA/tc_model.en\n",
    "#!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus $TILDE_DATA/combined.lv.tok.txt -model $TILDE_DATA/tc_model.lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c574a62a",
   "metadata": {
    "id": "YlDFOYvOIb2c"
   },
   "outputs": [],
   "source": [
    "cmd1 = f\"mosesdecoder/scripts/recaser/truecase.perl -model {TILDE_DATA}/tc_model.en < {TILDE_DATA}/combined.en.tok.txt > {TILDE_DATA}/combined.en.tc.txt\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"mosesdecoder/scripts/recaser/truecase.perl -model {TILDE_DATA}/tc_model.lv < {TILDE_DATA}/combined.lv.tok.txt > {TILDE_DATA}/combined.lv.tc.txt\"\n",
    "#! $cmd2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33ce120e",
   "metadata": {
    "id": "aiMbyzugbFZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword-nmt learn-joint-bpe-and-vocab --input ./data/tilde/combined.lv.tc.txt ./data/tilde/combined.en.tc.txt -s 10000 -o ./data/tilde/bpe/tokens.lven --write-vocabulary ./data/tilde/bpe/token_freq.en ./data/tilde/bpe/token_freq.lv\r\n"
     ]
    }
   ],
   "source": [
    "# !subword-nmt learn-joint-bpe-and-vocab --input en.tc.txt lv.tc.txt -s 10000 -o tokens.txt --write-vocabulary token_freq.en.txt token_freq.lv.txt\n",
    "!mkdir -p $TILDE_DATA/bpe \n",
    "\n",
    "!echo subword-nmt learn-joint-bpe-and-vocab --input $TILDE_DATA/combined.lv.tc.txt $TILDE_DATA/combined.en.tc.txt -s $num_bpe_merges -o $TILDE_DATA/bpe/tokens.lven --write-vocabulary $TILDE_DATA/bpe/token_freq.en $TILDE_DATA/bpe/token_freq.lv\n",
    "\n",
    "## !subword-nmt learn-joint-bpe-and-vocab --input $HEMINGWAY_DATA/hemingway.en.tc.txt -s $num_bpe_merges -o $HEMINGWAY_DATA/bpe/tokens.en --write-vocabulary $HEMINGWAY_DATA/bpe/token_freq.en\n",
    "## !subword-nmt learn-joint-bpe-and-vocab --input $HEMINGWAY_DATA/hemingway.lv.tc.txt -s $num_bpe_merges -o $HEMINGWAY_DATA/bpe/tokens.lv --write-vocabulary $HEMINGWAY_DATA/bpe/token_freq.lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8c9cc8",
   "metadata": {
    "id": "30_qUBkgSNG1"
   },
   "outputs": [],
   "source": [
    "def build_vocab(freq_file, vocab_size):\n",
    "    vocab = Counter(['<unk>', '<pad>', '<eos>'])\n",
    "    with open(freq_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            token, num_occurs = line.split()\n",
    "            # vocab.append(token)\n",
    "            vocab[token] += int(num_occurs)\n",
    "    return vocab #[:vocab_size]\n",
    "#     return vocab[:vocab_size]\n",
    "\n",
    "en_vocab = build_vocab(f'{TILDE_DATA}/bpe/token_freq.en', vocab_size)\n",
    "lv_vocab = build_vocab(f'{TILDE_DATA}/bpe/token_freq.lv', vocab_size)\n",
    "\n",
    "joint_vocab = Counter(en_vocab)\n",
    "joint_vocab.update(lv_vocab)\n",
    "\n",
    "if False:\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.en', 'w') as f:\n",
    "        for i, token in enumerate(en_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {en_vocab[token]} \\n\")\n",
    "\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.lv', 'w') as f:\n",
    "        for i, token in enumerate(lv_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {lv_vocab[token]} \\n\")\n",
    "\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.lven', 'w') as f:\n",
    "        for i, token in enumerate(joint_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {joint_vocab[token]} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f080e114",
   "metadata": {
    "id": "CtzY4LjYbSiJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab: 10099 lv_vocab: 6477 joint_vocab 10519\n"
     ]
    }
   ],
   "source": [
    "print(\"en_vocab:\", len(en_vocab), \"lv_vocab:\", len(lv_vocab), \"joint_vocab\", len(joint_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df35a605",
   "metadata": {
    "id": "CtzY4LjYbSiJ"
   },
   "outputs": [],
   "source": [
    "#!subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.en --vocabulary $HEMINGWAY_DATA/bpe/vocab.en --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.en.tc.txt > $HEMINGWAY_DATA/hemingway.en.BPE.txt\n",
    "#!subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lv --vocabulary $HEMINGWAY_DATA/bpe/vocab.lv --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.lv.tc.txt > $HEMINGWAY_DATA/hemingway.lv.BPE.txt\n",
    "\n",
    "# !subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lven --vocabulary $HEMINGWAY_DATA/bpe/token_freq.en --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.en.tc.txt > $HEMINGWAY_DATA/hemingway.en.BPE.txt\n",
    "# !subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lven --vocabulary $HEMINGWAY_DATA/bpe/token_freq.lv --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.lv.tc.txt > $HEMINGWAY_DATA/hemingway.lv.BPE.txt\n",
    "\n",
    "\n",
    "cmd1 = f\"subword-nmt apply-bpe -c {TILDE_DATA}/bpe/tokens.lven --vocabulary {TILDE_DATA}/bpe/vocab.lven --vocabulary-threshold 1 < {TILDE_DATA}/combined.en.tc.txt > {TILDE_DATA}/combined.en.BPE.txt\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"subword-nmt apply-bpe -c {TILDE_DATA}/bpe/tokens.lven --vocabulary {TILDE_DATA}/bpe/vocab.lven --vocabulary-threshold 1 < {TILDE_DATA}/combined.lv.tc.txt > {TILDE_DATA}/combined.lv.BPE.txt\"\n",
    "#! $cmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "641723a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    special_tokens = ['<unk>', '<pad>', '<eos>', '<sep>'] #, '<S>', '</S>', '<bos>', '<eos>', '<sep>', '<NONE>', '<|>']\n",
    "                  \n",
    "    normalizer = normalizers.Sequence([Strip(), Lowercase()])\n",
    "    pre_tokenizer = Whitespace()\n",
    "\n",
    "    model = tokenizers.models.WordLevel(unk_token='<unk>')\n",
    "    # model = tokenizers.models.WordPiece()\n",
    "    tokenizer = tokenizers.Tokenizer(model=model)\n",
    "\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"test/*.pthru\"))\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"train/*.pthru\"))\n",
    "\n",
    "\n",
    "    # token_strs = [tok for (tok, span) in pre_tokenizer.pre_tokenize_str(str1)]\n",
    "    # print(token_strs)\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "\n",
    "    filelist = glob.glob(f\"{TILDE_DATA}/combined.*.BPE.txt\")\n",
    "\n",
    "    filelist = sorted(filelist)\n",
    "    print(len(filelist), filelist[:10])\n",
    "\n",
    "\n",
    "    # unigram_trainer = tokenizers.trainers.UnigramTrainer()\n",
    "    # trainer = tokenizers.trainers.WordPieceTrainer(vocab_size=vocab_size)\n",
    "    trainer = tokenizers.trainers.WordLevelTrainer(vocab_size=joint_vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer.train(files=filelist, trainer=trainer)\n",
    "\n",
    "    vocab_dict = tokenizer.get_vocab(with_added_tokens=False)\n",
    "    print(\"ACTUAL VOCAB SIZE =\", len(vocab_dict))\n",
    "    print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4e785a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! ACTUAL VOCAB SIZE = 900 (first try when joint_vocab but separate --vocabulary token_freq.lang)\n",
    "# ACTUAL VOCAB SIZE = 8637\n",
    "# ACTUAL VOCAB SIZE = 9048 (CUDA out of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17896d87",
   "metadata": {
    "id": "wcc2v_fHe81_"
   },
   "outputs": [],
   "source": [
    "with open(f'{TILDE_DATA}/combined.lv.BPE.txt', 'r') as f:\n",
    "    text_input = f.read()\n",
    "\n",
    "with open(f'{TILDE_DATA}/combined.en.BPE.txt', 'r') as f:\n",
    "    text_output = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a8a9f",
   "metadata": {
    "id": "PAHdOeNhYJ8v"
   },
   "source": [
    "#MinGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "360423f8",
   "metadata": {
    "id": "OcFt-zMBfqDt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def calculate_attention_token(attention, top_k, model):\n",
    "    logits = model.head(attention)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "    ix = torch.multinomial(probs, num_samples=top_k)\n",
    "\n",
    "    return ix[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None, output_attention=False):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    attention_state = [[] for _ in model.blocks]\n",
    "\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "        if output_attention:\n",
    "            b, t = x.size()\n",
    "\n",
    "            for block_id in range(len(model.blocks)):\n",
    "                att = model.blocks[block_id].attn.att\n",
    "                attention_state[block_id].append(att)\n",
    "\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    if output_attention:\n",
    "        return x, attention_state\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5882b8e6",
   "metadata": {
    "id": "cZGEd6UCfeou"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT model:\n",
    "- the initial stem consists of a combination of token encoding and a positional encoding\n",
    "- the meat of it is a uniform sequence of Transformer blocks\n",
    "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
    "    - all blocks feed into a central residual pathway similar to resnets\n",
    "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.att = None\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "\n",
    "        self.att = att\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53b4739f",
   "metadata": {
    "id": "0s5SO83OXVK6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import sacrebleu\n",
    "import math\n",
    "import logging\n",
    "from random import choice\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, valid_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self, postfix=''):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        checkpoint_path = self.config.ckpt_path + postfix + '.pt'\n",
    "        logger.info(\"saving %s\", checkpoint_path)\n",
    "        torch.save(raw_model.state_dict(), checkpoint_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset\n",
    "            if split == 'test':\n",
    "                data = self.test_dataset\n",
    "            elif split == 'valid':\n",
    "                data = self.valid_dataset\n",
    "                model.eval()\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size, # if is_train else 8,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "#             predicted_tokids = None\n",
    "            context_list = []\n",
    "            translation_results = []\n",
    "            eval_results = []\n",
    "            x_total = None\n",
    "            y_total = None\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    if split == 'valid':\n",
    "                        intent = (x == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True) #[0]\n",
    "                        #print(valid_dataset.tokenizer_input.encode(['<eos>']))\n",
    "                        #print(intent)\n",
    "                        #print(x.shape, y.shape, logits.shape)\n",
    "                        #for i in range(len(intent[0])):\n",
    "                        #    print(x[i][intent[1][i]], end=\", \")\n",
    "                        #print()\n",
    "\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        #print(probs.shape)\n",
    "                        for i in range(len(probs)):\n",
    "                            # sample from the distribution or take the most likely\n",
    "                            _, predicted = torch.topk(probs[i], k=1, dim=-1)\n",
    "                            if len(predicted.shape) > 1:\n",
    "                                # print(\"PREDICTED:\", predicted.shape, predicted)\n",
    "                                predicted = predicted.squeeze()\n",
    "                                if len(predicted.shape) > 1:\n",
    "                                    print(\"AFTER predicted.squeeze(1):\", predicted.shape)\n",
    "                            sep = intent[1][i]\n",
    "                            # print(\"sep=\", sep)\n",
    "                            #print(\"***CONTEXT\")\n",
    "                            context = clean_tokens(data.tokenizer_input.decode(x[i][:sep - 1], True))\n",
    "                            #print(context)\n",
    "                            #print(\"***COMPLETION\")\n",
    "                            completion = clean_tokens(data.tokenizer_output.decode(predicted[sep:], True))\n",
    "                            #print(completion)\n",
    "                            #print(\"***REAL\")\n",
    "                            real = clean_tokens(data.tokenizer_output.decode(y[i][sep:], True))\n",
    "                            #print(real)\n",
    "                            context_list.append(context)\n",
    "                            translation_results.append(completion)\n",
    "                            eval_results.append(real)\n",
    "\n",
    "#                         probs = F.softmax(logits, dim=-1)\n",
    "#                         # sample from the distribution or take the most likely\n",
    "#                         _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "#                         if predicted_tokids is None:\n",
    "#                             predicted_tokids = [predicted]\n",
    "#                             x_total = x\n",
    "#                             y_total = y\n",
    "#                         else:\n",
    "#                             predicted_tokids.append(predicted)\n",
    "#                             x_total = torch.cat((x_total, x), dim=0)\n",
    "#                             y_total = torch.cat((y_total, y), dim=0)\n",
    "                        \n",
    "\n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. mean loss: {float(np.mean(losses)):.5f}. lr {lr:e}\")\n",
    "\n",
    "            if split == 'train':\n",
    "                train_loss = float(np.mean(losses))\n",
    "                print(f\"train loss: {train_loss}\")\n",
    "                return train_loss\n",
    "\n",
    "            if split == 'test':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"test loss: {test_loss}\")\n",
    "                return test_loss\n",
    "\n",
    "            if split == 'valid':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"valid loss: {test_loss}\")\n",
    "\n",
    "#                 eval_results = []\n",
    "#                 translation_results = []\n",
    "#                 context_list = []\n",
    "\n",
    "#                 for idx in range(len(logits_total)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     probs = F.softmax(logits_total[idx], dim=-1)\n",
    "#                     # sample from the distribution or take the most likely\n",
    "#                     _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "#                 for idx in range(len(predicted_tokids)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     predicted = predicted_tokids[idx]\n",
    "#                     print(\"***CONTEXT\")\n",
    "#                     context = clean_tokens(data.tokenizer_input.decode(x_total[idx][:intent - 1], True))\n",
    "#                     print(\"***COMPLETION\")\n",
    "#                     completion = clean_tokens(data.tokenizer_output.decode(predicted[intent:], True))\n",
    "#                     print(\"***REAL\")\n",
    "#                     real = clean_tokens(data.tokenizer_output.decode(y_total[idx][intent:], True))\n",
    "\n",
    "#                     context_list.append(context)\n",
    "#                     translation_results.append(completion)\n",
    "#                     eval_results.append(real)\n",
    "                \n",
    "                with open('valid.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "                with open('eval.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(eval_results))\n",
    "\n",
    "                with open('context.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(context_list))\n",
    "\n",
    "\n",
    "                !cat valid.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > valid.detok.txt\n",
    "                !cat eval.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > eval.detok.txt\n",
    "                !cat context.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > context.detok.txt\n",
    "\n",
    "                with open('eval.detok.txt', 'r') as f:\n",
    "                    eval_results = [l.strip() for l in f.readlines()]\n",
    "                with open('valid.detok.txt', 'r') as f:\n",
    "                    translation_results = [l.strip() for l in f.readlines()]\n",
    "                with open('context.detok.txt', 'r') as f:\n",
    "                    context_list = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#                 idx = choice(range(len(context_list)))\n",
    "                valid_sentences = ['the driver wore a cap and his face was thin and very tanned.',\n",
    "                                   'outside it was getting dark.',\n",
    "                                   'the two girls were asleep.',\n",
    "                                   'I would like to have had the uniform off although I did not care much about the outward forms.',\n",
    "                                   'I watched the flashes on San Gabriele.',\n",
    "                                   'I asked.',\n",
    "                                   '\"no.']\n",
    "\n",
    "                idx_list = [i for i, sentence in enumerate(eval_results) if sentence in valid_sentences]\n",
    "                \n",
    "                for idx in idx_list:\n",
    "                    print(f'Input:            {context_list[idx]}')\n",
    "                    print(f'Predicted output: {translation_results[idx]}')\n",
    "                    print(f'Real output:      {eval_results[idx]}')\n",
    "                    print('--------------------------------------------------')\n",
    "\n",
    "                refs = [eval_results]\n",
    "                sys = translation_results\n",
    "                bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "                print(f'BLEU: {bleu.score}')\n",
    "                print('##############################################################')\n",
    "\n",
    "                return test_loss, bleu.score\n",
    "\n",
    "        train_loss_list = []\n",
    "        test_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        valid_bleu_list = []\n",
    "        best_loss = float('inf')\n",
    "        best_bleu = 0.0\n",
    "        bleu_score = -1.0\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train')\n",
    "            train_loss_list.append(train_loss)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "                test_loss_list.append(test_loss)\n",
    "\n",
    "            if self.valid_dataset is not None:\n",
    "                valid_loss, bleu_score = run_epoch('valid')\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                valid_bleu_list.append(bleu_score)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            # good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            good_model = self.valid_dataset is None or bleu_score > best_bleu\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                best_bleu = bleu_score\n",
    "                self.save_checkpoint(\"_best\")\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.save_checkpoint(f\"_{epoch}\")\n",
    "\n",
    "            self.save_checkpoint(\"_last\")\n",
    "\n",
    "        return train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8c09a",
   "metadata": {
    "id": "jKCnnx4-XTg1"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c93a65f",
   "metadata": {
    "id": "Ds0Rr1hrbRDq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size, vocab):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = set(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "        if self.vocab_size != vocab_size:\n",
    "            logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
    "        print(f\"Tokenizer vocab_size={vocab_size} len(vocab)={len(self.vocab)}\")\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "    \n",
    "    def tokenize(self, data, block_size):\n",
    "        tokenized_text = data.split()\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if tokenized in self.vocab:\n",
    "                result.append(tokenized)\n",
    "            else:\n",
    "                logger.warn(f\"Tokenizer UNKNOWN TOKEN: |{tokenized}|\")\n",
    "                result.append('<unk>')\n",
    "\n",
    "        # in case the sentence is longer, than block_size, we trim the sentence\n",
    "        return result[:block_size]\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data, clean_paddings=False):\n",
    "        if len(data.shape) > 1:\n",
    "            print(data.shape)\n",
    "            print(data)\n",
    "        text = ' '.join([self.itos[int(i)] for i in data if int(i) >= 0])\n",
    "\n",
    "        if not clean_paddings:\n",
    "            return text\n",
    "        return text.replace('<pad>', '').replace('  ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644be50a",
   "metadata": {
    "id": "advised-shelter"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-c38ff7f36e88>:7: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size=5500 len(vocab)=10519\n",
      "Tokenizer vocab_size=5500 len(vocab)=10519\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 10000\n",
    "\n",
    "# vocab_input = None\n",
    "# if os.path.exists('vocab_input.pkl'):\n",
    "#     with open('vocab_input.pkl', 'rb') as f:\n",
    "#         vocab_input = pickle.load(f)\n",
    "        \n",
    "# vocab_output = None\n",
    "# if os.path.exists('vocab_output.pkl'):\n",
    "#     with open('vocab_output.pkl', 'rb') as f:\n",
    "#         vocab_output = pickle.load(f)\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
    "tokenizer_input = Tokenizer(text_input, vocab_size, list(joint_vocab))\n",
    "tokenizer_output = Tokenizer(text_output, vocab_size, list(joint_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b55696",
   "metadata": {
    "id": "7d8f6ac2-0118-4d55-99c0-a478bfa6fe27"
   },
   "outputs": [],
   "source": [
    "# with open('vocab_input.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer_input.vocab, f)\n",
    "\n",
    "# with open('vocab_output.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer_output.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce5ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613611 1613611\n"
     ]
    }
   ],
   "source": [
    "assert len(text_input.splitlines()) == len(text_output.splitlines()), \\\n",
    "   f\"{len(text_input.splitlines())} {len(text_output.splitlines())}\"\n",
    "# assert len(text_lv.splitlines()) == len(text_en.splitlines())\n",
    "# assert len(text_lv.splitlines()) == len(text_input.splitlines())\n",
    "line_idxs = list(range(len(text_input.splitlines())))\n",
    "random.shuffle(line_idxs)\n",
    "print(len(line_idxs), len(text_input.splitlines()))\n",
    "# print(line_idxs[:10], line_idxs[-10:])\n",
    "\n",
    "train_dataset_size = round(0.75 * len(line_idxs))\n",
    "test_dataset_size = round(0.15 * len(line_idxs))\n",
    "valid_dataset_size = round(0.1 * len(line_idxs))\n",
    "\n",
    "train_idxs = line_idxs[:train_dataset_size]\n",
    "test_idxs = line_idxs[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_idxs = line_idxs[-valid_dataset_size:]\n",
    "\n",
    "assert len(train_idxs) + len(valid_idxs) + len(test_idxs) == len(line_idxs)\n",
    "assert set(line_idxs) == set(train_idxs) | set(valid_idxs) | set(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "add21564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 gadus vec@@ s pa@@ vār@@ s atra@@ sts mir@@ is S@@ an@@ f@@ ran@@ c@@ isko liel@@ veik@@ al@@ ā\n",
      "28 gadus vec@@ s pa@@ vār@@ s , kurš nesen pār@@ cē@@ l@@ ies uz S@@ an@@ f@@ ran@@ c@@ isko , š@@ on\n",
      "1613611\n"
     ]
    }
   ],
   "source": [
    "print(text_input[:200])\n",
    "print(f\"{len(text_input.splitlines())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7c8d079",
   "metadata": {
    "id": "emotional-metadata"
   },
   "outputs": [],
   "source": [
    "# Shuffle texts by lines\n",
    "# texts = list(zip(text_output.splitlines(), text_input.splitlines()))\n",
    "# random.shuffle(texts)\n",
    "# output_texts, input_texts = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a8554dc",
   "metadata": {
    "id": "floral-ridge"
   },
   "outputs": [],
   "source": [
    "# Split texts into train, test and validation datasets\n",
    "# train_dataset_size = round(0.75 * len(output_texts))\n",
    "# test_dataset_size = round(0.15 * len(output_texts))\n",
    "# valid_dataset_size = round(0.1 * len(output_texts))\n",
    "\n",
    "# train_input = input_texts[:train_dataset_size]\n",
    "# test_input = input_texts[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "# valid_input = input_texts[-valid_dataset_size:]\n",
    "\n",
    "# train_output = output_texts[:train_dataset_size]\n",
    "# test_output = output_texts[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "# valid_output = output_texts[-valid_dataset_size:]\n",
    "\n",
    "def separate_lines(text, train_idxs, valid_idxs, test_idxs):\n",
    "    text_lines = text.splitlines()\n",
    "    train_lines = [text_lines[idx] for idx in train_idxs]\n",
    "    valid_lines = [text_lines[idx] for idx in valid_idxs]\n",
    "    test_lines = [text_lines[idx] for idx in test_idxs]\n",
    "    return train_lines, valid_lines, test_lines\n",
    "\n",
    "train_input, valid_input, test_input = separate_lines(text_input, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "train_output, valid_output, test_output = separate_lines(text_output, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "print(len(train_input), len(valid_input), len(test_input))\n",
    "assert len(train_input) == len(train_output)\n",
    "assert len(valid_input) == len(valid_output)\n",
    "assert len(test_input) == len(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2250b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/tilde/train2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_input))\n",
    "\n",
    "with open('data/tilde/test2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_input))\n",
    "\n",
    "with open('data/tilde/valid2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_input))\n",
    "\n",
    "\n",
    "with open('data/tilde/train2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_output))\n",
    "\n",
    "with open('data/tilde/test2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_output))\n",
    "\n",
    "with open('data/tilde/valid2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55e8ab19",
   "metadata": {
    "id": "oZUwkrJeb0p4"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, output_text, input_text, tokenizer_output, tokenizer_input, block_size):\n",
    "        self.tokenizer_output = tokenizer_output\n",
    "        self.tokenizer_input = tokenizer_input\n",
    "\n",
    "        self.block_size = block_size * 2 + 1\n",
    "        self.output_text = [tokenizer_output.tokenize(t, block_size) for t in output_text]\n",
    "        self.input_text = [tokenizer_input.tokenize(t, block_size) for t in input_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The idea is to get the input sentence\n",
    "        and translate it to output sentence (sentences could be on any language).\n",
    "\n",
    "        In the init method we already split a sentence into tokens and filled with spaces,\n",
    "        to have an equal sentence size. In this method we just encode the tokens to\n",
    "        ids (a list of numbers), and we're trying to map ids sequences\n",
    "        \"\"\"\n",
    "\n",
    "        tokenized_input_text = self.tokenizer_input.encode(self.input_text[idx])\n",
    "        tokenized_output_text = self.tokenizer_output.encode(self.output_text[idx])\n",
    "\n",
    "        dix = tokenized_input_text + self.tokenizer_output.encode(['<eos>']) + tokenized_output_text\n",
    "        if len(dix) < self.block_size:\n",
    "            dix += self.tokenizer_output.encode(['<pad>']) * (self.block_size - len(dix))\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        y[:len(tokenized_input_text) - 1] = -100\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bfc39d7",
   "metadata": {
    "id": "fitted-resident"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ encode Datasets - Start time: 2021-07-19 12:14:21.260499\n",
      "================ encode Datasets - Finished : 2021-07-19 12:15:03.128231 -- elapsed: 0:00:41.867732\n"
     ]
    }
   ],
   "source": [
    "block_size = 100  # the estimate how long lines the text could be (token count)\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Start time: {start_time}\")\n",
    "\n",
    "# for faster debuging of Out of Memory during validation\n",
    "_train_limit = len(train_output)   # 10000  # len(train_output)\n",
    "_eval_limit = 10000   # -1    # 5000\n",
    "\n",
    "train_dataset = WordDataset(train_output[:_train_limit], train_input[:_train_limit],\n",
    "                            tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "if _eval_limit > 0:\n",
    "    test_dataset = WordDataset(test_output[:_eval_limit], test_input[:_eval_limit],\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output[:_eval_limit], valid_input[:_eval_limit],\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "else:\n",
    "    test_dataset = WordDataset(test_output, test_input,\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output, valid_input,\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "finish_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Finished : {finish_time} -- elapsed: {finish_time-start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d661bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1210208"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: fixed, no longer shows UNKNOWN TOKEN\n",
    "\n",
    "# joint_vocab -s 10000\n",
    "# UNKNOWN TOKEN\n",
    "\n",
    "# |;@@| (2040)  # I &@@ apos@@ ;@@ m\n",
    "# |q@@| (148)\n",
    "# |R| (40)\n",
    "# |v| (409)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ceffb0e",
   "metadata": {
    "id": "ruled-astronomy"
   },
   "outputs": [],
   "source": [
    "number_of_heads = 8\n",
    "number_of_layers = 6\n",
    "\n",
    "# from mingpt.model import GPT, GPTConfig\n",
    "embd_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "\n",
    "max_vocab = max(tokenizer_input.vocab_size, tokenizer_output.vocab_size)\n",
    "mconf = GPTConfig(max_vocab, train_dataset.block_size,\n",
    "                  n_layer=number_of_layers, n_head=number_of_heads, n_embd=512,\n",
    "                  embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop)\n",
    "\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27fc754d",
   "metadata": {
    "id": "bUgb5MxODXBc"
   },
   "outputs": [],
   "source": [
    "# from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "tokens_per_epoch = len(train_dataset) * block_size\n",
    "train_epochs = 100\n",
    "_batch_size = 128\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=train_epochs, \n",
    "                      batch_size=_batch_size, learning_rate=3e-4,\n",
    "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "                      ckpt_path='minGPT-Tilde-LV-EN-translator_model',\n",
    "                      num_workers=1, weight_decay=0.0001, betas=(0.9, 0.98))\n",
    "trainer = Trainer(model, train_dataset, test_dataset, valid_dataset, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4982f64b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPH3nCe9JB63",
    "outputId": "382f135d-e133-4e0b-9bb5-2f7b01269cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters count: 29789696\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(f'Parameters count: {param_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "342f964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters count: 28628480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafea904",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG17yBsSf3uc",
    "outputId": "ca906c3a-1793-4ab9-84c3-3f6c5f59c37d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9455 [00:00<?, ?it/s]/home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 9454: train loss 0.37617. mean loss: 0.69961. lr 2.999637e-04: 100%|██████████| 9455/9455 [40:53<00:00,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.6996148201601017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.36083900023110305\n",
      "valid loss: 0.3672173690946796\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 29.023342985987902\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 9454: train loss 0.28366. mean loss: 0.34594. lr 2.995702e-04: 100%|██████████| 9455/9455 [40:05<00:00,  3.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.34593802052537703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.28575764124906516\n",
      "valid loss: 0.28697750821143764\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 36.45319734511089\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 iter 9454: train loss 0.27194. mean loss: 0.29497. lr 2.987451e-04: 100%|██████████| 9455/9455 [40:15<00:00,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2949696494876109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.26050413767748243\n",
      "valid loss: 0.2612633097775375\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 39.23990755373973\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 iter 9454: train loss 0.29803. mean loss: 0.27224. lr 2.974907e-04: 100%|██████████| 9455/9455 [40:16<00:00,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2722373852633346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2475800348233573\n",
      "valid loss: 0.2479765484981899\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 40.65377349379643\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 iter 9454: train loss 0.24641. mean loss: 0.25836. lr 2.958108e-04: 100%|██████████| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.25836024345196723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.23765818001348762\n",
      "valid loss: 0.2390122851238975\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 41.7472891931476\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 iter 9454: train loss 0.24769. mean loss: 0.24861. lr 2.937100e-04: 100%|██████████| 9455/9455 [40:22<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24861045683867586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.232530456177796\n",
      "valid loss: 0.23358417311801186\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 42.45037871859485\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 iter 9454: train loss 0.27818. mean loss: 0.24121. lr 2.911946e-04: 100%|██████████| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24120904027564635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22838797769214533\n",
      "valid loss: 0.22867248009277297\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 43.07260916009157\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 iter 9454: train loss 0.26262. mean loss: 0.23528. lr 2.882717e-04: 100%|██████████| 9455/9455 [40:14<00:00,  3.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.23527597282564494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22521848580505274\n",
      "valid loss: 0.22536798613735393\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 43.65638135527434\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 iter 9454: train loss 0.24683. mean loss: 0.23033. lr 2.849498e-04: 100%|██████████| 9455/9455 [40:49<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2303313877013926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22130452568017983\n",
      "valid loss: 0.22205081242549268\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.12289428224524\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10 iter 9454: train loss 0.22590. mean loss: 0.22611. lr 2.812385e-04: 100%|██████████| 9455/9455 [40:27<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2261057166685067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21927649612668193\n",
      "valid loss: 0.21927647613271883\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.41774529250004\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11 iter 9454: train loss 0.21838. mean loss: 0.22241. lr 2.771485e-04: 100%|██████████| 9455/9455 [40:14<00:00,  3.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2224081448686621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2157665193080902\n",
      "valid loss: 0.2173823997189727\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.74593714633582\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12 iter 9454: train loss 0.20836. mean loss: 0.21917. lr 2.726915e-04: 100%|██████████| 9455/9455 [40:41<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2191733108601956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21537618395648425\n",
      "valid loss: 0.2158568824016595\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.00463234087414\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13 iter 9454: train loss 0.20313. mean loss: 0.21626. lr 2.678805e-04: 100%|██████████| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21625527119365462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21373417615136014\n",
      "valid loss: 0.21526709632782998\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.269307256196036\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14 iter 9454: train loss 0.19538. mean loss: 0.21363. lr 2.627293e-04: 100%|██████████| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21363016688306768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21166858850400658\n",
      "valid loss: 0.21385281617882884\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.41680600923591\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15 iter 9454: train loss 0.22387. mean loss: 0.21117. lr 2.572529e-04: 100%|██████████| 9455/9455 [40:28<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21116990949948714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21177052640462224\n",
      "valid loss: 0.21240954553779168\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.59820519734781\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16 iter 9454: train loss 0.21955. mean loss: 0.20891. lr 2.514669e-04: 100%|██████████| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20890747796663336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20916303897960276\n",
      "valid loss: 0.21087813735762728\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.74848845976228\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17 iter 9454: train loss 0.22276. mean loss: 0.20676. lr 2.453881e-04: 100%|██████████| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20675669054326906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20803766782525218\n",
      "valid loss: 0.21013710638390312\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.943301461503374\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18 iter 9454: train loss 0.18144. mean loss: 0.20483. lr 2.390341e-04: 100%|██████████| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2048345522424142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2080847983118854\n",
      "valid loss: 0.20898202495484414\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.09281382821358\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19 iter 9454: train loss 0.20288. mean loss: 0.20297. lr 2.324231e-04: 100%|██████████| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20296576087087756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.207252168202702\n",
      "valid loss: 0.20917992859701567\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.153103416634124\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20 iter 9454: train loss 0.21421. mean loss: 0.20121. lr 2.255743e-04: 100%|██████████| 9455/9455 [40:30<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2012118115619274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20678809643546237\n",
      "valid loss: 0.20792522818981846\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.406856579350915\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21 iter 9454: train loss 0.16354. mean loss: 0.19950. lr 2.185074e-04: 100%|██████████| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19949543326226318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20507498575916774\n",
      "valid loss: 0.2068335598782648\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.628576311842565\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22 iter 9454: train loss 0.20187. mean loss: 0.19793. lr 2.112428e-04: 100%|██████████| 9455/9455 [40:35<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19792634532887868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20490934690342674\n",
      "valid loss: 0.2065088828153248\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.66234109392401\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 23 iter 9454: train loss 0.18201. mean loss: 0.19637. lr 2.038015e-04: 100%|██████████| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19637382556243527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2043918065632446\n",
      "valid loss: 0.2047792848529695\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.7183603453369\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24 iter 9454: train loss 0.19402. mean loss: 0.19490. lr 1.962049e-04: 100%|██████████| 9455/9455 [40:45<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19490345557614774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20384961781622488\n",
      "valid loss: 0.20556440704231022\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.8894360556575\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 25 iter 9454: train loss 0.17490. mean loss: 0.19347. lr 1.884750e-04: 100%|██████████| 9455/9455 [40:28<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19346737188835234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2050862210460856\n",
      "valid loss: 0.20475829854796204\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.88064001940165\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26 iter 9454: train loss 0.20170. mean loss: 0.19209. lr 1.806340e-04: 100%|██████████| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19208716121790334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2030955924640728\n",
      "valid loss: 0.20354613605179364\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.109954288906174\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 27 iter 9454: train loss 0.15688. mean loss: 0.19080. lr 1.727047e-04: 100%|██████████| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19080226524427163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20222395696217502\n",
      "valid loss: 0.2042009289128871\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.27382218675211\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 28 iter 9454: train loss 0.19092. mean loss: 0.18950. lr 1.647098e-04: 100%|██████████| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18949553765581123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20100688934326172\n",
      "valid loss: 0.2018579433235941\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.23330071376662\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29 iter 9454: train loss 0.20935. mean loss: 0.18825. lr 1.566725e-04: 100%|██████████| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18825023457494633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2010484296309797\n",
      "valid loss: 0.20340422550334206\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.409370359589175\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30 iter 9454: train loss 0.18692. mean loss: 0.18699. lr 1.486160e-04: 100%|██████████| 9455/9455 [40:39<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1869891067997354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2011563600618628\n",
      "valid loss: 0.20210107538519026\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.560529580980116\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31 iter 9454: train loss 0.18474. mean loss: 0.18582. lr 1.405634e-04: 100%|██████████| 9455/9455 [40:35<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18581868690009598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2004196458979498\n",
      "valid loss: 0.2022080445968652\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.588476973404354\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 32 iter 9454: train loss 0.18383. mean loss: 0.18469. lr 1.325381e-04: 100%|██████████| 9455/9455 [40:44<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1846891991002165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1996266534434089\n",
      "valid loss: 0.20358680679073818\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.661625183435305\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 33 iter 9454: train loss 0.19077. mean loss: 0.18353. lr 1.245631e-04: 100%|██████████| 9455/9455 [40:39<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.183534984508877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19887856818452665\n",
      "valid loss: 0.20092384743539593\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.73602168591616\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 34 iter 9454: train loss 0.17051. mean loss: 0.18246. lr 1.166616e-04: 100%|██████████| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1824570844313381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1995796736659883\n",
      "valid loss: 0.20057060956200468\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.86665113728899\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 35 iter 9454: train loss 0.20175. mean loss: 0.18137. lr 1.088562e-04: 100%|██████████| 9455/9455 [40:38<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18136992486679385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19864992457854597\n",
      "valid loss: 0.20065974283821975\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.88581977204235\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 36 iter 9454: train loss 0.19712. mean loss: 0.18034. lr 1.011696e-04: 100%|██████████| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18034425895091782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19923306586621684\n",
      "valid loss: 0.1994970382391652\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.02032018617099\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 37 iter 9454: train loss 0.16863. mean loss: 0.17932. lr 9.362393e-05: 100%|██████████| 9455/9455 [40:42<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17932144434032207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19972689393200452\n",
      "valid loss: 0.20203917501847954\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.123408130312036\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 38 iter 9454: train loss 0.18200. mean loss: 0.17840. lr 8.624092e-05: 100%|██████████| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1783985232452688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19790536768828768\n",
      "valid loss: 0.2002376178397408\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.13085537766696\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 39 iter 9454: train loss 0.17471. mean loss: 0.17744. lr 7.904189e-05: 100%|██████████| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17744279825800882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19784491424319112\n",
      "valid loss: 0.2005579250900051\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.277171792616514\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 40 iter 9454: train loss 0.16865. mean loss: 0.17654. lr 7.204763e-05: 100%|██████████| 9455/9455 [42:55<00:00,  3.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17654110790559818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1980528703218774\n",
      "valid loss: 0.1986544368010533\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.31917366907318\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41 iter 9454: train loss 0.18227. mean loss: 0.17573. lr 6.527832e-05: 100%|██████████| 9455/9455 [48:10<00:00,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17573457686816618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19793730427192735\n",
      "valid loss: 0.19966118282909634\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.395523990694976\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42 iter 9454: train loss 0.17776. mean loss: 0.17487. lr 5.875349e-05: 100%|██████████| 9455/9455 [41:09<00:00,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17486677899964584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19823104633560665\n",
      "valid loss: 0.19842650524423092\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.45918003592048\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43 iter 9454: train loss 0.15216. mean loss: 0.17407. lr 5.249197e-05: 100%|██████████| 9455/9455 [49:23<00:00,  3.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1740701851581279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19738954437684408\n",
      "valid loss: 0.19837068266506436\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.463979358418534\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 44 iter 9454: train loss 0.16014. mean loss: 0.17329. lr 4.651183e-05: 100%|██████████| 9455/9455 [44:45<00:00,  3.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17329278724777328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19883684493318388\n",
      "valid loss: 0.1986763286816923\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.49418865821421\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 45 iter 9454: train loss 0.18177. mean loss: 0.17261. lr 4.083033e-05: 100%|██████████| 9455/9455 [45:44<00:00,  3.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1726077829235953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1967835024565081\n",
      "valid loss: 0.1982685452020621\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.53441302345551\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 46 iter 9454: train loss 0.17749. mean loss: 0.17192. lr 3.546385e-05: 100%|██████████| 9455/9455 [49:01<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17192277030173211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19700185803672934\n",
      "valid loss: 0.19870617042613936\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.63323423462656\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 47 iter 9454: train loss 0.14104. mean loss: 0.17129. lr 3.042790e-05: 100%|██████████| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1712914384810903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19920070186445984\n",
      "valid loss: 0.19794500553155248\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.602968233400496\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 48 iter 9454: train loss 0.20204. mean loss: 0.17076. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:05<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17075890384058925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1962979328406008\n",
      "valid loss: 0.19930218726019316\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66759613322437\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 49 iter 9454: train loss 0.18914. mean loss: 0.17063. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:06<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1706344845905183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19695725414571882\n",
      "valid loss: 0.19833394526680814\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66008366042197\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 50 iter 9454: train loss 0.19319. mean loss: 0.17053. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:58<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17052941837051064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19821986424017557\n",
      "valid loss: 0.1983115865459925\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67672420109884\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 51 iter 9454: train loss 0.16709. mean loss: 0.17043. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:03<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17042742783291387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19660844493515883\n",
      "valid loss: 0.1980167914040481\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67497569816444\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 52 iter 9454: train loss 0.14669. mean loss: 0.17029. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17029285709625322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19613911400112924\n",
      "valid loss: 0.19867383434048183\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.669754173433546\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 53 iter 9454: train loss 0.18860. mean loss: 0.17019. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:01<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17018792826321819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1963504245386848\n",
      "valid loss: 0.19885536179512361\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.633725855044034\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 54 iter 9454: train loss 0.19103. mean loss: 0.17009. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:54<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1700896316777771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19719673655455625\n",
      "valid loss: 0.19910381168504304\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.678039226953736\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 55 iter 9454: train loss 0.16854. mean loss: 0.16998. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:51<00:00,  3.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16997740764283176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19753534261938893\n",
      "valid loss: 0.1987545712839199\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.671821181483054\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 56 iter 9454: train loss 0.15969. mean loss: 0.16990. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:09<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16990491046217118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19853379002100305\n",
      "valid loss: 0.19891305418708657\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.68985469988521\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 57 iter 9454: train loss 0.19053. mean loss: 0.16982. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16981655132953355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1975858030439932\n",
      "valid loss: 0.19993347684039345\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.6112342089157\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 58 iter 9454: train loss 0.15873. mean loss: 0.16972. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:07<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1697181096000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19676471586468852\n",
      "valid loss: 0.19872604235063626\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.68102753441791\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 59 iter 9454: train loss 0.15122. mean loss: 0.16964. lr 3.000000e-05: 100%|██████████| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16963610662032158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19659348963936674\n",
      "valid loss: 0.19825328452677665\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.69478228381596\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 60 iter 9454: train loss 0.17749. mean loss: 0.16955. lr 3.000000e-05: 100%|██████████| 9455/9455 [49:07<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1695509258352342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1966380807040613\n",
      "valid loss: 0.19968913797336288\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.80495715291374\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 61 iter 9454: train loss 0.17364. mean loss: 0.16949. lr 3.000000e-05: 100%|██████████| 9455/9455 [41:09<00:00,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16949200037923962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.197903176463103\n",
      "valid loss: 0.19911990418464323\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.77611517190088\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 62 iter 9454: train loss 0.18548. mean loss: 0.16941. lr 3.000000e-05: 100%|██████████| 9455/9455 [40:27<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1694119602709709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19700280850446678\n",
      "valid loss: 0.19770609974106657\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.73129038408612\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 63 iter 9454: train loss 0.19856. mean loss: 0.16932. lr 3.000000e-05: 100%|██████████| 9455/9455 [40:22<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1693200916122785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19739639212059068\n",
      "valid loss: 0.19915987060794346\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.69322224466296\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 64 iter 9454: train loss 0.16781. mean loss: 0.16928. lr 3.000000e-05: 100%|██████████| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16927531727450287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19742316384858724\n",
      "valid loss: 0.1974438459058351\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.694955531176085\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 65 iter 9454: train loss 0.17940. mean loss: 0.16921. lr 3.000000e-05: 100%|██████████| 9455/9455 [40:41<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16920688348447502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1977805968704103\n",
      "valid loss: 0.19841452565374254\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66952644559219\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 66 iter 7881: train loss 0.16907. mean loss: 0.16889. lr 3.000000e-05:  83%|████████▎ | 7882/9455 [33:42<06:49,  3.84it/s]  "
     ]
    }
   ],
   "source": [
    "train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ce1e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "EHFUYpv8P9lr",
    "outputId": "77da5c3a-eeaa-4132-abe1-2de95323a3e7"
   },
   "outputs": [],
   "source": [
    "epochs = range(len(test_loss_list))\n",
    "# plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(20, 10))\n",
    "axs[0].plot(epochs, train_loss_list)\n",
    "axs[0].set_title('Train loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[0].plot(epochs, test_loss_list)\n",
    "axs[0].set_title('Test loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(epochs, valid_loss_list)\n",
    "axs[1].set_title('Validation loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "\n",
    "axs[2].plot(epochs, valid_bleu_list)\n",
    "axs[2].set_title('Validation BLEU')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('BLEU')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba47345",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"tilde_losses.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176e9ad",
   "metadata": {
    "id": "1Og-0W-MtKoM"
   },
   "source": [
    "#Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29477d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss_list)\n",
    "print()\n",
    "print(f\"Max BLEU: {max(valid_bleu_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fd641",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d029ed9c-9170-435a-805f-416774999287",
    "outputId": "78ebc7df-beec-45fa-eddf-cc1829c07374"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('minGPT-Tilde-LV-EN-translator_model_best.pt')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tilde_train_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in train_loss_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ecf80",
   "metadata": {
    "id": "afC6uEVGXIPL"
   },
   "outputs": [],
   "source": [
    "with open('tilde_test_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in test_loss_list]))\n",
    "\n",
    "with open('tilde_valid_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in valid_loss_list]))\n",
    "\n",
    "with open('tilde_valid_bleu.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in valid_bleu_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8e088",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "greek-travel",
    "outputId": "8b5ee427-7a71-46ab-e62c-a8730ac37bb8"
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = choice(range(len(valid_output)))\n",
    "\n",
    "    context = valid_input[idx]\n",
    "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
    "\n",
    "    intent = len(encoded_input) + 1\n",
    "\n",
    "    predicted = y[intent:]\n",
    "    completion = tokenizer_output.decode(predicted, True)\n",
    "    print(f'Input:            {context}')\n",
    "    print(f'Predicted output: {completion}')\n",
    "    print(f'Real output:      {valid_output[idx]}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba89a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF3lesDzM_UY",
    "outputId": "8d15a610-69b9-47c9-c296-425e51f35e9f"
   },
   "outputs": [],
   "source": [
    "idx = choice(range(len(valid_output)))\n",
    "\n",
    "context = valid_input[idx]\n",
    "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
    "\n",
    "intent = len(encoded_input) + 1\n",
    "\n",
    "predicted = y[0][intent:]\n",
    "completion = tokenizer_output.decode(predicted,)\n",
    "print(f'Input:            {context}')\n",
    "print(f'Predicted output: {completion}')\n",
    "print(f'Real output:      {valid_output[idx]}')\n",
    "print('--------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869966d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "1uY06xs7n5sx",
    "outputId": "0b2993a4-b919-41a6-e99e-d395cde20c1e"
   },
   "outputs": [],
   "source": [
    "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "\n",
    "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
    "\n",
    "axis_text.append('<eos>')\n",
    "\n",
    "axis_text += tokenizer_input.decode(predicted, True).split()\n",
    "\n",
    "limit = len(axis_text)\n",
    "for bi in range(number_of_layers):\n",
    "    for hi in range(number_of_heads):\n",
    "        attetion_plot = torch.zeros(limit, limit)\n",
    "        for di in range(limit):\n",
    "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
    "\n",
    "        ax = plots[bi][hi]\n",
    "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
    "        ax.set_yticklabels([''] + axis_text)\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        # Set up a title\n",
    "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373e071",
   "metadata": {
    "id": "CXFro4HhyE8D"
   },
   "outputs": [],
   "source": [
    "# In case the previous cell is not plotting anything, uncomment the code below and execute. After that, the plotting should be fine.\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# x = np.linspace(0, 10, 100)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(x, np.sin(x), '-')\n",
    "# plt.plot(x, np.cos(x), '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a8ce0",
   "metadata": {
    "id": "bxvZ1nVstR7j"
   },
   "source": [
    "#Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7c39e",
   "metadata": {
    "id": "JlVOSUDaNqaz"
   },
   "outputs": [],
   "source": [
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd6bd6",
   "metadata": {
    "id": "4VAAvPyR4GMv"
   },
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# smooth = SmoothingFunction().method7\n",
    "\n",
    "translation_results = []\n",
    "eval_text = []\n",
    "bleu_results = []\n",
    "for idx, context in enumerate(valid_input):\n",
    "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
    "\n",
    "    intent = len(encoded_input) + 1\n",
    "    predicted = y[intent:]\n",
    "    completion = clean_tokens(tokenizer_output.decode(predicted, True))\n",
    "    translation_results.append(completion)\n",
    "\n",
    "    eval = clean_tokens(valid_output[idx])\n",
    "    eval_text.append(eval)\n",
    "    # bleu = sentence_bleu([eval], completion, smoothing_function=smooth)\n",
    "    # bleu_results.append(bleu)\n",
    "\n",
    "# print(f\"Averare BLEU: {np.mean(bleu_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c29254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_vocab -s 10000\n",
    "# UNKNOWN TOKEN\n",
    "\n",
    "# |v|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689604d6",
   "metadata": {
    "id": "4xQ1sDwfWS9S"
   },
   "outputs": [],
   "source": [
    "with open('tilde_valid.out', 'w') as f:\n",
    "    f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "with open('tilde_valid.ref', 'w') as f:\n",
    "    f.write(\"\\n\".join(eval_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34659f33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8TILVBwWncc",
    "outputId": "771c5462-601d-4e6e-9c44-d1b99918df5f"
   },
   "outputs": [],
   "source": [
    "!perl mosesdecoder/scripts/generic/multi-bleu.perl tilde_valid.ref < tilde_valid.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05237bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU = 7.92, 38.4/12.4/4.2/2.0 (BP=1.000, ratio=1.021, hyp_len=9711, ref_len=9509)\n",
    "\n",
    "# joint_vocab -s 10,000\n",
    "# BLEU = 8.61, 44.4/15.1/5.5/2.8 (BP=0.852, ratio=0.862, hyp_len=8198, ref_len=9509)\n",
    "\n",
    "# full joint_vocab\n",
    "# BLEU = 9.18, 41.7/14.1/5.4/2.8 (BP=0.948, ratio=0.950, hyp_len=9030, ref_len=9509)\n",
    "\n",
    "# model_best.pt\n",
    "# BLEU = 13.47, 48.0/19.6/9.4/5.5 (BP=0.908, ratio=0.912, hyp_len=8670, ref_len=9509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955e5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc00896",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0ccdKVEi_51",
    "outputId": "1325a0f7-bbc4-42ce-87c8-bdec30a82a37"
   },
   "outputs": [],
   "source": [
    "!cat tilde_valid.out | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.out\n",
    "!cat tilde_valid.ref | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775cb3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ_G2YbxgdnH",
    "outputId": "16446218-4a72-4072-d04d-fad681783dc5"
   },
   "outputs": [],
   "source": [
    "#!pip install sacrebleu\n",
    "!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e76d09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Z2dQh7gnq2",
    "outputId": "8a4d34d6-f607-4067-ab6c-bb6abb6d70c1"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "with open('tilde_valid.detok.ref', 'r') as f:\n",
    "    eval_ref = [l.strip() for l in f.readlines()]\n",
    "with open('tilde_valid.detok.out', 'r') as f:\n",
    "    translation_results = [l.strip() for l in f.readlines()]\n",
    "\n",
    "refs = [eval_ref]\n",
    "sys = translation_results\n",
    "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "print(bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43edc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.918993465381516\n",
    "# joint_vocab -s 10000  8.534786641173136\n",
    "\n",
    "# full joint_vocab 9.174070997058795\n",
    "\n",
    "# model_best.pt \n",
    "13.481896471451254"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f92ecb",
   "metadata": {
    "id": "q8b-0-iFkRRA"
   },
   "source": [
    "#Interactive translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9a4fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LHYXSA190x8G",
    "outputId": "75c2aeed-055e-4a30-b252-b1039a2b3fe0"
   },
   "outputs": [],
   "source": [
    "context = input(\"Enter your English text to translate: \")\n",
    "\n",
    "# Predict Latvian output\n",
    "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
    "\n",
    "intent = len(encoded_input) + 1\n",
    "\n",
    "predicted = y[0][intent:]\n",
    "completion = tokenizer_output.decode(predicted, True)\n",
    "print(f'Input:            {context}')\n",
    "print(f'Predicted output: {completion}')\n",
    "\n",
    "\n",
    "# Plot attention\n",
    "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "\n",
    "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
    "\n",
    "axis_text.append('<eos>')\n",
    "\n",
    "axis_text += tokenizer_input.decode(predicted, True).split()\n",
    "\n",
    "limit = len(axis_text)\n",
    "for bi in range(number_of_layers):\n",
    "    for hi in range(number_of_heads):\n",
    "        attetion_plot = torch.zeros(limit, limit)\n",
    "        for di in range(limit):\n",
    "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
    "\n",
    "        ax = plots[bi][hi]\n",
    "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
    "        ax.set_yticklabels([''] + axis_text)\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        # Set up a title\n",
    "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0ea9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "with valid Proper packages minGPT EN-LV translator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
