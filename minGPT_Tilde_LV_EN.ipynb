{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d31be3b",
   "metadata": {
    "id": "varying-atlas"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a11686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "#from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.pre_tokenizers import Punctuation\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, Strip, Replace, Sequence\n",
    "from tokenizers.trainers import UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c902685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cb9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILDE_DATA = './data/tilde'\n",
    "!mkdir -p $TILDE_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b763c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALC2-FQxnZ1l",
    "outputId": "3e4d0bbd-8b0d-4b9d-884a-eb88c0d3c556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sacrebleu\r\n",
      "Version: 1.5.1\r\n",
      "Summary: Hassle-free computation of shareable, comparable, and reproducible BLEU, chrF, and TER scores\r\n",
      "Home-page: https://github.com/mjpost/sacrebleu\r\n",
      "Author: Matt Post\r\n",
      "Author-email: post@cs.jhu.edu\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: portalocker\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install sacrebleu\n",
    "!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b97bf5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A8WZUBGiyv7",
    "outputId": "cbe405e8-f665-4298-d4ae-44c533bf0711"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3a9b6",
   "metadata": {
    "id": "Ex11yfCVaueI"
   },
   "source": [
    "#Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086c645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE_ops=10000 vocab_size=5500 joint_vocab_size=11000\r\n"
     ]
    }
   ],
   "source": [
    "num_bpe_merges = 10000\n",
    "vocab_size = 5500\n",
    "joint_vocab_size = 2*vocab_size\n",
    "\n",
    "!echo BPE_ops=$num_bpe_merges vocab_size=$vocab_size joint_vocab_size=$joint_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414dae43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTfvBWhIaHTh",
    "outputId": "e75b4122-7e6e-4abc-827a-134aec8cf165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: subword-nmt\r\n",
      "Version: 0.3.7\r\n",
      "Summary: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation\r\n",
      "Home-page: https://github.com/rsennrich/subword-nmt\r\n",
      "Author: Rico Sennrich\r\n",
      "Author-email: None\r\n",
      "License: MIT\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install subword-nmt\n",
    "!pip show subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0a192f",
   "metadata": {
    "id": "historic-thermal"
   },
   "outputs": [],
   "source": [
    "# # Read Hemingway texts from URL. There are Hemingway's \"A Farewell to arms\"\n",
    "# text_en = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_en_small.txt').read().decode(\"utf-8\", \"ignore\")\n",
    "# text_lv = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_lv_small.txt').read().decode(\"utf-8-sig\", \"ignore\")\n",
    "\n",
    "# HEMINGWAY_SRC_EN = f'{HEMINGWAY_DATA}/hemingway.en.txt'\n",
    "# HEMINGWAY_SRC_LV = f'{HEMINGWAY_DATA}/hemingway.lv.txt'\n",
    "\n",
    "# with open(HEMINGWAY_SRC_EN, 'w') as f:\n",
    "#     f.write(text_en)\n",
    "\n",
    "# with open(HEMINGWAY_SRC_LV, 'w') as f:\n",
    "#     f.write(text_lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd8c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tilde/combined.lv.tok.txt ./data/tilde/combined.en.tok.txt\r\n"
     ]
    }
   ],
   "source": [
    "TILDE_ALL_EN = f'{TILDE_DATA}/all.norm2.en'\n",
    "TILDE_ALL_LV = f'{TILDE_DATA}/all.norm2.lv'\n",
    "\n",
    "TILDE_TOK_EN = f'{TILDE_DATA}/combined.en.tok.txt'\n",
    "TILDE_TOK_LV = f'{TILDE_DATA}/combined.lv.tok.txt'\n",
    "\n",
    "!echo $TILDE_DATA/combined.lv.tok.txt $TILDE_TOK_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac7c794",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzgEuTRkV4kK",
    "outputId": "6ac1bf6e-9a2a-4cf7-c6cc-e3c76ec72522"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/moses-smt/mosesdecoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59777cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_qVpRZMGJDl",
    "outputId": "d51d2bb1-1337-4e36-eb1f-28f67fdfbd29"
   },
   "outputs": [],
   "source": [
    "# Normalize and tokenize texts\n",
    "cmd1 = f\"cat {TILDE_ALL_EN} | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en > {TILDE_TOK_EN}\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"cat {TILDE_ALL_LV} | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l lv | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l lv > {TILDE_TOK_LV}\"\n",
    "#! $cmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766b1373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_qVpRZMGJDl",
    "outputId": "d51d2bb1-1337-4e36-eb1f-28f67fdfbd29"
   },
   "outputs": [],
   "source": [
    "# # Normalize and tokenize texts\n",
    "\n",
    "# #!cat hemingway.en.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en \\\n",
    "# !cat hemingway.en.txt \\\n",
    "#   | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en > hemingway.en.tok.txt\n",
    "\n",
    "# # !cat hemingway.lv.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l lv \\\n",
    "# !cat hemingway.lv.txt \\\n",
    "#   | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l lv > hemingway.lv.tok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0cbdfb",
   "metadata": {
    "id": "YlDFOYvOIb2c"
   },
   "outputs": [],
   "source": [
    "#!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus $TILDE_DATA/combined.en.tok.txt -model $TILDE_DATA/tc_model.en\n",
    "#!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus $TILDE_DATA/combined.lv.tok.txt -model $TILDE_DATA/tc_model.lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb838d9e",
   "metadata": {
    "id": "YlDFOYvOIb2c"
   },
   "outputs": [],
   "source": [
    "cmd1 = f\"mosesdecoder/scripts/recaser/truecase.perl -model {TILDE_DATA}/tc_model.en < {TILDE_DATA}/combined.en.tok.txt > {TILDE_DATA}/combined.en.tc.txt\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"mosesdecoder/scripts/recaser/truecase.perl -model {TILDE_DATA}/tc_model.lv < {TILDE_DATA}/combined.lv.tok.txt > {TILDE_DATA}/combined.lv.tc.txt\"\n",
    "#! $cmd2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5314467",
   "metadata": {
    "id": "aiMbyzugbFZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword-nmt learn-joint-bpe-and-vocab --input ./data/tilde/combined.lv.tc.txt ./data/tilde/combined.en.tc.txt -s 10000 -o ./data/tilde/bpe/tokens.lven --write-vocabulary ./data/tilde/bpe/token_freq.en ./data/tilde/bpe/token_freq.lv\r\n"
     ]
    }
   ],
   "source": [
    "# !subword-nmt learn-joint-bpe-and-vocab --input en.tc.txt lv.tc.txt -s 10000 -o tokens.txt --write-vocabulary token_freq.en.txt token_freq.lv.txt\n",
    "!mkdir -p $TILDE_DATA/bpe \n",
    "\n",
    "!echo subword-nmt learn-joint-bpe-and-vocab --input $TILDE_DATA/combined.lv.tc.txt $TILDE_DATA/combined.en.tc.txt -s $num_bpe_merges -o $TILDE_DATA/bpe/tokens.lven --write-vocabulary $TILDE_DATA/bpe/token_freq.en $TILDE_DATA/bpe/token_freq.lv\n",
    "\n",
    "## !subword-nmt learn-joint-bpe-and-vocab --input $HEMINGWAY_DATA/hemingway.en.tc.txt -s $num_bpe_merges -o $HEMINGWAY_DATA/bpe/tokens.en --write-vocabulary $HEMINGWAY_DATA/bpe/token_freq.en\n",
    "## !subword-nmt learn-joint-bpe-and-vocab --input $HEMINGWAY_DATA/hemingway.lv.tc.txt -s $num_bpe_merges -o $HEMINGWAY_DATA/bpe/tokens.lv --write-vocabulary $HEMINGWAY_DATA/bpe/token_freq.lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57607ca",
   "metadata": {
    "id": "30_qUBkgSNG1"
   },
   "outputs": [],
   "source": [
    "def build_vocab(freq_file, vocab_size):\n",
    "    vocab = Counter(['<unk>', '<pad>', '<eos>'])\n",
    "    with open(freq_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            token, num_occurs = line.split()\n",
    "            # vocab.append(token)\n",
    "            vocab[token] += int(num_occurs)\n",
    "    return vocab #[:vocab_size]\n",
    "#     return vocab[:vocab_size]\n",
    "\n",
    "en_vocab = build_vocab(f'{TILDE_DATA}/bpe/token_freq.en', vocab_size)\n",
    "lv_vocab = build_vocab(f'{TILDE_DATA}/bpe/token_freq.lv', vocab_size)\n",
    "\n",
    "joint_vocab = Counter(en_vocab)\n",
    "joint_vocab.update(lv_vocab)\n",
    "\n",
    "if False:\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.en', 'w') as f:\n",
    "        for i, token in enumerate(en_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {en_vocab[token]} \\n\")\n",
    "\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.lv', 'w') as f:\n",
    "        for i, token in enumerate(lv_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {lv_vocab[token]} \\n\")\n",
    "\n",
    "    with open(f'{TILDE_DATA}/bpe/vocab.lven', 'w') as f:\n",
    "        for i, token in enumerate(joint_vocab):\n",
    "            # f.write(f\"{token} {i + 1} \\n\")\n",
    "            f.write(f\"{token} {joint_vocab[token]} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9712800b",
   "metadata": {
    "id": "CtzY4LjYbSiJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab: 10099 lv_vocab: 6477 joint_vocab 10519\n"
     ]
    }
   ],
   "source": [
    "print(\"en_vocab:\", len(en_vocab), \"lv_vocab:\", len(lv_vocab), \"joint_vocab\", len(joint_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dccb610",
   "metadata": {
    "id": "CtzY4LjYbSiJ"
   },
   "outputs": [],
   "source": [
    "#!subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.en --vocabulary $HEMINGWAY_DATA/bpe/vocab.en --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.en.tc.txt > $HEMINGWAY_DATA/hemingway.en.BPE.txt\n",
    "#!subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lv --vocabulary $HEMINGWAY_DATA/bpe/vocab.lv --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.lv.tc.txt > $HEMINGWAY_DATA/hemingway.lv.BPE.txt\n",
    "\n",
    "# !subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lven --vocabulary $HEMINGWAY_DATA/bpe/token_freq.en --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.en.tc.txt > $HEMINGWAY_DATA/hemingway.en.BPE.txt\n",
    "# !subword-nmt apply-bpe -c $HEMINGWAY_DATA/bpe/tokens.lven --vocabulary $HEMINGWAY_DATA/bpe/token_freq.lv --vocabulary-threshold 1 < $HEMINGWAY_DATA/hemingway.lv.tc.txt > $HEMINGWAY_DATA/hemingway.lv.BPE.txt\n",
    "\n",
    "\n",
    "cmd1 = f\"subword-nmt apply-bpe -c {TILDE_DATA}/bpe/tokens.lven --vocabulary {TILDE_DATA}/bpe/vocab.lven --vocabulary-threshold 1 < {TILDE_DATA}/combined.en.tc.txt > {TILDE_DATA}/combined.en.BPE.txt\"\n",
    "#! $cmd1\n",
    "\n",
    "cmd2 = f\"subword-nmt apply-bpe -c {TILDE_DATA}/bpe/tokens.lven --vocabulary {TILDE_DATA}/bpe/vocab.lven --vocabulary-threshold 1 < {TILDE_DATA}/combined.lv.tc.txt > {TILDE_DATA}/combined.lv.BPE.txt\"\n",
    "#! $cmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0980aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    special_tokens = ['<unk>', '<pad>', '<eos>', '<sep>'] #, '<S>', '</S>', '<bos>', '<eos>', '<sep>', '<NONE>', '<|>']\n",
    "                  \n",
    "    normalizer = normalizers.Sequence([Strip(), Lowercase()])\n",
    "    pre_tokenizer = Whitespace()\n",
    "\n",
    "    model = tokenizers.models.WordLevel(unk_token='<unk>')\n",
    "    # model = tokenizers.models.WordPiece()\n",
    "    tokenizer = tokenizers.Tokenizer(model=model)\n",
    "\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"test/*.pthru\"))\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"train/*.pthru\"))\n",
    "\n",
    "\n",
    "    # token_strs = [tok for (tok, span) in pre_tokenizer.pre_tokenize_str(str1)]\n",
    "    # print(token_strs)\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "\n",
    "    filelist = glob.glob(f\"{TILDE_DATA}/combined.*.BPE.txt\")\n",
    "\n",
    "    filelist = sorted(filelist)\n",
    "    print(len(filelist), filelist[:10])\n",
    "\n",
    "\n",
    "    # unigram_trainer = tokenizers.trainers.UnigramTrainer()\n",
    "    # trainer = tokenizers.trainers.WordPieceTrainer(vocab_size=vocab_size)\n",
    "    trainer = tokenizers.trainers.WordLevelTrainer(vocab_size=joint_vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer.train(files=filelist, trainer=trainer)\n",
    "\n",
    "    vocab_dict = tokenizer.get_vocab(with_added_tokens=False)\n",
    "    print(\"ACTUAL VOCAB SIZE =\", len(vocab_dict))\n",
    "    print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a403d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! ACTUAL VOCAB SIZE = 900 (first try when joint_vocab but separate --vocabulary token_freq.lang)\n",
    "# ACTUAL VOCAB SIZE = 8637\n",
    "# ACTUAL VOCAB SIZE = 9048 (CUDA out of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6efc05bb",
   "metadata": {
    "id": "wcc2v_fHe81_"
   },
   "outputs": [],
   "source": [
    "with open(f'{TILDE_DATA}/combined.lv.BPE.txt', 'r') as f:\n",
    "    text_input = f.read()\n",
    "\n",
    "with open(f'{TILDE_DATA}/combined.en.BPE.txt', 'r') as f:\n",
    "    text_output = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc48ac4",
   "metadata": {
    "id": "PAHdOeNhYJ8v"
   },
   "source": [
    "#MinGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3032f156",
   "metadata": {
    "id": "OcFt-zMBfqDt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def calculate_attention_token(attention, top_k, model):\n",
    "    logits = model.head(attention)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "    ix = torch.multinomial(probs, num_samples=top_k)\n",
    "\n",
    "    return ix[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None, output_attention=False):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    attention_state = [[] for _ in model.blocks]\n",
    "\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "        if output_attention:\n",
    "            b, t = x.size()\n",
    "\n",
    "            for block_id in range(len(model.blocks)):\n",
    "                att = model.blocks[block_id].attn.att\n",
    "                attention_state[block_id].append(att)\n",
    "\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    if output_attention:\n",
    "        return x, attention_state\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8228ae85",
   "metadata": {
    "id": "cZGEd6UCfeou"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT model:\n",
    "- the initial stem consists of a combination of token encoding and a positional encoding\n",
    "- the meat of it is a uniform sequence of Transformer blocks\n",
    "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
    "    - all blocks feed into a central residual pathway similar to resnets\n",
    "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.att = None\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "\n",
    "        self.att = att\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8872e73",
   "metadata": {
    "id": "0s5SO83OXVK6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import sacrebleu\n",
    "import math\n",
    "import logging\n",
    "from random import choice\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, valid_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self, postfix=''):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        checkpoint_path = self.config.ckpt_path + postfix + '.pt'\n",
    "        logger.info(\"saving %s\", checkpoint_path)\n",
    "        torch.save(raw_model.state_dict(), checkpoint_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset\n",
    "            if split == 'test':\n",
    "                data = self.test_dataset\n",
    "            elif split == 'valid':\n",
    "                data = self.valid_dataset\n",
    "                model.eval()\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size, # if is_train else 8,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "#             predicted_tokids = None\n",
    "            context_list = []\n",
    "            translation_results = []\n",
    "            eval_results = []\n",
    "            x_total = None\n",
    "            y_total = None\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    if split == 'valid':\n",
    "                        intent = (x == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True) #[0]\n",
    "                        #print(valid_dataset.tokenizer_input.encode(['<eos>']))\n",
    "                        #print(intent)\n",
    "                        #print(x.shape, y.shape, logits.shape)\n",
    "                        #for i in range(len(intent[0])):\n",
    "                        #    print(x[i][intent[1][i]], end=\", \")\n",
    "                        #print()\n",
    "\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        #print(probs.shape)\n",
    "                        for i in range(len(probs)):\n",
    "                            # sample from the distribution or take the most likely\n",
    "                            _, predicted = torch.topk(probs[i], k=1, dim=-1)\n",
    "                            if len(predicted.shape) > 1:\n",
    "                                # print(\"PREDICTED:\", predicted.shape, predicted)\n",
    "                                predicted = predicted.squeeze()\n",
    "                                if len(predicted.shape) > 1:\n",
    "                                    print(\"AFTER predicted.squeeze(1):\", predicted.shape)\n",
    "                            sep = intent[1][i]\n",
    "                            # print(\"sep=\", sep)\n",
    "                            #print(\"***CONTEXT\")\n",
    "                            context = clean_tokens(data.tokenizer_input.decode(x[i][:sep - 1], True))\n",
    "                            #print(context)\n",
    "                            #print(\"***COMPLETION\")\n",
    "                            completion = clean_tokens(data.tokenizer_output.decode(predicted[sep:], True))\n",
    "                            #print(completion)\n",
    "                            #print(\"***REAL\")\n",
    "                            real = clean_tokens(data.tokenizer_output.decode(y[i][sep:], True))\n",
    "                            #print(real)\n",
    "                            context_list.append(context)\n",
    "                            translation_results.append(completion)\n",
    "                            eval_results.append(real)\n",
    "\n",
    "#                         probs = F.softmax(logits, dim=-1)\n",
    "#                         # sample from the distribution or take the most likely\n",
    "#                         _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "#                         if predicted_tokids is None:\n",
    "#                             predicted_tokids = [predicted]\n",
    "#                             x_total = x\n",
    "#                             y_total = y\n",
    "#                         else:\n",
    "#                             predicted_tokids.append(predicted)\n",
    "#                             x_total = torch.cat((x_total, x), dim=0)\n",
    "#                             y_total = torch.cat((y_total, y), dim=0)\n",
    "                        \n",
    "\n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. mean loss: {float(np.mean(losses)):.5f}. lr {lr:e}\")\n",
    "\n",
    "            if split == 'train':\n",
    "                train_loss = float(np.mean(losses))\n",
    "                print(f\"train loss: {train_loss}\")\n",
    "                return train_loss\n",
    "\n",
    "            if split == 'test':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"test loss: {test_loss}\")\n",
    "                return test_loss\n",
    "\n",
    "            if split == 'valid':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"valid loss: {test_loss}\")\n",
    "\n",
    "#                 eval_results = []\n",
    "#                 translation_results = []\n",
    "#                 context_list = []\n",
    "\n",
    "#                 for idx in range(len(logits_total)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     probs = F.softmax(logits_total[idx], dim=-1)\n",
    "#                     # sample from the distribution or take the most likely\n",
    "#                     _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "#                 for idx in range(len(predicted_tokids)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     predicted = predicted_tokids[idx]\n",
    "#                     print(\"***CONTEXT\")\n",
    "#                     context = clean_tokens(data.tokenizer_input.decode(x_total[idx][:intent - 1], True))\n",
    "#                     print(\"***COMPLETION\")\n",
    "#                     completion = clean_tokens(data.tokenizer_output.decode(predicted[intent:], True))\n",
    "#                     print(\"***REAL\")\n",
    "#                     real = clean_tokens(data.tokenizer_output.decode(y_total[idx][intent:], True))\n",
    "\n",
    "#                     context_list.append(context)\n",
    "#                     translation_results.append(completion)\n",
    "#                     eval_results.append(real)\n",
    "                \n",
    "                with open('valid.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "                with open('eval.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(eval_results))\n",
    "\n",
    "                with open('context.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(context_list))\n",
    "\n",
    "\n",
    "                !cat valid.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > valid.detok.txt\n",
    "                !cat eval.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > eval.detok.txt\n",
    "                !cat context.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > context.detok.txt\n",
    "\n",
    "                with open('eval.detok.txt', 'r') as f:\n",
    "                    eval_results = [l.strip() for l in f.readlines()]\n",
    "                with open('valid.detok.txt', 'r') as f:\n",
    "                    translation_results = [l.strip() for l in f.readlines()]\n",
    "                with open('context.detok.txt', 'r') as f:\n",
    "                    context_list = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#                 idx = choice(range(len(context_list)))\n",
    "                valid_sentences = ['the driver wore a cap and his face was thin and very tanned.',\n",
    "                                   'outside it was getting dark.',\n",
    "                                   'the two girls were asleep.',\n",
    "                                   'I would like to have had the uniform off although I did not care much about the outward forms.',\n",
    "                                   'I watched the flashes on San Gabriele.',\n",
    "                                   'I asked.',\n",
    "                                   '\"no.']\n",
    "\n",
    "                idx_list = [i for i, sentence in enumerate(eval_results) if sentence in valid_sentences]\n",
    "                \n",
    "                for idx in idx_list:\n",
    "                    print(f'Input:            {context_list[idx]}')\n",
    "                    print(f'Predicted output: {translation_results[idx]}')\n",
    "                    print(f'Real output:      {eval_results[idx]}')\n",
    "                    print('--------------------------------------------------')\n",
    "\n",
    "                refs = [eval_results]\n",
    "                sys = translation_results\n",
    "                bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "                print(f'BLEU: {bleu.score}')\n",
    "                print('##############################################################')\n",
    "\n",
    "                return test_loss, bleu.score\n",
    "\n",
    "        train_loss_list = []\n",
    "        test_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        valid_bleu_list = []\n",
    "        best_loss = float('inf')\n",
    "        best_bleu = 0.0\n",
    "        bleu_score = -1.0\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train')\n",
    "            train_loss_list.append(train_loss)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "                test_loss_list.append(test_loss)\n",
    "\n",
    "            if self.valid_dataset is not None:\n",
    "                valid_loss, bleu_score = run_epoch('valid')\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                valid_bleu_list.append(bleu_score)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            # good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            good_model = self.valid_dataset is None or bleu_score > best_bleu\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                best_bleu = bleu_score\n",
    "                self.save_checkpoint(\"_best\")\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.save_checkpoint(f\"_{epoch}\")\n",
    "\n",
    "            self.save_checkpoint(\"_last\")\n",
    "\n",
    "        return train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e044d",
   "metadata": {
    "id": "jKCnnx4-XTg1"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b51b5af",
   "metadata": {
    "id": "Ds0Rr1hrbRDq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size, vocab):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = set(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "        if self.vocab_size != vocab_size:\n",
    "            logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
    "        print(f\"Tokenizer vocab_size={vocab_size} len(vocab)={len(self.vocab)}\")\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "    \n",
    "    def tokenize(self, data, block_size):\n",
    "        tokenized_text = data.split()\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if tokenized in self.vocab:\n",
    "                result.append(tokenized)\n",
    "            else:\n",
    "                logger.warn(f\"Tokenizer UNKNOWN TOKEN: |{tokenized}|\")\n",
    "                result.append('<unk>')\n",
    "\n",
    "        # in case the sentence is longer, than block_size, we trim the sentence\n",
    "        return result[:block_size]\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data, clean_paddings=False):\n",
    "        if hasattr(data, \"shape\") and len(data.shape) > 1:\n",
    "            print(\"WARNING, unexpected data.shape:\", data.shape)\n",
    "            print(data)\n",
    "        text = ' '.join([self.itos[int(i)] for i in data if int(i) >= 0])\n",
    "\n",
    "        if not clean_paddings:\n",
    "            return text\n",
    "        return text.replace('<pad>', '').replace('  ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "263bb205",
   "metadata": {
    "id": "advised-shelter"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-c38ff7f36e88>:7: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size=5500 len(vocab)=10519\n",
      "Tokenizer vocab_size=5500 len(vocab)=10519\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 10000\n",
    "\n",
    "# vocab_input = None\n",
    "# if os.path.exists('vocab_input.pkl'):\n",
    "#     with open('vocab_input.pkl', 'rb') as f:\n",
    "#         vocab_input = pickle.load(f)\n",
    "        \n",
    "# vocab_output = None\n",
    "# if os.path.exists('vocab_output.pkl'):\n",
    "#     with open('vocab_output.pkl', 'rb') as f:\n",
    "#         vocab_output = pickle.load(f)\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
    "tokenizer_input = Tokenizer(text_input, vocab_size, list(joint_vocab))\n",
    "tokenizer_output = Tokenizer(text_output, vocab_size, list(joint_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00aaf3c2",
   "metadata": {
    "id": "7d8f6ac2-0118-4d55-99c0-a478bfa6fe27"
   },
   "outputs": [],
   "source": [
    "# with open('vocab_input.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer_input.vocab, f)\n",
    "\n",
    "# with open('vocab_output.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer_output.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2190eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613611 1613611\n"
     ]
    }
   ],
   "source": [
    "assert len(text_input.splitlines()) == len(text_output.splitlines()), \\\n",
    "   f\"{len(text_input.splitlines())} {len(text_output.splitlines())}\"\n",
    "# assert len(text_lv.splitlines()) == len(text_en.splitlines())\n",
    "# assert len(text_lv.splitlines()) == len(text_input.splitlines())\n",
    "line_idxs = list(range(len(text_input.splitlines())))\n",
    "random.shuffle(line_idxs)\n",
    "print(len(line_idxs), len(text_input.splitlines()))\n",
    "# print(line_idxs[:10], line_idxs[-10:])\n",
    "\n",
    "train_dataset_size = round(0.75 * len(line_idxs))\n",
    "test_dataset_size = round(0.15 * len(line_idxs))\n",
    "valid_dataset_size = round(0.1 * len(line_idxs))\n",
    "\n",
    "train_idxs = line_idxs[:train_dataset_size]\n",
    "test_idxs = line_idxs[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_idxs = line_idxs[-valid_dataset_size:]\n",
    "\n",
    "assert len(train_idxs) + len(valid_idxs) + len(test_idxs) == len(line_idxs)\n",
    "assert set(line_idxs) == set(train_idxs) | set(valid_idxs) | set(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7bac51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 gadus vec@@ s pa@@ vr@@ s atra@@ sts mir@@ is S@@ an@@ f@@ ran@@ c@@ isko liel@@ veik@@ al@@ \n",
      "28 gadus vec@@ s pa@@ vr@@ s , kur nesen pr@@ c@@ l@@ ies uz S@@ an@@ f@@ ran@@ c@@ isko , @@ on\n",
      "1613611\n"
     ]
    }
   ],
   "source": [
    "print(text_input[:200])\n",
    "print(f\"{len(text_input.splitlines())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e24f6d",
   "metadata": {
    "id": "emotional-metadata"
   },
   "outputs": [],
   "source": [
    "# Shuffle texts by lines\n",
    "# texts = list(zip(text_output.splitlines(), text_input.splitlines()))\n",
    "# random.shuffle(texts)\n",
    "# output_texts, input_texts = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a818bacc",
   "metadata": {
    "id": "floral-ridge"
   },
   "outputs": [],
   "source": [
    "# Split texts into train, test and validation datasets\n",
    "# train_dataset_size = round(0.75 * len(output_texts))\n",
    "# test_dataset_size = round(0.15 * len(output_texts))\n",
    "# valid_dataset_size = round(0.1 * len(output_texts))\n",
    "\n",
    "# train_input = input_texts[:train_dataset_size]\n",
    "# test_input = input_texts[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "# valid_input = input_texts[-valid_dataset_size:]\n",
    "\n",
    "# train_output = output_texts[:train_dataset_size]\n",
    "# test_output = output_texts[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "# valid_output = output_texts[-valid_dataset_size:]\n",
    "\n",
    "def separate_lines(text, train_idxs, valid_idxs, test_idxs):\n",
    "    text_lines = text.splitlines()\n",
    "    train_lines = [text_lines[idx] for idx in train_idxs]\n",
    "    valid_lines = [text_lines[idx] for idx in valid_idxs]\n",
    "    test_lines = [text_lines[idx] for idx in test_idxs]\n",
    "    return train_lines, valid_lines, test_lines\n",
    "\n",
    "train_input, valid_input, test_input = separate_lines(text_input, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "train_output, valid_output, test_output = separate_lines(text_output, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "print(len(train_input), len(valid_input), len(test_input))\n",
    "assert len(train_input) == len(train_output)\n",
    "assert len(valid_input) == len(valid_output)\n",
    "assert len(test_input) == len(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(train_idxs[:20])\n",
    "#print(valid_idxs[:20])\n",
    "#print(test_idxs[:20])\n",
    "\n",
    "with open(\"test_set.idxs\", \"w\") as f:\n",
    "    for idx in test_idxs:\n",
    "        f.write(f\"{idx}\\n\")\n",
    "with open(\"valid_set.idxs\", \"w\") as f:\n",
    "    for idx in valid_idxs:\n",
    "        f.write(f\"{idx}\\n\")\n",
    "with open(\"train_set.idxs\", \"w\") as f:\n",
    "    for idx in train_idxs:\n",
    "        f.write(f\"{idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4516fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210208 161361 242042\n",
      "1210208 161361 242042\n",
      "CHECK train_idxs\n",
      "1210207\n",
      "\n",
      "CHECK valid_idxs\n",
      "161360\n",
      "\n",
      "CHECK test_idxs\n",
      "242041\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "train_set = set(train_idxs)\n",
    "valid_set = set(valid_idxs)\n",
    "test_set = set(test_idxs)\n",
    "\n",
    "print(len(train_idxs), len(valid_idxs), len(test_idxs))\n",
    "print(len(train_set), len(valid_set), len(test_set))\n",
    "\n",
    "assert len(train_set) ==  len(train_idxs)\n",
    "assert len(test_set) ==  len(test_idxs)\n",
    "assert len(valid_set) ==  len(valid_idxs)\n",
    "\n",
    "assert len( train_set & valid_set ) == 0\n",
    "assert len( train_set & test_set ) == 0\n",
    "assert len( test_set & valid_set ) == 0\n",
    "\n",
    "print(\"CHECK train_idxs\")\n",
    "input_lines = text_input.splitlines()\n",
    "for i in range(len(train_idxs)):\n",
    "    assert train_input[i] == input_lines[train_idxs[i]], f\"[{i}]:\\n\\t{train_input[i]}\\n\\t{input_lines[train_idxs[i]]}\"\n",
    "print(i)\n",
    "print()\n",
    "print(\"CHECK valid_idxs\")\n",
    "for i in range(len(valid_idxs)):\n",
    "    assert valid_input[i] == input_lines[valid_idxs[i]], f\"[{i}]:\\n\\t{valid_input[i]}\\n\\t{input_lines[valid_idxs[i]]}\"\n",
    "print(i)\n",
    "print()\n",
    "print(\"CHECK test_idxs\")\n",
    "for i in range(len(test_idxs)):\n",
    "    assert test_input[i] == input_lines[test_idxs[i]], f\"[{i}]:\\n\\t{test_input[i]}\\n\\t{input_lines[test_idxs[i]]}\"\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b4a9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/tilde/train2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_input))\n",
    "\n",
    "with open('data/tilde/test2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_input))\n",
    "\n",
    "with open('data/tilde/valid2.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_input))\n",
    "\n",
    "\n",
    "with open('data/tilde/train2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_output))\n",
    "\n",
    "with open('data/tilde/test2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_output))\n",
    "\n",
    "with open('data/tilde/valid2.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d706c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('data/tilde/train2.lv', 'w') as f:\n",
    "#     f.write(\"\\n\".join(train_input))\n",
    "\n",
    "# with open('data/tilde/train2.en', 'w') as f:\n",
    "#     f.write(\"\\n\".join(train_output))\n",
    "\n",
    "\n",
    "assert _eval_limit == 10000\n",
    "\n",
    "with open('data/tilde/test2_10000.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_input[:_eval_limit]))\n",
    "\n",
    "with open('data/tilde/valid2_10000.lv', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_input[:_eval_limit]))\n",
    "\n",
    "\n",
    "with open('data/tilde/test2_10000.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_output[:_eval_limit]))\n",
    "\n",
    "with open('data/tilde/valid2_10000.en', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_output[:_eval_limit]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5faa5bc",
   "metadata": {
    "id": "oZUwkrJeb0p4"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, output_text, input_text, tokenizer_output, tokenizer_input, block_size):\n",
    "        self.tokenizer_output = tokenizer_output\n",
    "        self.tokenizer_input = tokenizer_input\n",
    "\n",
    "        self.block_size = block_size * 2 + 1\n",
    "        self.output_text = [tokenizer_output.tokenize(t, block_size) for t in output_text]\n",
    "        self.input_text = [tokenizer_input.tokenize(t, block_size) for t in input_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The idea is to get the input sentence\n",
    "        and translate it to output sentence (sentences could be on any language).\n",
    "\n",
    "        In the init method we already split a sentence into tokens and filled with spaces,\n",
    "        to have an equal sentence size. In this method we just encode the tokens to\n",
    "        ids (a list of numbers), and we're trying to map ids sequences\n",
    "        \"\"\"\n",
    "\n",
    "        tokenized_input_text = self.tokenizer_input.encode(self.input_text[idx])\n",
    "        tokenized_output_text = self.tokenizer_output.encode(self.output_text[idx])\n",
    "\n",
    "        dix = tokenized_input_text + self.tokenizer_output.encode(['<eos>']) + tokenized_output_text\n",
    "        if len(dix) < self.block_size:\n",
    "            dix += self.tokenizer_output.encode(['<pad>']) * (self.block_size - len(dix))\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        y[:len(tokenized_input_text) - 1] = -100\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27f5f4ef",
   "metadata": {
    "id": "fitted-resident"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ encode Datasets - Start time: 2021-07-19 12:14:21.260499\n",
      "================ encode Datasets - Finished : 2021-07-19 12:15:03.128231 -- elapsed: 0:00:41.867732\n"
     ]
    }
   ],
   "source": [
    "block_size = 100  # the estimate how long lines the text could be (token count)\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Start time: {start_time}\")\n",
    "\n",
    "# for faster debuging of Out of Memory during validation\n",
    "_train_limit = len(train_output)   # 10000  # len(train_output)\n",
    "_eval_limit = 10000   # -1    # 5000\n",
    "\n",
    "train_dataset = WordDataset(train_output[:_train_limit], train_input[:_train_limit],\n",
    "                            tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "if _eval_limit > 0:\n",
    "    test_dataset = WordDataset(test_output[:_eval_limit], test_input[:_eval_limit],\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output[:_eval_limit], valid_input[:_eval_limit],\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "else:\n",
    "    test_dataset = WordDataset(test_output, test_input,\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output, valid_input,\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "finish_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Finished : {finish_time} -- elapsed: {finish_time-start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8f53dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1210208"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: fixed, no longer shows UNKNOWN TOKEN\n",
    "\n",
    "# joint_vocab -s 10000\n",
    "# UNKNOWN TOKEN\n",
    "\n",
    "# |;@@| (2040)  # I &@@ apos@@ ;@@ m\n",
    "# |q@@| (148)\n",
    "# |R| (40)\n",
    "# |v| (409)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0b3a1fd",
   "metadata": {
    "id": "ruled-astronomy"
   },
   "outputs": [],
   "source": [
    "number_of_heads = 8\n",
    "number_of_layers = 6\n",
    "\n",
    "# from mingpt.model import GPT, GPTConfig\n",
    "embd_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "\n",
    "max_vocab = max(tokenizer_input.vocab_size, tokenizer_output.vocab_size)\n",
    "mconf = GPTConfig(max_vocab, train_dataset.block_size,\n",
    "                  n_layer=number_of_layers, n_head=number_of_heads, n_embd=512,\n",
    "                  embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop)\n",
    "\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "524cf885",
   "metadata": {
    "id": "bUgb5MxODXBc"
   },
   "outputs": [],
   "source": [
    "# from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "tokens_per_epoch = len(train_dataset) * block_size\n",
    "train_epochs = 100\n",
    "_batch_size = 128\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=train_epochs, \n",
    "                      batch_size=_batch_size, learning_rate=3e-4,\n",
    "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "                      ckpt_path='minGPT-Tilde-LV-EN-translator_model',\n",
    "                      num_workers=1, weight_decay=0.0001, betas=(0.9, 0.98))\n",
    "trainer = Trainer(model, train_dataset, test_dataset, valid_dataset, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea5c2cab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPH3nCe9JB63",
    "outputId": "382f135d-e133-4e0b-9bb5-2f7b01269cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters count: 29789696\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(f'Parameters count: {param_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdeb1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters count: 28628480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca3d9eb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG17yBsSf3uc",
    "outputId": "ca906c3a-1793-4ab9-84c3-3f6c5f59c37d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9455 [00:00<?, ?it/s]/home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 9454: train loss 0.37617. mean loss: 0.69961. lr 2.999637e-04: 100%|| 9455/9455 [40:53<00:00,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.6996148201601017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.36083900023110305\n",
      "valid loss: 0.3672173690946796\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 29.023342985987902\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 9454: train loss 0.28366. mean loss: 0.34594. lr 2.995702e-04: 100%|| 9455/9455 [40:05<00:00,  3.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.34593802052537703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.28575764124906516\n",
      "valid loss: 0.28697750821143764\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 36.45319734511089\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 iter 9454: train loss 0.27194. mean loss: 0.29497. lr 2.987451e-04: 100%|| 9455/9455 [40:15<00:00,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2949696494876109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.26050413767748243\n",
      "valid loss: 0.2612633097775375\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 39.23990755373973\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 iter 9454: train loss 0.29803. mean loss: 0.27224. lr 2.974907e-04: 100%|| 9455/9455 [40:16<00:00,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2722373852633346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2475800348233573\n",
      "valid loss: 0.2479765484981899\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 40.65377349379643\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 iter 9454: train loss 0.24641. mean loss: 0.25836. lr 2.958108e-04: 100%|| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.25836024345196723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.23765818001348762\n",
      "valid loss: 0.2390122851238975\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 41.7472891931476\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 iter 9454: train loss 0.24769. mean loss: 0.24861. lr 2.937100e-04: 100%|| 9455/9455 [40:22<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24861045683867586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.232530456177796\n",
      "valid loss: 0.23358417311801186\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 42.45037871859485\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 iter 9454: train loss 0.27818. mean loss: 0.24121. lr 2.911946e-04: 100%|| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24120904027564635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22838797769214533\n",
      "valid loss: 0.22867248009277297\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 43.07260916009157\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 iter 9454: train loss 0.26262. mean loss: 0.23528. lr 2.882717e-04: 100%|| 9455/9455 [40:14<00:00,  3.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.23527597282564494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22521848580505274\n",
      "valid loss: 0.22536798613735393\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 43.65638135527434\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 iter 9454: train loss 0.24683. mean loss: 0.23033. lr 2.849498e-04: 100%|| 9455/9455 [40:49<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2303313877013926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.22130452568017983\n",
      "valid loss: 0.22205081242549268\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.12289428224524\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10 iter 9454: train loss 0.22590. mean loss: 0.22611. lr 2.812385e-04: 100%|| 9455/9455 [40:27<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2261057166685067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21927649612668193\n",
      "valid loss: 0.21927647613271883\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.41774529250004\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11 iter 9454: train loss 0.21838. mean loss: 0.22241. lr 2.771485e-04: 100%|| 9455/9455 [40:14<00:00,  3.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2224081448686621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2157665193080902\n",
      "valid loss: 0.2173823997189727\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 44.74593714633582\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12 iter 9454: train loss 0.20836. mean loss: 0.21917. lr 2.726915e-04: 100%|| 9455/9455 [40:41<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2191733108601956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21537618395648425\n",
      "valid loss: 0.2158568824016595\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.00463234087414\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13 iter 9454: train loss 0.20313. mean loss: 0.21626. lr 2.678805e-04: 100%|| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21625527119365462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21373417615136014\n",
      "valid loss: 0.21526709632782998\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.269307256196036\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14 iter 9454: train loss 0.19538. mean loss: 0.21363. lr 2.627293e-04: 100%|| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21363016688306768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21166858850400658\n",
      "valid loss: 0.21385281617882884\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.41680600923591\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15 iter 9454: train loss 0.22387. mean loss: 0.21117. lr 2.572529e-04: 100%|| 9455/9455 [40:28<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21116990949948714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.21177052640462224\n",
      "valid loss: 0.21240954553779168\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.59820519734781\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16 iter 9454: train loss 0.21955. mean loss: 0.20891. lr 2.514669e-04: 100%|| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20890747796663336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20916303897960276\n",
      "valid loss: 0.21087813735762728\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.74848845976228\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17 iter 9454: train loss 0.22276. mean loss: 0.20676. lr 2.453881e-04: 100%|| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20675669054326906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20803766782525218\n",
      "valid loss: 0.21013710638390312\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 45.943301461503374\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18 iter 9454: train loss 0.18144. mean loss: 0.20483. lr 2.390341e-04: 100%|| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2048345522424142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2080847983118854\n",
      "valid loss: 0.20898202495484414\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.09281382821358\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19 iter 9454: train loss 0.20288. mean loss: 0.20297. lr 2.324231e-04: 100%|| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20296576087087756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.207252168202702\n",
      "valid loss: 0.20917992859701567\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.153103416634124\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20 iter 9454: train loss 0.21421. mean loss: 0.20121. lr 2.255743e-04: 100%|| 9455/9455 [40:30<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2012118115619274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20678809643546237\n",
      "valid loss: 0.20792522818981846\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.406856579350915\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21 iter 9454: train loss 0.16354. mean loss: 0.19950. lr 2.185074e-04: 100%|| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19949543326226318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20507498575916774\n",
      "valid loss: 0.2068335598782648\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.628576311842565\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22 iter 9454: train loss 0.20187. mean loss: 0.19793. lr 2.112428e-04: 100%|| 9455/9455 [40:35<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19792634532887868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20490934690342674\n",
      "valid loss: 0.2065088828153248\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.66234109392401\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 23 iter 9454: train loss 0.18201. mean loss: 0.19637. lr 2.038015e-04: 100%|| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19637382556243527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2043918065632446\n",
      "valid loss: 0.2047792848529695\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.7183603453369\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24 iter 9454: train loss 0.19402. mean loss: 0.19490. lr 1.962049e-04: 100%|| 9455/9455 [40:45<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19490345557614774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20384961781622488\n",
      "valid loss: 0.20556440704231022\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.8894360556575\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 25 iter 9454: train loss 0.17490. mean loss: 0.19347. lr 1.884750e-04: 100%|| 9455/9455 [40:28<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19346737188835234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2050862210460856\n",
      "valid loss: 0.20475829854796204\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 46.88064001940165\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26 iter 9454: train loss 0.20170. mean loss: 0.19209. lr 1.806340e-04: 100%|| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19208716121790334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2030955924640728\n",
      "valid loss: 0.20354613605179364\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.109954288906174\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 27 iter 9454: train loss 0.15688. mean loss: 0.19080. lr 1.727047e-04: 100%|| 9455/9455 [40:26<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19080226524427163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20222395696217502\n",
      "valid loss: 0.2042009289128871\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.27382218675211\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 28 iter 9454: train loss 0.19092. mean loss: 0.18950. lr 1.647098e-04: 100%|| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18949553765581123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20100688934326172\n",
      "valid loss: 0.2018579433235941\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.23330071376662\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29 iter 9454: train loss 0.20935. mean loss: 0.18825. lr 1.566725e-04: 100%|| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18825023457494633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2010484296309797\n",
      "valid loss: 0.20340422550334206\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.409370359589175\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30 iter 9454: train loss 0.18692. mean loss: 0.18699. lr 1.486160e-04: 100%|| 9455/9455 [40:39<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1869891067997354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2011563600618628\n",
      "valid loss: 0.20210107538519026\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.560529580980116\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31 iter 9454: train loss 0.18474. mean loss: 0.18582. lr 1.405634e-04: 100%|| 9455/9455 [40:35<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18581868690009598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2004196458979498\n",
      "valid loss: 0.2022080445968652\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.588476973404354\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 32 iter 9454: train loss 0.18383. mean loss: 0.18469. lr 1.325381e-04: 100%|| 9455/9455 [40:44<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1846891991002165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1996266534434089\n",
      "valid loss: 0.20358680679073818\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.661625183435305\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 33 iter 9454: train loss 0.19077. mean loss: 0.18353. lr 1.245631e-04: 100%|| 9455/9455 [40:39<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.183534984508877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19887856818452665\n",
      "valid loss: 0.20092384743539593\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.73602168591616\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 34 iter 9454: train loss 0.17051. mean loss: 0.18246. lr 1.166616e-04: 100%|| 9455/9455 [40:33<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1824570844313381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1995796736659883\n",
      "valid loss: 0.20057060956200468\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.86665113728899\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 35 iter 9454: train loss 0.20175. mean loss: 0.18137. lr 1.088562e-04: 100%|| 9455/9455 [40:38<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18136992486679385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19864992457854597\n",
      "valid loss: 0.20065974283821975\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.88581977204235\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 36 iter 9454: train loss 0.19712. mean loss: 0.18034. lr 1.011696e-04: 100%|| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18034425895091782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19923306586621684\n",
      "valid loss: 0.1994970382391652\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.02032018617099\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 37 iter 9454: train loss 0.16863. mean loss: 0.17932. lr 9.362393e-05: 100%|| 9455/9455 [40:42<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17932144434032207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19972689393200452\n",
      "valid loss: 0.20203917501847954\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.123408130312036\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 38 iter 9454: train loss 0.18200. mean loss: 0.17840. lr 8.624092e-05: 100%|| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1783985232452688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19790536768828768\n",
      "valid loss: 0.2002376178397408\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.13085537766696\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 39 iter 9454: train loss 0.17471. mean loss: 0.17744. lr 7.904189e-05: 100%|| 9455/9455 [40:34<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17744279825800882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19784491424319112\n",
      "valid loss: 0.2005579250900051\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.277171792616514\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 40 iter 9454: train loss 0.16865. mean loss: 0.17654. lr 7.204763e-05: 100%|| 9455/9455 [42:55<00:00,  3.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17654110790559818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1980528703218774\n",
      "valid loss: 0.1986544368010533\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.31917366907318\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41 iter 9454: train loss 0.18227. mean loss: 0.17573. lr 6.527832e-05: 100%|| 9455/9455 [48:10<00:00,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17573457686816618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19793730427192735\n",
      "valid loss: 0.19966118282909634\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.395523990694976\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42 iter 9454: train loss 0.17776. mean loss: 0.17487. lr 5.875349e-05: 100%|| 9455/9455 [41:09<00:00,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17486677899964584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19823104633560665\n",
      "valid loss: 0.19842650524423092\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.45918003592048\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43 iter 9454: train loss 0.15216. mean loss: 0.17407. lr 5.249197e-05: 100%|| 9455/9455 [49:23<00:00,  3.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1740701851581279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19738954437684408\n",
      "valid loss: 0.19837068266506436\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.463979358418534\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 44 iter 9454: train loss 0.16014. mean loss: 0.17329. lr 4.651183e-05: 100%|| 9455/9455 [44:45<00:00,  3.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17329278724777328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19883684493318388\n",
      "valid loss: 0.1986763286816923\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.49418865821421\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 45 iter 9454: train loss 0.18177. mean loss: 0.17261. lr 4.083033e-05: 100%|| 9455/9455 [45:44<00:00,  3.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1726077829235953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1967835024565081\n",
      "valid loss: 0.1982685452020621\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.53441302345551\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 46 iter 9454: train loss 0.17749. mean loss: 0.17192. lr 3.546385e-05: 100%|| 9455/9455 [49:01<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17192277030173211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19700185803672934\n",
      "valid loss: 0.19870617042613936\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.63323423462656\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 47 iter 9454: train loss 0.14104. mean loss: 0.17129. lr 3.042790e-05: 100%|| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1712914384810903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19920070186445984\n",
      "valid loss: 0.19794500553155248\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.602968233400496\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 48 iter 9454: train loss 0.20204. mean loss: 0.17076. lr 3.000000e-05: 100%|| 9455/9455 [49:05<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17075890384058925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1962979328406008\n",
      "valid loss: 0.19930218726019316\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66759613322437\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 49 iter 9454: train loss 0.18914. mean loss: 0.17063. lr 3.000000e-05: 100%|| 9455/9455 [49:06<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1706344845905183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19695725414571882\n",
      "valid loss: 0.19833394526680814\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66008366042197\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 50 iter 9454: train loss 0.19319. mean loss: 0.17053. lr 3.000000e-05: 100%|| 9455/9455 [48:58<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17052941837051064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19821986424017557\n",
      "valid loss: 0.1983115865459925\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67672420109884\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 51 iter 9454: train loss 0.16709. mean loss: 0.17043. lr 3.000000e-05: 100%|| 9455/9455 [49:03<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17042742783291387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19660844493515883\n",
      "valid loss: 0.1980167914040481\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67497569816444\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 52 iter 9454: train loss 0.14669. mean loss: 0.17029. lr 3.000000e-05: 100%|| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17029285709625322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19613911400112924\n",
      "valid loss: 0.19867383434048183\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.669754173433546\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 53 iter 9454: train loss 0.18860. mean loss: 0.17019. lr 3.000000e-05: 100%|| 9455/9455 [49:01<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17018792826321819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1963504245386848\n",
      "valid loss: 0.19885536179512361\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.633725855044034\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 54 iter 9454: train loss 0.19103. mean loss: 0.17009. lr 3.000000e-05: 100%|| 9455/9455 [48:54<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1700896316777771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19719673655455625\n",
      "valid loss: 0.19910381168504304\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.678039226953736\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 55 iter 9454: train loss 0.16854. mean loss: 0.16998. lr 3.000000e-05: 100%|| 9455/9455 [48:51<00:00,  3.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16997740764283176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19753534261938893\n",
      "valid loss: 0.1987545712839199\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.671821181483054\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 56 iter 9454: train loss 0.15969. mean loss: 0.16990. lr 3.000000e-05: 100%|| 9455/9455 [49:09<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16990491046217118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19853379002100305\n",
      "valid loss: 0.19891305418708657\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.68985469988521\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 57 iter 9454: train loss 0.19053. mean loss: 0.16982. lr 3.000000e-05: 100%|| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16981655132953355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1975858030439932\n",
      "valid loss: 0.19993347684039345\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.6112342089157\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 58 iter 9454: train loss 0.15873. mean loss: 0.16972. lr 3.000000e-05: 100%|| 9455/9455 [49:07<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1697181096000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19676471586468852\n",
      "valid loss: 0.19872604235063626\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.68102753441791\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 59 iter 9454: train loss 0.15122. mean loss: 0.16964. lr 3.000000e-05: 100%|| 9455/9455 [48:59<00:00,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16963610662032158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19659348963936674\n",
      "valid loss: 0.19825328452677665\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.69478228381596\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 60 iter 9454: train loss 0.17749. mean loss: 0.16955. lr 3.000000e-05: 100%|| 9455/9455 [49:07<00:00,  3.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1695509258352342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1966380807040613\n",
      "valid loss: 0.19968913797336288\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.80495715291374\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 61 iter 9454: train loss 0.17364. mean loss: 0.16949. lr 3.000000e-05: 100%|| 9455/9455 [41:09<00:00,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16949200037923962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.197903176463103\n",
      "valid loss: 0.19911990418464323\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.77611517190088\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 62 iter 9454: train loss 0.18548. mean loss: 0.16941. lr 3.000000e-05: 100%|| 9455/9455 [40:27<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1694119602709709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19700280850446678\n",
      "valid loss: 0.19770609974106657\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.73129038408612\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 63 iter 9454: train loss 0.19856. mean loss: 0.16932. lr 3.000000e-05: 100%|| 9455/9455 [40:22<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1693200916122785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19739639212059068\n",
      "valid loss: 0.19915987060794346\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.69322224466296\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 64 iter 9454: train loss 0.16781. mean loss: 0.16928. lr 3.000000e-05: 100%|| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16927531727450287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19742316384858724\n",
      "valid loss: 0.1974438459058351\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.694955531176085\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 65 iter 9454: train loss 0.17940. mean loss: 0.16921. lr 3.000000e-05: 100%|| 9455/9455 [40:41<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16920688348447502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1977805968704103\n",
      "valid loss: 0.19841452565374254\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.66952644559219\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 66 iter 9454: train loss 0.16454. mean loss: 0.16913. lr 3.000000e-05: 100%|| 9455/9455 [40:24<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1691279811432978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19820142565648766\n",
      "valid loss: 0.19872611289537406\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.75508320564905\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 67 iter 9454: train loss 0.15356. mean loss: 0.16907. lr 3.000000e-05: 100%|| 9455/9455 [40:27<00:00,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16907428263877197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19878603838667086\n",
      "valid loss: 0.19846681477148323\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.69902080717255\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 68 iter 9454: train loss 0.17852. mean loss: 0.16900. lr 3.000000e-05: 100%|| 9455/9455 [40:30<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16899603326001664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19722337586970268\n",
      "valid loss: 0.19716574911829793\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.72282505274609\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 69 iter 9454: train loss 0.17666. mean loss: 0.16895. lr 3.000000e-05: 100%|| 9455/9455 [40:42<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16895140435867748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19724931596200676\n",
      "valid loss: 0.19788130438780482\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.694214787543046\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 70 iter 9454: train loss 0.16366. mean loss: 0.16886. lr 3.000000e-05: 100%|| 9455/9455 [40:37<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16885569068346623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19807158353962476\n",
      "valid loss: 0.19866539915151235\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67482783960515\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 71 iter 9454: train loss 0.14708. mean loss: 0.16880. lr 3.000000e-05: 100%|| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1687989980904341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19863804288302797\n",
      "valid loss: 0.19871157634107373\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.75267128174804\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 72 iter 9454: train loss 0.18545. mean loss: 0.16888. lr 3.478137e-05: 100%|| 9455/9455 [40:48<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16888382901973917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1970400936618636\n",
      "valid loss: 0.1990582248832606\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.77091486473016\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 73 iter 9454: train loss 0.16607. mean loss: 0.16933. lr 4.010518e-05: 100%|| 9455/9455 [40:40<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16932679753827393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19701051315929316\n",
      "valid loss: 0.19844734027415892\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.71952830943483\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 74 iter 9454: train loss 0.17592. mean loss: 0.16978. lr 4.574611e-05: 100%|| 9455/9455 [40:52<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16978250025662714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19702990202209616\n",
      "valid loss: 0.1986257650806934\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.67154966305596\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 75 iter 9454: train loss 0.15324. mean loss: 0.17027. lr 5.168789e-05: 100%|| 9455/9455 [40:32<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17026908665911222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1970403081254114\n",
      "valid loss: 0.19885391657111012\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.65970051180914\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 76 iter 9454: train loss 0.17562. mean loss: 0.17086. lr 5.791337e-05: 100%|| 9455/9455 [40:48<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1708561425444501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19795357992377463\n",
      "valid loss: 0.1994199643406687\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.50685082254446\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 77 iter 9454: train loss 0.16062. mean loss: 0.17144. lr 6.440458e-05: 100%|| 9455/9455 [40:39<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17143972114650488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1991728174535534\n",
      "valid loss: 0.20112929785553413\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.576434063098034\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 78 iter 9454: train loss 0.16938. mean loss: 0.17204. lr 7.114279e-05: 100%|| 9455/9455 [40:38<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17204295803099823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.198777608101881\n",
      "valid loss: 0.20105428126039385\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.388325429701375\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 79 iter 9454: train loss 0.19931. mean loss: 0.17270. lr 7.810857e-05: 100%|| 9455/9455 [40:42<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17269757152832985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19935764485522162\n",
      "valid loss: 0.2001549912781655\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.46197795570353\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 80 iter 9454: train loss 0.16519. mean loss: 0.17332. lr 8.528180e-05: 100%|| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17332037286916654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19935164025312738\n",
      "valid loss: 0.20039620731450333\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.394469102833625\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 81 iter 9454: train loss 0.16398. mean loss: 0.17404. lr 9.264179e-05: 100%|| 9455/9455 [40:32<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17403672320511152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19996607020685944\n",
      "valid loss: 0.1999004967982256\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.34497538940401\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 82 iter 9454: train loss 0.16953. mean loss: 0.17468. lr 1.001673e-04: 100%|| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17467848808440123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19998389256151416\n",
      "valid loss: 0.2004095364975024\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.328365736030136\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 83 iter 9454: train loss 0.17688. mean loss: 0.17543. lr 1.078366e-04: 100%|| 9455/9455 [40:49<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17542714966566458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19928333823439442\n",
      "valid loss: 0.20070798827123038\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.25512303314671\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 84 iter 9454: train loss 0.14551. mean loss: 0.17606. lr 1.156276e-04: 100%|| 9455/9455 [40:35<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.176057551307126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19937179341346403\n",
      "valid loss: 0.2012943697126606\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.267756170799515\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 85 iter 9454: train loss 0.14598. mean loss: 0.17677. lr 1.235178e-04: 100%|| 9455/9455 [40:36<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17677004064030902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19948889616923995\n",
      "valid loss: 0.20097028794167918\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.1103120350795\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 86 iter 9454: train loss 0.17430. mean loss: 0.17749. lr 1.314843e-04: 100%|| 9455/9455 [40:46<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17749227563235445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.19963548172123824\n",
      "valid loss: 0.2004886892022966\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.06296938168104\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 87 iter 9454: train loss 0.17235. mean loss: 0.17818. lr 1.395044e-04: 100%|| 9455/9455 [40:31<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17818400097202208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20057790528369857\n",
      "valid loss: 0.20243212976787664\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.05716546219544\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 88 iter 9454: train loss 0.18126. mean loss: 0.17888. lr 1.475547e-04: 100%|| 9455/9455 [40:33<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17887666114797546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20142068968543522\n",
      "valid loss: 0.2021064054739626\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.98130348808379\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 89 iter 9454: train loss 0.15231. mean loss: 0.17952. lr 1.556120e-04: 100%|| 9455/9455 [40:29<00:00,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17952325096021943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2009637961282006\n",
      "valid loss: 0.20178766020491154\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 48.02709045658239\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 90 iter 9454: train loss 0.19307. mean loss: 0.18014. lr 1.636532e-04: 100%|| 9455/9455 [40:41<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1801365806683228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20121427691435512\n",
      "valid loss: 0.2021710883967484\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.9492516730295\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 91 iter 9454: train loss 0.16281. mean loss: 0.18075. lr 1.716550e-04: 100%|| 9455/9455 [40:50<00:00,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1807451027322862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20152146831343445\n",
      "valid loss: 0.20421527024311356\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.79275772477171\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 92 iter 9454: train loss 0.16926. mean loss: 0.18136. lr 1.795943e-04: 100%|| 9455/9455 [40:40<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18135790154298484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2012810803289655\n",
      "valid loss: 0.20241985102243062\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.87051795395237\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 93 iter 9454: train loss 0.19372. mean loss: 0.18189. lr 1.874481e-04: 100%|| 9455/9455 [40:33<00:00,  3.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18188677887116583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20202962111068679\n",
      "valid loss: 0.20348373508151574\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.79933448355385\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 94 iter 9454: train loss 0.18075. mean loss: 0.18242. lr 1.951940e-04: 100%|| 9455/9455 [40:42<00:00,  3.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18242199795230996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20235751663582235\n",
      "valid loss: 0.20378367527376248\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.729281126161595\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 95 iter 9454: train loss 0.20112. mean loss: 0.18285. lr 2.028094e-04: 100%|| 9455/9455 [48:19<00:00,  3.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18285113244949353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20127036186713207\n",
      "valid loss: 0.20184963073911547\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.58523074831814\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 96 iter 9454: train loss 0.19049. mean loss: 0.18330. lr 2.102724e-04: 100%|| 9455/9455 [49:13<00:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18330140532418449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20173621347433404\n",
      "valid loss: 0.20424867619442033\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.61227113404924\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 97 iter 9454: train loss 0.17060. mean loss: 0.18368. lr 2.175614e-04: 100%|| 9455/9455 [49:10<00:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1836807532451696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20253652296488797\n",
      "valid loss: 0.20350500846965403\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.73258574655407\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 98 iter 9454: train loss 0.20826. mean loss: 0.18401. lr 2.246556e-04: 100%|| 9455/9455 [49:23<00:00,  3.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18401224735330363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20398859604250028\n",
      "valid loss: 0.2038825695650487\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.51812430259157\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 99 iter 9454: train loss 0.17610. mean loss: 0.18428. lr 2.315342e-04: 100%|| 9455/9455 [49:34<00:00,  3.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18428413553851927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20255496154857588\n",
      "valid loss: 0.2034222972166689\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.53716286367059\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 100 iter 9454: train loss 0.16800. mean loss: 0.18453. lr 2.381776e-04: 100%|| 9455/9455 [49:15<00:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18452731620408064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.20424895290332504\n",
      "valid loss: 0.20371557772159576\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "BLEU: 47.603698119136624\n",
      "##############################################################\n"
     ]
    }
   ],
   "source": [
    "train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0bda4532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "EHFUYpv8P9lr",
    "outputId": "77da5c3a-eeaa-4132-abe1-2de95323a3e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-6ad795e9aedc>:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "epochs = range(len(test_loss_list))\n",
    "# plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(20, 10))\n",
    "axs[0].plot(epochs, train_loss_list)\n",
    "axs[0].plot(epochs, test_loss_list)\n",
    "axs[0].plot(epochs, valid_loss_list)\n",
    "axs[0].set_title('Train vs Valid/Test loss')\n",
    "# axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(epochs, test_loss_list)\n",
    "axs[1].set_title('Validation & Test losses')\n",
    "# axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(epochs, valid_loss_list)\n",
    "#axs[1].set_title('Validation loss')\n",
    "#axs[1].set_xlabel('Epochs')\n",
    "#axs[1].set_ylabel('Loss')\n",
    "\n",
    "axs[2].plot(epochs, valid_bleu_list)\n",
    "axs[2].set_title('Validation BLEU')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('BLEU')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7187fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"tilde_losses.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8870ce",
   "metadata": {
    "id": "1Og-0W-MtKoM"
   },
   "source": [
    "#Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3e90caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6996148201601017, 0.34593802052537703, 0.2949696494876109, 0.2722373852633346, 0.25836024345196723, 0.24861045683867586, 0.24120904027564635, 0.23527597282564494, 0.2303313877013926, 0.2261057166685067, 0.2224081448686621, 0.2191733108601956, 0.21625527119365462, 0.21363016688306768, 0.21116990949948714, 0.20890747796663336, 0.20675669054326906, 0.2048345522424142, 0.20296576087087756, 0.2012118115619274, 0.19949543326226318, 0.19792634532887868, 0.19637382556243527, 0.19490345557614774, 0.19346737188835234, 0.19208716121790334, 0.19080226524427163, 0.18949553765581123, 0.18825023457494633, 0.1869891067997354, 0.18581868690009598, 0.1846891991002165, 0.183534984508877, 0.1824570844313381, 0.18136992486679385, 0.18034425895091782, 0.17932144434032207, 0.1783985232452688, 0.17744279825800882, 0.17654110790559818, 0.17573457686816618, 0.17486677899964584, 0.1740701851581279, 0.17329278724777328, 0.1726077829235953, 0.17192277030173211, 0.1712914384810903, 0.17075890384058925, 0.1706344845905183, 0.17052941837051064, 0.17042742783291387, 0.17029285709625322, 0.17018792826321819, 0.1700896316777771, 0.16997740764283176, 0.16990491046217118, 0.16981655132953355, 0.1697181096000056, 0.16963610662032158, 0.1695509258352342, 0.16949200037923962, 0.1694119602709709, 0.1693200916122785, 0.16927531727450287, 0.16920688348447502, 0.1691279811432978, 0.16907428263877197, 0.16899603326001664, 0.16895140435867748, 0.16885569068346623, 0.1687989980904341, 0.16888382901973917, 0.16932679753827393, 0.16978250025662714, 0.17026908665911222, 0.1708561425444501, 0.17143972114650488, 0.17204295803099823, 0.17269757152832985, 0.17332037286916654, 0.17403672320511152, 0.17467848808440123, 0.17542714966566458, 0.176057551307126, 0.17677004064030902, 0.17749227563235445, 0.17818400097202208, 0.17887666114797546, 0.17952325096021943, 0.1801365806683228, 0.1807451027322862, 0.18135790154298484, 0.18188677887116583, 0.18242199795230996, 0.18285113244949353, 0.18330140532418449, 0.1836807532451696, 0.18401224735330363, 0.18428413553851927, 0.18452731620408064]\n",
      "\n",
      "Max BLEU: [59] 48.80495715291374\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_list)\n",
    "print()\n",
    "max_bleu = max(valid_bleu_list)\n",
    "best_bleu_epoch = valid_bleu_list.index(max_bleu)\n",
    "print(f\"Max BLEU: [{best_bleu_epoch}] {max_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c2b248e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d029ed9c-9170-435a-805f-416774999287",
    "outputId": "78ebc7df-beec-45fa-eddf-cc1829c07374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('minGPT-Tilde-LV-EN-translator_model_best.pt')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49bad596",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tilde_train_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in train_loss_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38e5d7e6",
   "metadata": {
    "id": "afC6uEVGXIPL"
   },
   "outputs": [],
   "source": [
    "with open('tilde_test_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in test_loss_list]))\n",
    "\n",
    "with open('tilde_valid_loss.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in valid_loss_list]))\n",
    "\n",
    "with open('tilde_valid_bleu.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(s) for s in valid_bleu_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb18a48d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "greek-travel",
    "outputId": "8b5ee427-7a71-46ab-e62c-a8730ac37bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:            im zio@@ jumam var sek@@ ot tiesbu akta prieklikums .\n",
      "Predicted output: this report may be followed by a legislative proposal . \n",
      "Real output:      this report may be accompanied by a legislative proposal .\n",
      "--------------------------------------------------\n",
      "Input:            vii pa@@ met savu valsti , jo taj nav kar@@ jer@@ as iespju , piemrotu al@@ gu vai ldzeku pt@@ niecbai , ptniecbas centr@@ u dur@@ vis ir slg@@ tas , jo tiem trkst gan ldzeku , gan organizcijas , lai uzem@@ tu jaunas grupas un attst@@ tu jaunas ide@@ jas .\n",
      "Predicted output: they leave their country because they do not have the care@@ er option , apply the sal@@ ary or the funds for research , the do@@ or of research centres is closed because they lack both resources and organisations to take up new groups and develop new ideas .\n",
      "Real output:      they leave because there are no care@@ er pro@@ spects , suitable sal@@ aries or funds for research , and the do@@ ors to research centres are bar@@ red because , as well as la@@ c@@ king funds , they lack the organisation to receive new groups and develop new ideas .\n",
      "--------------------------------------------------\n",
      "Input:            man grib@@ tos tic@@ t , ka komisrs A. Pie@@ bal@@ gs cent@@ sies nodroint to , ka Komisija nosta man informciju par specif@@ iskajiem jautjumiem , kurus esmu uzdev@@ is .\n",
      "Predicted output: I would like to believe that Commissioner Pie@@ bal@@ gs will try to ensure that the Commission s@@ ends me information on the specific issues that I have raised . \n",
      "Real output:      I would like to believe that Commissioner Pie@@ bal@@ gs will interven@@ e to ensure that the Commission s@@ ends me the information on the specific questions I have asked .\n",
      "--------------------------------------------------\n",
      "Input:            Komisija atzm , ka ir skaidri redz@@ ams - AR@@ G@@ O programma 2003. gad mrus nav sasnieg@@ usi .\n",
      "Predicted output: the Commission notes that it is clear that the objectives of the AR@@ G@@ O programme in 2003 have not been achieved .\n",
      "Real output:      in this report the Commission noted that for 2003 there is a clear ind@@ ication of the AR@@ G@@ O programme having under @-@ perform@@ ed .\n",
      "--------------------------------------------------\n",
      "Input:            ( 8 ) T@@ d@@  ir btiski Kopien saska@@ ot noteikumus par vielu un ma@@ is@@ jumu klas@@ ific@@ anu un mar@@ anu un to kritrijus , pilnb emot vr G@@ H@@ S klas@@ ific@@ anas kritrijus un mar@@ anas noteikumus , k ar balstoties uz 40 gadus ilg@@ o pieredzi , stenojot eso@@ os Kopienas tiesbu aktus m@@ isko vielu jom , un saglab@@ jot aizsardzbas lmeni , ko nodroina klas@@ ific@@ anas un mar@@ anas saska@@ oanas sistmas izmantoana , emot vr Kopienas bst@@ ambas kla@@ ses , kas vl nav iekau@@ tas G@@ H@@ S , k ar eso@@ os mar@@ anas un iepa@@ ko@@ anas noteikumus .\n",
      "Predicted output: ( 8 ) I@@ t is therefore essential to harmon@@ ise Community provisions on the class@@ ification and labelling of substances and m@@ ix@@ t@@ ures , taking full account of the criteria and labelling of G@@ H@@ S class@@ ification , and building on the 40 years of experience of the implementation of existing Community chem@@ ic@@ als legislation , and to maintain the level of protection affor@@ ded by the use of the system of harmonisation of class@@ ification and labelling , taking into account the haz@@ ards of the Community cla@@ ss which are not yet\n",
      "Real output:      ( 8 ) T@@ he@@ re@@ fore it is essential to harmon@@ ise the provisions and criteria for the class@@ ification and labelling of substances and m@@ ix@@ t@@ ures within the Community , taking into full account the class@@ ification criteria and labelling rules of the G@@ H@@ S , but also by building on the 40 years of experience obtained through implementation of existing Community chem@@ ic@@ als legislation and maintaining the level of protection achieved through the system of harmonisation of class@@ ification and labelling , through Community haz@@ ard cla@@ sses not yet part of the G@@ H@@ S as well as through current labelling and pack@@ aging rules .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = choice(range(len(valid_output)))\n",
    "\n",
    "    context = valid_input[idx]\n",
    "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
    "\n",
    "    intent = len(encoded_input) + 1\n",
    "\n",
    "    predicted = y[intent:]\n",
    "    completion = tokenizer_output.decode(predicted, True)\n",
    "    print(f'Input:            {context}')\n",
    "    print(f'Predicted output: {completion}')\n",
    "    print(f'Real output:      {valid_output[idx]}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32d871fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF3lesDzM_UY",
    "outputId": "8d15a610-69b9-47c9-c296-425e51f35e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:            III pielikuma apak@@ vir@@ s@@ rakst@@ a &quot; Ne@@ tie@@ ie paskumi &quot; 6. punkts\n",
      "Predicted output: Annex III , sub@@ t@@ itle &apos; In@@ direct actions &apos; , paragraph 6 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Real output:      Annex III , sub@@ t@@ itle &quot; In@@ direct A@@ ctions , &quot; paragraph 6\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx = choice(range(len(valid_output)))\n",
    "\n",
    "context = valid_input[idx]\n",
    "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
    "\n",
    "intent = len(encoded_input) + 1\n",
    "\n",
    "predicted = y[0][intent:]\n",
    "completion = tokenizer_output.decode(predicted,)\n",
    "print(f'Input:            {context}')\n",
    "print(f'Predicted output: {completion}')\n",
    "print(f'Real output:      {valid_output[idx]}')\n",
    "print('--------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c13f73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "1uY06xs7n5sx",
    "outputId": "0b2993a4-b919-41a6-e99e-d395cde20c1e"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5d85184c86e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maxis_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxis_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-c38ff7f36e88>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, data, clean_paddings)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "\n",
    "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
    "\n",
    "axis_text.append('<eos>')\n",
    "\n",
    "axis_text += tokenizer_input.decode(predicted, True).split()\n",
    "\n",
    "limit = len(axis_text)\n",
    "for bi in range(number_of_layers):\n",
    "    for hi in range(number_of_heads):\n",
    "        attetion_plot = torch.zeros(limit, limit)\n",
    "        for di in range(limit):\n",
    "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
    "\n",
    "        ax = plots[bi][hi]\n",
    "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
    "        ax.set_yticklabels([''] + axis_text)\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        # Set up a title\n",
    "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe81fd",
   "metadata": {
    "id": "CXFro4HhyE8D"
   },
   "outputs": [],
   "source": [
    "# In case the previous cell is not plotting anything, uncomment the code below and execute. After that, the plotting should be fine.\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# x = np.linspace(0, 10, 100)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(x, np.sin(x), '-')\n",
    "# plt.plot(x, np.cos(x), '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9016d0f",
   "metadata": {
    "id": "bxvZ1nVstR7j"
   },
   "source": [
    "#Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264263cd",
   "metadata": {
    "id": "JlVOSUDaNqaz"
   },
   "outputs": [],
   "source": [
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff34d5",
   "metadata": {
    "id": "4VAAvPyR4GMv"
   },
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    translation_results = []\n",
    "    eval_text = []\n",
    "    bleu_results = []\n",
    "    for idx, context in enumerate(valid_input):\n",
    "        encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "        x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "        y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
    "\n",
    "        intent = len(encoded_input) + 1\n",
    "        predicted = y[intent:]\n",
    "        completion = clean_tokens(tokenizer_output.decode(predicted, True))\n",
    "        translation_results.append(completion)\n",
    "\n",
    "        eval = clean_tokens(valid_output[idx])\n",
    "        eval_text.append(eval)\n",
    "        # bleu = sentence_bleu([eval], completion, smoothing_function=smooth)\n",
    "        # bleu_results.append(bleu)\n",
    "\n",
    "    with open('tilde_valid.out', 'w') as f:\n",
    "        f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "    with open('tilde_valid.ref', 'w') as f:\n",
    "        f.write(\"\\n\".join(eval_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_vocab -s 10000\n",
    "# UNKNOWN TOKEN\n",
    "\n",
    "# |v|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b3970",
   "metadata": {
    "id": "4xQ1sDwfWS9S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876c725",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8TILVBwWncc",
    "outputId": "771c5462-601d-4e6e-9c44-d1b99918df5f"
   },
   "outputs": [],
   "source": [
    "#!perl mosesdecoder/scripts/generic/multi-bleu.perl tilde_valid.ref < tilde_valid.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b359058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU = 7.92, 38.4/12.4/4.2/2.0 (BP=1.000, ratio=1.021, hyp_len=9711, ref_len=9509)\n",
    "\n",
    "# joint_vocab -s 10,000\n",
    "# BLEU = 8.61, 44.4/15.1/5.5/2.8 (BP=0.852, ratio=0.862, hyp_len=8198, ref_len=9509)\n",
    "\n",
    "# full joint_vocab\n",
    "# BLEU = 9.18, 41.7/14.1/5.4/2.8 (BP=0.948, ratio=0.950, hyp_len=9030, ref_len=9509)\n",
    "\n",
    "# model_best.pt\n",
    "# BLEU = 13.47, 48.0/19.6/9.4/5.5 (BP=0.908, ratio=0.912, hyp_len=8670, ref_len=9509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9359548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf0d01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0ccdKVEi_51",
    "outputId": "1325a0f7-bbc4-42ce-87c8-bdec30a82a37"
   },
   "outputs": [],
   "source": [
    "#!cat tilde_valid.out | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.out\n",
    "#!cat tilde_valid.ref | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a2b68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ_G2YbxgdnH",
    "outputId": "16446218-4a72-4072-d04d-fad681783dc5"
   },
   "outputs": [],
   "source": [
    "#!pip install sacrebleu\n",
    "#!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae7ed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Z2dQh7gnq2",
    "outputId": "8a4d34d6-f607-4067-ab6c-bb6abb6d70c1"
   },
   "outputs": [],
   "source": [
    "#import sacrebleu\n",
    "\n",
    "#with open('tilde_valid.detok.ref', 'r') as f:\n",
    "#    eval_ref = [l.strip() for l in f.readlines()]\n",
    "#with open('tilde_valid.detok.out', 'r') as f:\n",
    "#    translation_results = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#refs = [eval_ref]\n",
    "#sys = translation_results\n",
    "#bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "#print(bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.918993465381516\n",
    "# joint_vocab -s 10000  8.534786641173136\n",
    "\n",
    "# full joint_vocab 9.174070997058795\n",
    "\n",
    "# model_best.pt \n",
    "13.481896471451254"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44884c",
   "metadata": {
    "id": "q8b-0-iFkRRA"
   },
   "source": [
    "#Interactive translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d013ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LHYXSA190x8G",
    "outputId": "75c2aeed-055e-4a30-b252-b1039a2b3fe0"
   },
   "outputs": [],
   "source": [
    "context = input(\"Enter your English text to translate: \")\n",
    "\n",
    "# Predict Latvian output\n",
    "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
    "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
    "\n",
    "intent = len(encoded_input) + 1\n",
    "\n",
    "predicted = y[0][intent:]\n",
    "completion = tokenizer_output.decode(predicted, True)\n",
    "print(f'Input:            {context}')\n",
    "print(f'Predicted output: {completion}')\n",
    "\n",
    "\n",
    "# Plot attention\n",
    "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "\n",
    "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
    "\n",
    "axis_text.append('<eos>')\n",
    "\n",
    "axis_text += tokenizer_input.decode(predicted, True).split()\n",
    "\n",
    "limit = len(axis_text)\n",
    "for bi in range(number_of_layers):\n",
    "    for hi in range(number_of_heads):\n",
    "        attetion_plot = torch.zeros(limit, limit)\n",
    "        for di in range(limit):\n",
    "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
    "\n",
    "        ax = plots[bi][hi]\n",
    "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
    "        ax.set_yticklabels([''] + axis_text)\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        # Set up a title\n",
    "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfe53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "with valid Proper packages minGPT EN-LV translator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
