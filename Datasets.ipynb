{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regular-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "diagnostic-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from datasets import list_datasets, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lyric-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\r\n",
      "Version: 0.10.2\r\n",
      "Summary: Fast and Customizable Tokenizers\r\n",
      "Home-page: https://github.com/huggingface/tokenizers\r\n",
      "Author: Anthony MOI\r\n",
      "Author-email: anthony@huggingface.co\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: transformers\r\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-federal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.6.0\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages/transformers-4.6.0-py3.8.egg\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, regex, requests, sacremoses, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, catalonia_independence, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, code_x_glue_cc_clone_detection_big_clone_bench, code_x_glue_cc_clone_detection_poj104, code_x_glue_cc_cloze_testing_all, code_x_glue_cc_cloze_testing_maxmin, code_x_glue_cc_code_completion_line, code_x_glue_cc_code_completion_token, code_x_glue_cc_code_refinement, code_x_glue_cc_code_to_code_trans, code_x_glue_cc_defect_detection, code_x_glue_ct_code_to_text, code_x_glue_tc_nl_code_search_adv, code_x_glue_tc_text_to_code, code_x_glue_tt_text_to_text, com_qa, common_gen, common_voice, commonsense_qa, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hendrycks_test, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jnlpba, journalists_questions, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, klue, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, math_dataset, math_qa, matinf, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, swag, swahili, swahili_news, swda, swedish_ner_corpus, swedish_reviews, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, tilde_model, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, Abdo1Kamr/Arabic_Nine_Hadiths_Books, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Avishekavi/Avi, Binbin/my_dataset, Darren/data, EMBO/biolang, EMBO/sd-nlp, Eymen3455/xsum_tr, FRTNX/cosuju, Felix-ML/quoteli3, Firoj/CrisisBench, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Halilyesilceng/autonlp-data-nameEntityRecognition, HarleyQ/WitcherDialogue, Harveenchadha/Gujarati-Monolingual-Data, Jean-Baptiste/wikiner_fr, KETI-AIR/klue, KETI-AIR/kor_corpora, KETI-AIR/korquad, KETI-AIR/nikl, LIAMF-USP/arc-retrieval-c4, MKK/Dhivehi-English, MarianaSahagun/test, Melinoe/TheLabTexts, NTUYG/RAGTest, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, Ofrit/tmp, QA/abk-eng, Remesita/tagged_reviews, SajjadAyoubi/persian_qa, TRoboto/masc, Tatyana/ru_sentiment_dataset, Terry0107/RiSAWOZ, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, XiangXiang/clt, Yves/fhnw_swiss_parliament, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/re_dial, ajmbell/test-dataset, alireza655/alireza655, allenai/c4, anukaver/EstQA, ashish-shrivastava/dont-know-dataset, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, caca/zscczs, canwenxu/dogwhistle, ccccccc/hdjw_94ejrjr, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, cheulyop/ksponspeech, clarin-pl/cst-wikinews, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, cnrcastroli/aaaa, congpt/dstc23_asr, corypaik/prost, ctl/ConceptualCaptions, dasago78/dasago78dataset, dataset/wikipedia_bn, david-wb/zeshel, deepset/germandpr, deepset/germanquad, dfgvhxfgv/fghghj, dgknrsln/Yorumsepeti, dispenst/jhghdghfd, dispix/test-dataset, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ervis/aaa, ervis/qqq, fatvvs/autonlp-data-entity_model_conll2003, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, fvillena/cantemist, fvillena/spanish_diagnostics, german-nlp-group/german_common_crawl, godzillavskongonlinetv/ergfdg, godzillavskongonlinetv/godzillavskongfullmovie, gpt3mix/rt20, gpt3mix/sst2, gustavecortal/fr_covid_news, hartzeer/kdfjdshfje, hfface/poopi, huggingFaceUser02/air21_grp13_tokenized_results, huseinzol05/translated-The-Pile, iamshsdf/sssssssssss, jaimin/wav2vec2-large-xlsr-gujarati-demo, jdepoix/junit_test_completion, jglaser/binding_affinity, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, julien-c/dummy-dataset-from-colab, julien-c/reactiongif, k-halid/ar, karinev/lanuitdudroit, katoensp/VR-OP, kmyoo/klue-tc-dev, lavis-nlp/german_legal_sentences, lewtun/binary_classification_dummy, lewtun/text_classification_dummy, lhoestq/custom_squad, lhoestq/squad, lhoestq/test, lhoestq/wikipedia_bn, lkiouiou/o9ui7877687, lohanna/testedjkcxkf, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, majod/CleanNaturalQuestionsDataset, makanan/umich, medzaf/test, metalearning/kaggale-nlp-tutorial, mksaad/Arabic_news, mmm-da/rutracker_anime_torrent_titles, mohsenfayyaz/toxicity-classification-datasets, mrojas/abbreviation, mrojas/body, mrojas/disease, mrojas/family, mrojas/finding, mrojas/medication, mrojas/procedure, mulcyber/europarl-mono, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, nateraw/cats-and-dogs, nateraw/fairface, nateraw/test, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, nbroad/few-nerd, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/llama_test, parivartanayurveda/Malesexproblemsayurvedictreatment, pasinit/xlwic, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, pelican/test_100, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, rewardsignal/reddit_writing_prompts, rony/soccer-dialogues, roskoN/dstc8-reddit-corpus, salesken/Paraphrase_category_detection, sdfufygvjh/fgghuviugviu, seamew/Weibo, seamew/amazon_reviews_zh, seamew/weibo_avg, shahrukhx01/questions-vs-statements, sharejing/BiPaR, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/Salome, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/ms_marco_doc2query, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/openwebtext-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, susumu2357/squad_v2_sv, tals/test, tarudesu/UIT-ViCTSD, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, tommy19970714/common_voice, tommy19970714/jsut_asr, tommy19970714/jsut_asr_hiragana, tommy19970714/jsut_asr_hiragana_small, tommy19970714/laborotvspeech, turingbench/TuringBench, uasoyasser/rgfes, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vctc92/sdsd, vctc92/test, versae/adobo, vershasaxena91/datasets, vershasaxena91/squad_multitask, w-nicole/childes_data, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, yluisfern/PBU\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f234f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 6.11kB [00:00, 2.37MB/s]                   \n",
      "Downloading: 17.7kB [00:00, 8.38MB/s]                   \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['bg-el', 'cs-en', 'de-hr', 'en-no', 'es-pt']\nExample of usage:\n\t`load_dataset('tilde_model', 'bg-el')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ae43aa06d219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtilde_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tilde_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;31m# Instantiate the dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m     builder_instance: DatasetBuilder = builder_cls(\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;31m# Batch size used by the ArrowWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;31m# It defines the number of samples that are kept in memory before writing them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cache_dir, name, hash, features, **config_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"features\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIG_CLASS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         self.config, self.config_id = self._create_builder_config(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_create_builder_config\u001b[0;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIGS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mexample_of_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"load_dataset('{}', '{}')\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    312\u001b[0m                         \u001b[0;34m\"Config name is missing.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         \u001b[0;34m\"\\nPlease pick one among the available configs: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['bg-el', 'cs-en', 'de-hr', 'en-no', 'es-pt']\nExample of usage:\n\t`load_dataset('tilde_model', 'bg-el')`"
     ]
    }
   ],
   "source": [
    "tilde_ds = load_dataset('tilde_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unnecessary-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tiny_shakespeare (/home/guntis/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('tiny_shakespeare', split='train')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "italic-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTHRUS_DIR = '/ssd2tb/ftwc/playthru_data'\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    outfile = f\"{PTHRUS_DIR}/mingpt-{split}.textds\"\n",
    "    with open(outfile, \"w\") as out:\n",
    "        ptdirs = glob.glob(f\"{PTHRUS_DIR}/mingpt-{split}/*/\")\n",
    "        for ptdir in ptdirs:\n",
    "    #         print(\"ptdir=\", ptdir)\n",
    "            gamename = Path(ptdir).name\n",
    "            lines = []\n",
    "            for ptf in sorted(glob.glob(ptdir+'*.pthru')):\n",
    "    #             print(ptf)\n",
    "                with open(ptf, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            lines.append(line)\n",
    "            out.write(f'{{\"game\":\"{gamename}\"')\n",
    "            out.write(',\"text\":\"')\n",
    "            out.write(' | '.join(lines))\n",
    "            out.write('\"}')\n",
    "            out.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expired-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 45, 11, 9, 13, 9, 88, 120, 10, 9, 13, 18, 56, 10, 9, 13, 18, 55, 19, 10, 9, 13, 26, 56, 10, 9, 13, 18, 34, 10, 9, 13, 110, 55, 19, 10, 28, 10, 44, 8, 12, 14, 46, 11, 16, 8, 12, 39, 11, 9, 13, 9, 88, 68, 66, 67, 8, 12, 30, 121, 68, 66, 67, 108, 46, 33, 12, 35, 76, 41, 68, 66, 67, 24]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file('/ssd2tb/ftwc/ftwc_tokenizer_new.json')\n",
    "pttext = \"on counter : + raw + sliced tomato , + raw red apple , + raw red hot pepper ,\"+\\\n",
    "\" + raw yellow apple , + raw red onion , + raw green hot pepper , cookbook , knife ;\"+\\\n",
    "\" | on stove : nothing ; | carrying : + raw + sliced block of cheese ;\"+\\\n",
    "\" | >>>[ cook block of cheese with stove ]<<< | you fried the block of cheese .\"\n",
    "\n",
    "encoded_data = tokenizer.encode(pttext)\n",
    "print([tokid for tokid in encoded_data.ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passing-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >counter</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tomato</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >yellow</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >onion</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >green</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cookbook</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >knife</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >nothing</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >carrying</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >>>>[</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cook</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >]<<<</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >you</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >fried</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer)\n",
    "visualizer(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sunrise-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer2 = PreTrainedTokenizerFast(tokenizer_file='/ssd2tb/ftwc/ftwc_tokenizer_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alone-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14, 45, 11, 9, 13, 9, 88, 120, 10, 9, 13, 18, 56, 10, 9, 13, 18, 55, 19, 10, 9, 13, 26, 56, 10, 9, 13, 18, 34, 10, 9, 13, 110, 55, 19, 10, 28, 10, 44, 8, 12, 14, 46, 11, 16, 8, 12, 39, 11, 9, 13, 9, 88, 68, 66, 67, 8, 12, 30, 121, 68, 66, 67, 108, 46, 33, 12, 35, 76, 41, 68, 66, 67, 24], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "common-visit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/ssd2tb/ftwc/playthru_data/mingpt-train.textds', 'valid': '/ssd2tb/ftwc/playthru_data/mingpt-valid.textds', 'test': '/ssd2tb/ftwc/playthru_data/mingpt-test.textds'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-53aaefdfe252b4eb\n",
      "Reusing dataset json (/home/guntis/.cache/huggingface/datasets/json/default-53aaefdfe252b4eb/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 2506\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dsfiles = {split: f\"{PTHRUS_DIR}/mingpt-{split}.textds\" for split in ['train', 'valid', 'test']}\n",
    "print(dsfiles)\n",
    "\n",
    "dataset = load_dataset('json', data_files=dsfiles)\n",
    "print(dataset)\n",
    "# batch = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hungry-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.07ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.15ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.13ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(data:dict) -> dict:\n",
    "    data['input_ids'] = tokenizer.encode(data['text']).ids\n",
    "    return data\n",
    "\n",
    "# tokenized_ds = dataset.map(tokenize_text)\n",
    "tokenized_ds = dataset.map(lambda data: tokenizer2(data['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "simplified-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 2506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "absolute-raleigh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game name: tw-cooking-recipe4+take4+cut+go6-0dLNsp9YtYOJcv5d\n",
      ">>>[ start ]<<< | --------- Do : find kitchen , read cookbook , eat meal . | -= corridor =- | Exits | east to unknown ; | south to unknown ; | >>>[ go south ]<<< | You go south. | -= livingroom =- | ON sofa : nothing ; | Exits | east to unknown ; | n\n",
      "\n",
      ">>>[ start ]<<< | --------- do : find kitchen, read cookbook, eat meal. | -= corridor =- | exits | east to unknown ; | south to unknown ; | >>>[ go south ]<<< | you go south. | -= livingroom =- | on sofa : nothing ; | exits | east to unknown ; |\n",
      "\n",
      "tensor([ 30, 114,  33,  12,  21,  37,  11,  79,  27,  10,  61,  28,  10,  62,\n",
      "         25,  24,  12,  31,  77,  32,  12,  40,  12,  49,  17,  23,   8,  12,\n",
      "         52,  17,  23,   8,  12,  30,  53,  52,  33,  12,  35,  53,  52,  24,\n",
      "         12,  31,  85,  32,  12,  14, 128,  11,  16,   8,  12,  40,  12,  49,\n",
      "         17,  23,   8,  12])\n"
     ]
    }
   ],
   "source": [
    "_which_record = 0\n",
    "print(\"game name:\", tokenized_ds['test']['game'][0])\n",
    "print(tokenized_ds['test']['text'][_which_record][:250])\n",
    "print()\n",
    "sample_record = tokenized_ds['test']['input_ids'][_which_record]\n",
    "print(tokenizer2.decode(sample_record[:60]))\n",
    "print()\n",
    "print(sample_record[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "lightweight-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: switch to using transformers.tokenizers.PreTrainedTokenizerFast\n",
    "# instead of huggingface.tokenizers.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c791034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_ds[train]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[valid]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[test]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.set_format(type='torch', columns=['input_ids'])\n",
    "for k in tokenized_ds:\n",
    "    print(f\"tokenized_ds[{k}]: {tokenized_ds[k].format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "disturbed-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids'])\n",
      "Number of rows in batch = 10\n",
      "Shape of row[0]= torch.Size([476])\n",
      "Record lengths= [476, 642, 647, 1895, 839, 1503, 1078, 1808, 709, 1680]\n"
     ]
    }
   ],
   "source": [
    "bat = tokenized_ds['valid'][0:10]\n",
    "print(bat.keys())\n",
    "bat1 = bat['input_ids']\n",
    "print(\"Number of rows in batch =\",len(bat1))\n",
    "print(\"Shape of row[0]=\", bat1[0].shape)\n",
    "print(\"Record lengths=\", [b.shape[0] for b in bat1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "going-multiple",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>>[', 'start', ']<<<', '|', '---------', 'Do', ':', 'find', 'kitchen', ',', 'read', 'cookbook', ',', 'eat', 'meal', '.', '|', '-=', 'corridor', '=-', '|', 'Exits', '|', 'east', 'to', 'unknown', ';', '|', 'south', 'to', 'unknown', ';', '|', '>>>[', 'go', 'south', ']<<<', '|', 'You', 'go', 'south.', '|', '-=', 'livingroom', '=-', '|', 'ON', 'sofa', ':', 'nothing', ';', '|', 'Exits', '|', 'east', 'to', 'unknown', ';', '|', 'north', 'to', 'corridor', ';', '|', 'south', 'to', 'unknown', ';', '|', '>>>[', 'go', 'south', ']<<<', '|', 'You', 'go', 'south.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+closed', 'oven', ':', 'unknown', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'open', 'oven', ']<<<', '|', 'You', 'open', 'the', 'oven.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'read', 'cookbook', ']<<<', '|', 'You', 'read', 'the', 'recipe', '---------', 'Acquire', ':', 'green', 'hot', 'pepper', ',', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '---------', 'Do', ':', 'slice', 'green', 'hot', 'pepper', ',', 'roast', 'green', 'hot', 'pepper', ',', 'roast', 'red', 'potato', ',', 'prepare', 'meal', ';', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'take', 'green', 'hot', 'pepper', ']<<<', '|', 'You', 'take', 'the', 'green', 'hot', 'pepper', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ';', '|', '>>>[', 'take', 'knife', ']<<<', '|', 'You', 'take', 'the', 'knife', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ',', 'knife', ';', '|', '>>>[', 'slice', 'green', 'hot', 'pepper', 'with', 'knife', ']<<<', '|', 'You', 'slice', 'the', 'green', 'hot', 'pepper.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ';', '|', '>>>[', 'take', 'red', 'potato', ']<<<', '|', 'You', 'take', 'the', 'red', 'potato', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '|', '>>>[', 'go', 'east', ']<<<', '|', 'You', 'go', 'east.', '|', '-=', 'pantry', '=-', '|', 'ON', 'shelf', ':', 'salt', ';', '|', 'Exits', '|', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '|', '>>>[', 'take', 'salt', ']<<<', '|', 'You', 'take', 'the', 'salt', 'from', 'the', 'shelf.', '|', '-=', 'pantry', '=-', '|', 'ON', 'shelf', ':', 'nothing', ';', '|', 'Exits', '|', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '|', '>>>[', 'go', 'west', ']<<<', '|', 'You', 'go', 'west.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '|', '>>>[', 'take', 'yellow', 'bell', 'pepper', ']<<<', '|', 'You', 'take', 'the', 'yellow', 'bell', 'pepper', 'from', 'the', 'fridge.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '|', '>>>[', 'prepare', 'meal', ']<<<', '|', 'You', 'prepare', 'the', 'meal.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', 'knife', ',', 'meal', ';', '|', '>>>[', 'eat', 'meal', ']<<<', '|', 'You', 'eat', 'the', 'meal.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', 'knife', ';']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds['test']['text'][0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16738a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tw131': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd07c8357969da571df80afc132a07f2ae2c06ea8e29ade683f4578cc0e429fe076"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
