{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab3b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from datasets import list_datasets, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc4e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('/ssd2tb'):\n",
    "    DATA_BASEDIR = '/ssd2tb'\n",
    "else:\n",
    "    DATA_BASEDIR = '/work2/gstrazds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6092c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\r\n",
      "Version: 0.10.3\r\n",
      "Summary: Fast and Customizable Tokenizers\r\n",
      "Home-page: https://github.com/huggingface/tokenizers\r\n",
      "Author: Anthony MOI\r\n",
      "Author-email: anthony@huggingface.co\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/guntis/anaconda3/envs/tw15/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: transformers\r\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca343624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.12.5\r\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\r\n",
      "Author-email: thomas@huggingface.co\r\n",
      "License: Apache\r\n",
      "Location: /home/guntis/anaconda3/envs/tw15/lib/python3.8/site-packages\r\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, sacremoses, tokenizers, tqdm\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fedb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127\n",
      "0n1xus/codexglue, 0n1xus/pytorrent-standalone, AConsApart/anime_subtitles_DialoGPT, AI-Sweden/SuperLim, ARKseal/YFCC14M_subset_webdataset, ARTeLab/fanpage, ARTeLab/ilpost, ARTeLab/mlsum-it, Abdo1Kamr/Arabic_Hadith, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, AlekseyDorkin/extended_tweet_emojis, AlexMaclean/all-deletion-compressions, AlexMaclean/wikipedia-deletion-compressions, Alvenir/nst-da-16khz, Annielytics/DoctorsNotes, AryanLala/autonlp-data-Scientific_Title_Generator, Avishekavi/Avi, BSC-TeMU/SQAC, BSC-TeMU/ancora-ca-ner, BSC-TeMU/sts-ca, BSC-TeMU/tecla, BSC-TeMU/viquiquad, BSC-TeMU/xquad-ca, Babelscape/rebel-dataset, Binbin/my_dataset, BlakesOrb6/Fred-Flintstone, Bosio/pacman, Bosio/pacman_descriptions, CAGER/rick, CShorten/KerasBERT, CShorten/ZillowPrize, ChadxxxxHall/Inter-vision, Champion/vpc2020_clear_anon_speech, Check/a_re_gi, Check/region_1, Check/region_2, Check/region_3, Check/region_4, Check/region_5, Check/region_6, Check/region_7, Check/region_8, Check/region_9, Check/regions, Check/vverify, ChristophSchuhmann/MS_COCO_2017_URL_TEXT, Chun/dataset, Chuu/Vhh, CodedotAI/code-clippy-tfrecords, CodedotAI/code_clippy, Cropinky/flatearther, Cropinky/rap_lyrics_english, Cropinky/wow_fishing_bobber, Cyberfish/pos_tagger, Cyberfish/text_error_correction, DELith/github-issues, DSCI511G1/COP26_Energy_Transition_Tweets, Daniele/dante-corpus, Darren/data, Datatang/accented_english, Datatang/accented_mandarin, Datatang/chinese_dialect, Datatang/mandarin_chinese, Datatang/mixed_speech_chinese_english, Datatang/multi_language, Datatang/multi_language_conversation, Davlan/conll2003_de_noMISC, Davlan/conll2003_noMISC, Davlan/masakhanerV1, Dmitriy612/1, DoctorSlimm/yipee, Doohae/modern_music_re, EMBO/biolang, EMBO/sd-nlp, ESZER/H, Emanuel/UD_Portuguese-Bosque, Emon/sobuj, Enes3774/data, Eymen3455/xsum_tr, FL33TW00D/test-dataset, FRTNX/cosuju, FRTNX/worldbank-projects, Felix-ML/quoteli3, Fhrozen/JTubeSpeech, Finnish-NLP/mc4_fi_cleaned, Firoj/CrisisBench, Francois/futures_es, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Fraser/wiki_sentences, GEM/ART, GEM/BiSECT, GEM/CrossWOZ, GEM/OrangeSum, GEM/RiSAWOZ, GEM/RotoWire_English-German, GEM/SIMPITIKI, GEM/SciDuet, GEM/Taskmaster, GEM/cochrane-simplification, GEM/conversational_weather, GEM/dstc10_track2_task2, GEM/indonlg, GEM/mlb_data_to_text, GEM/mlsum, GEM/opusparcus, GEM/split_and_rephrase, GEM/sportsett_basketball, GEM/squad_v2, GEM/surface_realisation_st_2020, GEM/test-transform, GEM/test, GEM/totto, GEM/turku_hockey_data2text, GEM/turku_paraphrase_corpus, GEM/viggo, GEM/wiki_cat_sum, GEM/xlsum, GEM/xsum, Gabriel/squad_v2_sv, GalacticAI/Noirset, Gauravadlakha1509/new_one, Graphcore/wikipedia-bert-128, Graphcore/wikipedia-bert-512, Gwangho/NCBI-Sars-Cov-2, HHousen/ParaSCI, HHousen/quora, Halilyesilceng/autonlp-data-nameEntityRecognition, HarleyQ/WitcherDialogue, Harveenchadha/bol-models, HarveyBWest/mybot, Hellisotherpeople/DebateSum, Husain/intent-classification-en-fr, IFSTalfredoswald/MBTI, Iftoo95/Arabic_Sentiment_and_Topics, IlyaGusev/gazeta, IlyaGusev/headline_cause, Intel/WEC-Eng, Ishwar/Senti, JIsanan/war-ceb-wikipedia, Jack0508/TED2020_kor, Jack0508/TED2020_vi, Jack0508/TED2020vi_kor, Jack0508/demo, Jack0508/eng_vi_demo, Jack0508/test, Jack0508/vi-ko-TED-txt, Jean-Baptiste/wikiner_fr, Jeska/autonlp-data-vaccinfaq, Jeska/vaccinchat, JesseParvess/Elucidate_Audio_Data, JesseParvess/Elucidate_Audio_Text, Jikiwa/demo1, Jikiwa/demo2, Jikiwa/demo3, Jikiwa/demo4, Jikiwa/glue-mnli-train, Jikiwa/push-to-hub, Jikiwa/pushe-to-hub, Jikiwa/pushed-to-hub, Jikiwa/pushedd-to-hub, Jikiwa/random_repo, Jikiwa/stargazers, Jikiwa/temp-repo-valid, Jikiwa/test-16336477963335, Jikiwa/test-16336478042515, Jikiwa/test-16336479967338, Jikiwa/test-16336480189315, Jikiwa/test-16336486877862, Jikiwa/test-16340052901609, Jikiwa/test-16340052972855, Jikiwa/test-16344347220590, Jikiwa/test-16344347234752, Jikiwa/test-16344349332219, Jikiwa/test-16344349440339, Jikiwa/test-16344351925697, Jikiwa/test-16344360501144, Jikiwa/test-16344361893586, Jikiwa/test-16344362261113, Jikiwa/test-16344362895458, Jikiwa/test-16344364230608, Jikiwa/test-16344364547167, Jikiwa/test-16344367190179, Jikiwa/test-16344368182003, KBLab/suc3, KETI-AIR/aihub, KETI-AIR/klue, KETI-AIR/kor_corpora, KETI-AIR/korquad, KETI-AIR/nikl, Karavet/ARPA-Armenian-Paraphrase-Corpus, Karavet/ILUR-news-text-classification-corpus, Karavet/pioNER-Armenian-Named-Entity, Khanoooo/autonlp-data-Corona, Khondoker/SentNoB, LIAMF-USP/arc-retrieval-c4, Lenn/github-issues, LeoCordoba/CC-NEWS-ES-titles, LeoCordoba/CC-NEWS-ES, LoganKells/amazon_product_reviews_video_games, MBAH/MOVIESON, MKK/Dhivehi-English, Mansooreh/sharif-emotional-speech-dataset, MarianaSahagun/test, MarkusDressel/cord, Marzipan/QA4PC, Mateo/test_dataset, Mateo/testdataset, McGill-NLP/mlquestions, Melinoe/TheLabTexts, MickyMike/large_c_corpus, Mrleo1nid/Test_ru_dataset, Mulin/my_second_dataset, Mulin/my_third_dataset, NTUYG/RAGTest, Narsil/asr_dummy, Narsil/conversational_dummy, Narsil/image_dummy, NbAiLab/NCC_small, NbAiLab/NCC_small_100, NbAiLab/bokmaal_admin, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, NbAiLab/test_corpus, NbAiLab/tull, NikolajW/NPS_nonNormalized-Cased, Ofrit/tmp, Pengfei/test, Pengfei/test1, PlanTL-GOB-ES/SQAC, Pongsaky/Wiki_SCG, Pratik/Gujarati_OpenSLR, Pyke/patent_abstract, QA/abk-eng, RBG-AI/CoRePooL, Remesita/tagged_reviews, Renukswamy/Patent_sentiment_analysis, RohanAiLab/persian_blog, RohanAiLab/persian_daily_news, RohanAiLab/persian_news_dataset, Romrawin/mn-sim, SCourthial/test, SajjadAyoubi/persian_qa, Sam2021/Arguement_Mining_CL2017, Samip/func, SaulLu/Natural_Questions_HTML, SaulLu/Natural_Questions_HTML_Toy, SaulLu/Natural_Questions_HTML_reduced_all, SaulLu/test, SaulLu/toy_struc_dataset, Serhii/Custom_SQuAD, ShinyQ/PPKM_Pemerintah, Smiling/webnovels-en, SoLID/shellcode_i_a32, SocialGrep/one-million-reddit-confessions, SocialGrep/one-million-reddit-jokes, SocialGrep/one-million-reddit-questions, SocialGrep/one-year-of-r-india, SocialGrep/reddit-crypto-aug-2021, SocialGrep/reddit-nonewnormal-complete, SocialGrep/reddit-wallstreetbets-aug-2021, SocialGrep/ten-million-reddit-answers, SocialGrep/the-reddit-covid-dataset, SophieTr/reddit_clean, TRoboto/masc, TaahaKazi/FCE, Tatyana/ru_sentiment_dataset, Terry0107/RiSAWOZ, Tevatron/msmarco-passage-corpus, Tevatron/msmarco-passage, Tevatron/scifact-corpus, Tevatron/scifact, Tevatron/wikipedia-curated-corpus, Tevatron/wikipedia-curated, Tevatron/wikipedia-nq-corpus, Tevatron/wikipedia-nq, Tevatron/wikipedia-squad-corpus, Tevatron/wikipedia-squad, Tevatron/wikipedia-trivia-corpus, Tevatron/wikipedia-trivia, Tevatron/wikipedia-wq-corpus, Tevatron/wikipedia-wq, TheBlindBandit/SpongeNot, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, TurkuNLP/turku_hockey_data2text, TurkuNLP/turku_paraphrase_corpus, Tyler/wikimatrix_collapsed, Uri/tmp, Usin2705/test, VJGamer/test, VadorMazer/skyrimdialogstest, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, Wiedy/be, Wiedy/wav2vec2-large-xls-r-300m-tr-colab, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, Wikidepia/mc4-filter, Wuhu0/output, WyrdCurt/AO4W, XiangPan/iflytek, XiangPan/snli_break, XiangXiang/clt, Xinghua/test, Yatoro/github-issues, Yatoro/github_issues, Yeva/arm-summary, Yves/fhnw_swiss_parliament, Zaid/coqa_expanded, Zaid/quac_expanded, Zoe10/ner_dataset, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/FewShotWoz, adamlin/companion, adamlin/coqa_squad, adamlin/daily_dialog, adamlin/domain_classification, adamlin/multiwoz_dst, adamlin/qa_verification, adamlin/roc_story, adamlin/rs, adamlin/weibo_ner, ajmbell/test-dataset, akumar33/manufacturing, albertvillanova/datasets-tests-compression, albertvillanova/dummy_libri2mix, albertvillanova/tests-public-raw-jsonl, albertvillanova/tests-raw-jsonl, albertvillanova/tmp-tests, alireza655/alireza655, alittleie/mis_238, allegro/klej-allegro-reviews, allegro/klej-cbd, allegro/klej-cdsc-e, allegro/klej-cdsc-r, allegro/klej-dyk, allegro/klej-nkjp-ner, allegro/klej-polemo2-in, allegro/klej-polemo2-out, allegro/klej-psc, allegro/polish-question-passage-pairs, allegro/summarization-allegro-articles, allegro/summarization-polish-summaries-corpus, allenai/c4, allenai/scico, alperbayram/TwitterDuygu, alvp/autonlp-data-alberti-stanza-names, alvp/autonlp-data-alberti-stanzas-finetuning, ami-wav2vec2/ami_multi_headset_segmented_and_chunked, ami-wav2vec2/ami_multi_headset_segmented_and_chunked_and_concatenated, ami-wav2vec2/ami_multi_headset_segmented_and_chunked_dummy, ami-wav2vec2/ami_single_headset_segmented, ami-wav2vec2/ami_single_headset_segmented_and_chunked, ami-wav2vec2/ami_single_headset_segmented_and_chunked_and_concatenated, ami-wav2vec2/ami_single_headset_segmented_and_chunked_dummy, ami-wav2vec2/ami_single_headset_segmented_dummy, animesh/autonlp-data-peptides, anton-l/common_language, anton-l/superb, anton-l/superb_demo, anton-l/superb_dummy, anukaver/EstQA, arjunth2001/online_privacy_qna, artyeth/Dorian, aseifert/merlin, aseifert/pie-synthetic, ashish-shrivastava/dont-know-dataset, asi/wikitext_fr, asoroa/bsbasque, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, astarostap/autonlp-data-antisemitism-2, astrideducation/cefr-combined-no-cefr-test, atelders/politweets, athar/QA, athar/a_b, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bertin-project/mc4-es-sampled, bertin-project/mc4-sampling, bhadresh-savani/web_split, bigscience/LanguageResourceCatalogue, bigscience/P3, bigscience/mc4-sampled, bigscience/open_subtitles_monolingual, biu-nlp/qa_align, biu-nlp/qa_discourse, biu-nlp/qa_srl2018, biu-nlp/qa_srl2020, biu-nlp/qamr, biu-nlp/qanom, blinoff/medical_qa_ru_data, bondarchukb/autonlp-data-iab_classification, braincode/braincode, brunodorneles/ner, bs-modeling-metadata/OSCAR_Entity_13_000, bs-modeling-metadata/c4-en-reduced-10000-with-metadata, bs-modeling-metadata/c4-en-reduced-10000, bs-modeling-metadata/c4-en-reduced-with-metadata, bs-modeling-metadata/c4-en-reduced, bs-modeling-metadata/c4_newslike_url_only, bs-modeling-metadata/html-enwiki-200806-extract-raw, bs-modeling-metadata/openwebtext-html-cc, bs-modeling-metadata/website_metadata_c4, bs-modeling-metadata/wiki_dump, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, bwu2018/anime-tagging-dataset, caca/zscczs, cakiki/args_me, cakiki/paperswithcode, canwenxu/dogwhistle, cassandra-themis/QR-AN, castorini/afriberta, castorini/mr-tydi-corpus, castorini/mr-tydi, castorini/msmarco_v1_doc_doc2query-t5_expansions, castorini/msmarco_v1_doc_segmented_doc2query-t5_expansions, castorini/msmarco_v1_passage_doc2query-t5_expansions, castorini/msmarco_v2_doc_doc2query-t5_expansions, castorini/msmarco_v2_doc_segmented_doc2query-t5_expansions, castorini/msmarco_v2_passage_doc2query-t5_expansions, caythuoc/caoduoclieu, ccccccc/hdjw_94ejrjr, ccdv/arxiv-classification, ccdv/arxiv-summarization, ccdv/cnn_dailymail, ccdv/govreport-summarization, ccdv/patent-classification, ccdv/pubmed-summarization, cdleong/piglatin-mt, cdleong/temp_africaNLP_keyword_spotting_for_african_languages, cdminix/iwslt2011, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, cgarciae/point-cloud-mnist, cgarciae/point_cloud_mnist, chenghao/mc4_eu_dedup, chenghao/scielo_books, chenyuxuan/wikigold, cheulyop/dementiabank, cheulyop/ksponspeech, chopey/dhivehi, clarin-pl/aspectemo, clarin-pl/cst-wikinews, clarin-pl/kpwr-ner, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/FRENK-hate-en, classla/FRENK-hate-hr, classla/FRENK-hate-sl, classla/copa_hr, classla/hr500k, classla/janes_tag, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, classla/ssj500k, clem/autonlp-data-french_word_detection, clips/mfaq, clips/mqa, cnrcastroli/aaaa, coala/kkk, congpt/dstc23_asr, corypaik/coda, corypaik/prost, craffel/openai_lambada, crich/cider, csarron/image-captions, csebuetnlp/xlsum, csebuetnlp/xnli_bn, cstrathe435/Task2Dial, ctgowrie/chessgames, ctl/ConceptualCaptions, cyko/books, dalle-mini/YFCC100M_OpenAI_subset, dalle-mini/encoded-vqgan_imagenet_f16_16384, dalle-mini/encoded, dansbecker/hackernews_hiring_posts, dasago78/dasago78dataset, dataset/wikipedia_bn, david-wb/zeshel, davidwisdom/reddit-randomness, debatelab/aaac, deepset/germandpr, deepset/germanquad, deokgu/fooddetection, dfgvhxfgv/fghghj, dfki-nlp/few-nerd, dfki-nlp/mobie, dgao/librispeech_nc_test, dgknrsln/Yorumsepeti, diiogo/brwac-clean, diiogo/harem-seletivo, diiogo/harem-total, diiogo/oscar, dispenst/jhghdghfd, dispix/test-dataset, dk-crazydiv/huggingface-modelhub, dog/friends-of-mine, dog/punks, dongpil/test, dragosnicolae555/RoITD, ducatyb/github-issues, dvilasuero/ag_news_training_set_losses, dweb/squad_with_cola_scores, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edge2992/github-issues, edsas/fgrdtgrdtdr, edsas/grttyi, ehcalabres/ravdess_speech, ejjaffe/onion_headlines_2_sources, eliza-dukim/load_klue_re, elonmuskceo/persistent-space-dataset, emrecan/stsb-mt-turkish, erikacardenas300/Zillow_Economics_Dataset, ervis/aaa, ervis/qqq, eugenesiow/BSD100, eugenesiow/Div2k, eugenesiow/PIRM, eugenesiow/Set14, eugenesiow/Set5, eugenesiow/Urban100, ewdrtfwe/54refyghrtf, fanluo/hotpot_bridge, fatvvs/autonlp-data-entity_model_conll2003, fengzhang/fzTestDatasets, fihtrotuld/asu, flax-community/code_clippy_data, flax-community/conceptual-12m-mbart-50-multilingual, flax-community/conceptual-12m-multilingual-marian-128, flax-community/conceptual-12m-multilingual-marian-es, flax-community/conceptual-12m-multilingual-marian, flax-community/conceptual-captions-12, flax-community/dummy-oscar-als-32, flax-community/german-common-voice-processed, flax-community/german_common_crawl, flax-community/multilingual-vqa, flax-community/norwegian-clean-dummy, flax-community/swahili-safi, flax-sentence-embeddings/Gender_Bias_Evaluation_Set, flax-sentence-embeddings/paws-jsonl, flax-sentence-embeddings/stackexchange_math_jsonl, flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_title_body_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_xml, flexudy/OMQD, formermagic/github_python_1m, formu/CVT, frahman/github-issues, frtna/jwt300_mt, frtna/opensubtitles_mt, frtna/ted_mt, frtna/translationResults_base, fulai/DuReader, fuliucansheng/minicoco, fuliucansheng/mininlp, fuliucansheng/pascal_voc, fuyun1107/clip-for-vlp, fvillena/cantemist, fvillena/spanish_diagnostics, gagan3012/fake-news, gagan3012/grover-data, gar1t/test, gayanin/pubmed-gastro-maskfilling, gayanin/pubmed-gastro-paraphrasing, gayanin/pubmed-gastro-summarisation, gcaillaut/citeseer, gcaillaut/cora, gcaillaut/pubmed, geekydevu/mlquestions, german-nlp-group/german_common_crawl, gmnlp/tico19, gpt3mix/rt20, gpt3mix/sst2, gsarti/change_it, gsarti/clean_mc4_it, gsarti/flores_101, gsarti/itacola, gsarti/wmt_vat, gustavecortal/fr_covid_news, gusu/mymodel1, habu24/fdz, hartzeer/kdfjdshfje, herbievore/test, hf-internal-testing/cats_vs_dogs_sample, hf-internal-testing/fixtures_ade20k, hf-internal-testing/fixtures_docvqa, hf-internal-testing/fixtures_image_utils, hf-internal-testing/fixtures_ocr, hf-internal-testing/fixtures_sintel, hf-internal-testing/librispeech_asr_demo, hf-internal-testing/librispeech_asr_dummy, hf-internal-testing/test-dataset, hf-internal-testing/transformers-metadata, hfface/poopi, holodata/sensai, holylovenia/recam, honghungle/dataset, howardmiddleton382/esuyertusutr, howardmiddleton382/wgweagwege, huggingFaceUser02/air21_grp13_inference_results, huggingFaceUser02/air21_grp13_tokenized_results, huggingartists/21-savage, huggingartists/25-17, huggingartists/50-cent, huggingartists/5nizza, huggingartists/5opka, huggingartists/6ix9ine, huggingartists/aaron-watson, huggingartists/abba, huggingartists/adele, huggingartists/agata-christie, huggingartists/aikko, huggingartists/aimer, huggingartists/ajr, huggingartists/alan-walker, huggingartists/andre-3000, huggingartists/arash, huggingartists/architects, huggingartists/arctic-monkeys, huggingartists/ariana-grande, huggingartists/ariya, huggingartists/armin-van-buuren, huggingartists/as-i-lay-dying, huggingartists/asdfgfa, huggingartists/asper-x, huggingartists/baklan, huggingartists/big-baby-tape, huggingartists/big-russian-boss, huggingartists/bill-wurtz, huggingartists/billie-eilish, huggingartists/billy-talent, huggingartists/bladee, huggingartists/bob-dylan, huggingartists/bones, huggingartists/booker, huggingartists/boris-grebenshikov, huggingartists/braii, huggingartists/bring-me-the-horizon, huggingartists/bruce-springsteen, huggingartists/bryan-adams, huggingartists/burzum, huggingartists/bushido-zho, huggingartists/cardi-b, huggingartists/chester-bennington, huggingartists/chief-keef, huggingartists/cocomelon, huggingartists/coin, huggingartists/coldplay, huggingartists/dababy, huggingartists/david-bowie, huggingartists/ddt, huggingartists/death-grips, huggingartists/deep-purple, huggingartists/denderty, huggingartists/dermot-kennedy, huggingartists/dj-artem-artemov, huggingartists/doja-cat, huggingartists/drake, huggingartists/dua-lipa, huggingartists/duran-duran, huggingartists/dzhizus, huggingartists/ed-sheeran, huggingartists/egor-kreed, huggingartists/egor-letov, huggingartists/elton-john, huggingartists/eminem, huggingartists/enigma, huggingartists/enya, huggingartists/epic-rap-battles-of-history, huggingartists/face, huggingartists/fascinoma, huggingartists/fear-factory, huggingartists/florence-the-machine, huggingartists/freddie-dredd, huggingartists/freelancer, huggingartists/galenskaparna-and-after-shave, huggingartists/ghost, huggingartists/ghostemane, huggingartists/ghostmane, huggingartists/gizmo, huggingartists/gorillaz, huggingartists/green-day, huggingartists/grigory-leps, huggingartists/grimes, huggingartists/gspd, huggingartists/gunna, huggingartists/hillsong-worship, huggingartists/hyuna, huggingartists/i-dont-know-how-but-they-found-me, huggingartists/idktime, huggingartists/imagine-dragons, huggingartists/jah-khalib, huggingartists/jim-morrison, huggingartists/john-k-samson, huggingartists/john-lennon, huggingartists/joji, huggingartists/joni-mitchell, huggingartists/justin-bieber, huggingartists/kanye-west, huggingartists/kasta, huggingartists/kehlani, huggingartists/kendrick-lamar, huggingartists/king-krule, huggingartists/kipelov, huggingartists/kishlak, huggingartists/kizaru, huggingartists/kojey-radical, huggingartists/krechet, huggingartists/krept-and-konan-bugzy-malone-sl-morisson-abra-cadabra-rv-and-snap-capone, huggingartists/kurt-cobain, huggingartists/lady-gaga, huggingartists/lazy-jay, huggingartists/led-zeppelin, huggingartists/lil-baby, huggingartists/lil-nas-x, huggingartists/lil-peep, huggingartists/lil-skies, huggingartists/lil-uzi-vert, huggingartists/linkin-park, huggingartists/little-big, huggingartists/lizer, huggingartists/logic, huggingartists/lorde, huggingartists/loud-luxury, huggingartists/loverance, huggingartists/lovv66, huggingartists/lumen, huggingartists/lyapis-trubetskoy, huggingartists/macan, huggingartists/machine-gun-kelly, huggingartists/madonna, huggingartists/marillion, huggingartists/maroon-5, huggingartists/mashina-vremeni, huggingartists/mating-ritual, huggingartists/max-korzh, huggingartists/mayot, huggingartists/mc-ride, huggingartists/melanie-martinez, huggingartists/metallica, huggingartists/mf-doom, huggingartists/michael-jackson, huggingartists/mikhail-gorshenev, huggingartists/mikhail-krug, huggingartists/miyagi, huggingartists/mnogoznaal, huggingartists/morgenshtern, huggingartists/mumiy-troll, huggingartists/muse, huggingartists/nautilus-pompilius, huggingartists/nervy, huggingartists/nirvana, huggingartists/noize-mc, huggingartists/oasis, huggingartists/obladaet, huggingartists/og-buda, huggingartists/ot-rus, huggingartists/our-last-night, huggingartists/oxxxymiron, huggingartists/peter-paul-and-mary, huggingartists/pharaoh, huggingartists/phish, huggingartists/pink-floyd, huggingartists/placebo, huggingartists/platina, huggingartists/pop-smoke, huggingartists/post-malone, huggingartists/pyrokinesis, huggingartists/queen, huggingartists/radiohead, huggingartists/rage-against-the-machine, huggingartists/ramil, huggingartists/rammstein, huggingartists/red-hot-chili-peppers, huggingartists/rex-orange-county, huggingartists/rihanna, huggingartists/rocket, huggingartists/sam-kim, huggingartists/scriptonite, huggingartists/sektor-gaza, huggingartists/sergei-letov, huggingartists/shadowraze, huggingartists/sia, huggingartists/sid-sriram, huggingartists/skillet, huggingartists/slava-kpss, huggingartists/slava-marlow, huggingartists/snoop-dogg, huggingartists/sqwore, huggingartists/sugar-ray, huggingartists/suicideoscope, huggingartists/sum-41, huggingartists/sundara-karma, huggingartists/system-of-a-down, huggingartists/t-fest, huggingartists/tanzy-minus, huggingartists/taylor-swift, huggingartists/tedeschi-trucks-band, huggingartists/the-69-eyes, huggingartists/the-avalanches, huggingartists/the-beatles, huggingartists/the-gazette, huggingartists/the-grateful-dead, huggingartists/the-king-and-the-jester, huggingartists/the-notorious-big, huggingartists/the-sugarcubes, huggingartists/the-the-pigs, huggingartists/the-velvet-underground, huggingartists/the-weeknd, huggingartists/tiamat, huggingartists/till-lindemann, huggingartists/tom-waits, huggingartists/tony-raut-and-garry-topor, huggingartists/tool, huggingartists/totpoc, huggingartists/travis-scott, huggingartists/twenty-one-pilots, huggingartists/tyler-the-creator, huggingartists/upsahl, huggingartists/v-x-v-prince, huggingartists/van-morrison, huggingartists/veggietales, huggingartists/viktor-tsoi, huggingartists/vladimir-vysotsky, huggingartists/xxxtentacion, huggingartists/young-thug, huggingartists/yung-lean, huggingartists/yung-plague, huggingartists/zemfira, huggingface/DataMeasurementsFiles, huggingface/documentation-images, huggingface/label-files, huggingface/transformers-metadata, huggingface-course/codeparrot-ds-train, huggingface-course/codeparrot-ds-valid, huyongquan/d2, iamshsdf/sssssssssss, iarfmoose/qa_evaluator, iarfmoose/question_generator, image-search-2/unsplash_lite_image_dataset, imthanhlv/binhvq_dedup, imthanhlv/binhvq_news21_raw, indonesian-nlp/id_personachat, jaimin/wav2vec2-large-xlsr-gujarati-demo, jakeazcona/short-text-labeled-emotion-classification, jakeazcona/short-text-multi-labeled-emotion-classification, jakemarcus/MATH, jamol1741/test_dataset, jdepoix/junit_test_completion, jglaser/binding_affinity, jhqwqq/2, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jimregan/foinse, jimregan/lasid, jinmang2/KorQuAD-v1.0, jinmang2/aihub-book-mrc, jinmang2/aihub-mrc, jinmang2/aistage-mrc, jinmang2/load_klue_re, jinmang2/pred, jinmang2/temp, jinmang2/vision-data, jiyoojeong/targetizer, jlh/coco, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, jonfd/ICC, jozierski/ecomwebtexts-pl, jpcorb20/multidogo, jsgao/eli5_category, julien-c/dummy-dataset-from-colab, julien-c/persistent-space-dataset, julien-c/reactiongif, juliensimon/autonlp-data-song-lyrics-demo, juliensimon/autonlp-data-song-lyrics, juny116/few_glue, k-halid/ar, k0t1k/test, kaka10/fgfgfgfg, karinev/lanuitdudroit, kartikay/review-summarizer, katoensp/VR-OP, kenlevine/CUAD, keshan/clean-si-mc4, keshan/large-sinhala-asr-dataset, keshan/multispeaker-tts-sinhala, keshan/wit-dataset, kevinlu1248/personificationgen, khalidsaifullaah/detecThreats, khanbaba/online_love, kiamehr74/CoarseWSD-20, kiyoung2/aistage-mrc, kiyoung2/temp, kleinay/qa_srl, kmfoda/booksum, kmfoda/name_finder_v1, kmyoo/klue-tc-dev, knilakshan20/wikigold, kroshan/BioASQ, kroshan/qa_evaluator, laion/laion_100m_vqgan_f8, laion/laion_20m_dvae, lara-martin/Scifi_TV_Shows, larcane/ko-WIT, lavis-nlp/german_legal_sentences, layboard/layboard.in, lbourdois/CoFiF, leiping/jj, leiping/teeee, leoapolonio/AMI_Meeting_Corpus, lewtun/asr-preds-test, lewtun/asr_dummy, lewtun/binary_classification_dummy, lewtun/bulk-superb-s3p-superb-49606, lewtun/drug-reviews, lewtun/gem-test-predictions, lewtun/gem-test-references, lewtun/github-issues-test, lewtun/github-issues, lewtun/mnist-preds, lewtun/my-awesome-dataset, lewtun/s3prl-sd-dummy, lewtun/text_classification_dummy, lhoestq/custom_squad, lhoestq/demo1, lhoestq/squad, lhoestq/squad_titles, lhoestq/test, lhoestq/test2, lhoestq/tmp, lhoestq/wikipedia_bn, liam168/nlp_c4_sentiment, lidia/202111, limjiayi/hateful_memes_expanded, lincoln/newsquadfr, linhd-postdata/stanzas, liweili/c4_200m, lkiouiou/o9ui7877687, lkndsjkndgskjngkjsndkj/jsjdjsdvkjvszlhdskb, lohanna/testedjkcxkf, lorsorlah/Dadedadedam, loveguruji609/dfdfsdfsdfsdfsdfsd, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, lukasmasuch/my-test-repo-3, lukasmasuch/my-test-repo-4, lukasmasuch/test-2, lukasmasuch/test-3, lukasmasuch/test, lukesjordan/worldbank-project-documents, luofengge/mydata, luofengge/testDataset, luomingshuang/grid_lip_160_80, luozhouyang/dureader, luozhouyang/question-answering-datasets, lvwerra/codeparrot-clean-train, lvwerra/codeparrot-clean-valid, lvwerra/codeparrot-clean, lvwerra/codeparrot-valid-clean-minimal, lvwerra/codeparrot-valid, lvwerra/repo-images, lysandre/my-cool-dataset, m3hrdadfi/recipe_nlg_lite, mad/IndonesiaNewsDataset, maji/npo_mission_statement_ucf, majod/CleanNaturalQuestionsDataset, makanan/umich, manandey/OSCAR_Entity_Toy, manandey/entity_experiments, manishk31/Demo, mariosasko/exposure_errors, mariosasko/nao, markscrivo/OddsOn, martodaniel/terere, masked-neuron/amazon, masked-neuron/ccd, masked-neuron/qb, maximedb/mfaq_light, maximedb/paws-x-all, maximedb/vaccinchat, maximedb/wow, medzaf/test, meghanabhange/chaii, meghanabhange/hilm141021, meghanabhange/hitalm141021, meghanabhange/hitalmsandbox, meghanabhange/talm141021, merve/coco, merve/folk-mythology-tales, merve/poetry, merve/qqp, metalearning/kaggale-nlp-tutorial, metopedia/autonlp-data-Multiple-Source-Language-Consensus-Reconstruction-o, mhtoin/register_oscar, michaelbenayoun/wikipedia-bert-128, microsoft/codexglue_method_generation, midas/inspec_ke_tagged, midas/ldke3k_medium, midas/ldke3k_small, midas/ldkp3k_small, midas/semeval2010_ke_tagged, midas/semeval2017_ke_tagged, mishig/sample_images, mksaad/Arabic_news, ml6team/cnn_dailymail_nl, mldmm/glass_alloy_composition, mmcquade11-test/reuters-for-summarization-two, mmm-da/rutracker_anime_torrent_titles, mnemlaghi/widdd, moshew/my_raft, moumeneb1/French_arpa_lm, mozilla-foundation/common_voice_5_1, mr-robot/ec, mrm8488/fake-news, mrojas/abbreviation, mrojas/body, mrojas/disease, mrojas/family, mrojas/finding, mrojas/medication, mrojas/procedure, mrp/Thai-Semantic-Textual-Similarity-Benchmark, msivanes/github-issues, muhtasham/autonlp-data-Doctor_DE, mulcyber/europarl-mono, munggok/mc4-id, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, mvarma/medwiki, nateraw/auto-cats-and-dogs, nateraw/auto-exp-2, nateraw/beans, nateraw/beans_old, nateraw/blahblah, nateraw/bulk-dummy, nateraw/cats-and-dogs, nateraw/cats_vs_dogs, nateraw/dummy-csv-dataset, nateraw/fairface, nateraw/filings-10k, nateraw/food101, nateraw/food101_old, nateraw/huggingpics-data-2, nateraw/huggingpics-data, nateraw/image-folder, nateraw/imagefolder, nateraw/imagenette, nateraw/img-demo, nateraw/rock_paper_scissors, nateraw/sync_food101, nateraw/test, nateraw/wit, nathanlsl/news, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, navjordj/nak_nb, ncats/EpiClassify4GARD, ncats/EpiSet4NER, ncats/GARD_EpiSet4TextClassification, ncduy/github-issues, ncoop57/athena_data, ncoop57/csnc_human_judgement, ncoop57/rico_captions, neelalex/raft-predictions, nielsr/FUNSD_layoutlmv2, nielsr/XFUN, nielsr/funsd, nlpaueb/test, nlpufg/brwac-pt, nlpufg/brwac, nlpufg/oscar-pt, notional/notional-python, ntutexas/amazon, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/codeparrot-train, osanseviero/llama_test, osanseviero/test, oscar-corpus/OSCAR-2109, ought/raft-submission, ought/raft, outman/test, papluca/language-identification, pariajm/sharif_emotional_speech_dataset, parivartanayurveda/Malesexproblemsayurvedictreatment, pasinit/scotus, pasinit/xlwic, patrickvonplaten/ami_single_headset_segmented_and_chunked, patrickvonplaten/common_voice_6_tr, patrickvonplaten/common_voice_processed_turkish, patrickvonplaten/helena_coworking, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/librispeech_local, patrickvonplaten/librispeech_local_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, pelican/test_100, pere/norwegian_colossal_corpus_v2_short100k, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, peterbonnesoeur/autonlp-data-test_text_summarization, philschmid/test_german_squad, phonlab-tcd/cngv1, phonlab-tcd/corpuscrawler-ga, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, pierreant-p/jcvd-or-linkedin, pierreguillou/test_datasetdict, pierresi/cord, pietrolesci/ag_news, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, proffttega/ILLUMINATI, proffttega/doc, proffttega/join_illuminati_to_become_rich, proffttega/persian_daily_news, project2you/asr, projecte-aina/ancora-ca-ner, projecte-aina/catalan_general_crawling, projecte-aina/catalan_government_crawling, projecte-aina/catalan_textual_corpus, projecte-aina/parlament_parla, projecte-aina/sts-ca, projecte-aina/teca, projecte-aina/tecla, projecte-aina/vilaquad, projecte-aina/viquiquad, projecte-aina/xquad-ca, pstroe/cc100-latin, pulmo/chest_xray, qa4pc/QA4PC, qfortier/instagram_ny, qr/cefr_book_sentences, quis/vnexpress-train, qwant/squad_fr, ragarwal/args-me-pairs, rahular/itihasa, rajeshradhakrishnan/malayalam_2020_wiki, rajeshradhakrishnan/malayalam_news, rajeshradhakrishnan/malayalam_wiki, ramybaly/conll2012, ramybaly/nerd, ranim/Algerian-Arabic, ranpox/xfund, rays2pix/example, rays2pix/example_dataset, readerbench/ChatLinks, rewardsignal/reddit_writing_prompts, rom1504/laion400m, ronaldvanos/testdata, rony/soccer-dialogues, rookieguy12/dataset, rosettarandd/rosetta_balcanica, roskoN/dailydialog, roskoN/dstc8-reddit-corpus, rubenwol/multi_news_qasrl, rwebe/rwebe, s-myk/test, s3h/arabic-gec, s3h/arabic-grammar-corrections, s3h/custom-qalb-classification, s3h/customized-qalb-v2, s3h/customized-qalb, sagnikrayc/mctest, sagnikrayc/quasar, salesken/Paraphrase_category_detection, samarlune/Holy_Coran, sanyu/aw, sanyu/er, sanyu/hh, sanyu/vb, sc2qa/sc2q_commoncrawl, sc2qa/sc2qa_commoncrawl, sdfufygvjh/fgghuviugviu, seamew/ChnSentiCorp, seamew/Hotel, seamew/THUCNews, seamew/THUCNewsText, seamew/THUCNewsTitle, seamew/Weibo, seanbethard/autonlp-data-summarization_model, sebastiaan/test-cefr, sentence-transformers/embedding-training-data, sentence-transformers/msmarco-hard-negatives, sentence-transformers/parallel-sentences, sentence-transformers/reddit-title-body, seregadgl/test_set, sevbqewre/vebdesbdty, severo/autonlp-data-sentiment_detection-3c8bcd36, severo/wit, shahrukhx01/questions-vs-statements, shanya/website_metadata_c4_toy, sharejing/BiPaR, shivkumarganesh/CoLA, shivmoha/squad-unanswerable, shivmoha/squad_adversarial_manual, shpotes/ms_coco, shpotes/tfcol, sijpapi/batch13, sijpapi/funsd, sijpapi/funsds, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/Genjer, somaimanguyat/Koboy, somaimanguyat/Movieonline2021, somaimanguyat/Salome, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/ms_marco_doc2query, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/c4-en-10k, stas/openwebtext-10k, stas/oscar-en-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stevhliu/demo, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, superb/superb-data, susumu2357/squad_v2_sv, svakulenk0/qrecc, svakulenk0/spoken_kgqa, svalabs/all-nli-german-translation-wmt19, svalabs/ms-marco-german-translation-wmt19, svanhvit/iceErrorCorpus, svanhvit/icelandic-ner-MIM-GOLD-NER, tals/test, tanfiona/causenet_wiki, tarudesu/UIT-ViCTSD, tau/scrolls, testOrganization01/test05, teven/c4_15M, teven/prompted_examples, teven/stackexchange, tharindu/MOLD, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, thomwolf/codeparrot-train, thomwolf/codeparrot-valid, thomwolf/codeparrot, thomwolf/github-dataset, thomwolf/github-python, thomwolf/very-good-dataset, thomwolf/very-test-dataset-2, thomwolf/very-test-dataset, tianbaoxiexxx/kvret, tianxing1994/temp, toloka/CrowdSpeech, toloka/VoxDIY-RusNews, tommy19970714/common_voice, toriving/kosimcse, toriving/talktalk-sentiment-210713-multi-singleturn-custom-multiturn, tranduyquang2205/vietnamese_dataset, transformersbook/codeparrot-train, transformersbook/codeparrot-valid, transformersbook/codeparrot, ttj/metadata_arxiv, turingbench/TuringBench, uasoyasser/rgfes, ujjawal1612/quora, unicamp-dl/mmarco, usc-isi/WikiConvert, uva-irlab/canard_quretec, uva-irlab/trec-cast-2019-multi-turn, uyeongjae/load_klue_re_agmented, vasudevgupta/amazon-ml-hack, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/gsoc-librispeech, vasudevgupta/natural-questions-validation, vasudevgupta/prml_data_contest, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vblagoje/eli5, vblagoje/eli5_support_docs, vblagoje/eli5v1, vblagoje/wikipedia_snippets_streamed, vctc92/sdsd, vctc92/test, vera-pro/ShadowLink, versae/bibles, versae/modernisa, versae/norwegian-t5-dataset-debug, versae/norwegian-t5-dataset-debug2, versae/norwegian-t5-dataset-debug3, vershasaxena91/datasets, vershasaxena91/squad_multitask, vesteinn/IC3, vesteinn/icelandic-ner-MIM-GOLD-NER, vidhur2k/multilingual-hate-speech, vishnun/huggingpics-data, vs4vijay/VizDS, w-nicole/childes_data, w-nicole/childes_data_no_tags, w-nicole/childes_data_no_tags_, w-nicole/childes_data_with_tags, w-nicole/childes_data_with_tags_, w11wo/imdb-javanese, wardenga/lsoie, webek18735/ddvoacantonesed, webek18735/dhikhscook, webimmunization/COVID-19-vaccine-attitude-tweets, webis/args_me, webis/conclugen, weijieliu/senteval_cn, wifis/ouivirtual, wikilee/ADFA_Mapping, wikimedia/wikipedia, wikimedia/wikisource, wisdomify/story, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, wzkariampuzha/EpiClassifySet, wzkariampuzha/EpiExtract4GARD, wzywzy/telegram_summary, x-tech/cantonese-mandarin-translations, xiaj/ds_test, xiaj/test0919, xiaobendanyn/demo, xiaobendanyn/nyt10, xiaobendanyn/tacred, yannobla/Sunshine, yazdipour/text-to-sparql-kdwd, yhavinga/mc4_nl_cleaned, yluisfern/PBU, yo/devparty, ytsaig/news-12factor, yuanchuan/annotated_reference_strings, yuvalkirstain/asset, yxchar/ag-tlm, yxchar/amazon-tlm, yxchar/chemprot-tlm, yxchar/citation_intent-tlm, yxchar/hyp-tlm, yxchar/imdb-tlm, yxchar/rct-20k-tlm, yxchar/sciie-tlm, z-uo/squad-it, zapsdcn/chemprot, zapsdcn/rct-20k, zf-org/org_dataset, zfaB4Hmm/test, zloelias/kinopoisk-reviews, zloelias/lenta-ru, zlucia/casehold, acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, americas_nli, ami, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, beans, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, biosses, blbooksgenre, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, casino, catalonia_independence, cats_vs_dogs, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cedr, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cmu_hinglish_dog, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, code_x_glue_cc_clone_detection_big_clone_bench, code_x_glue_cc_clone_detection_poj104, code_x_glue_cc_cloze_testing_all, code_x_glue_cc_cloze_testing_maxmin, code_x_glue_cc_code_completion_line, code_x_glue_cc_code_completion_token, code_x_glue_cc_code_refinement, code_x_glue_cc_code_to_code_trans, code_x_glue_cc_defect_detection, code_x_glue_ct_code_to_text, code_x_glue_tc_nl_code_search_adv, code_x_glue_tc_text_to_code, code_x_glue_tt_text_to_text, com_qa, common_gen, common_language, common_voice, commonsense_qa, competition_math, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, disfl_qa, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, eduge, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, food101, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, greek_legal_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hendrycks_test, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonli, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jigsaw_unintended_bias, jnlpba, journalists_questions, kan_hope, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, klue, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, lex_glue, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, masakhaner, math_dataset, math_qa, matinf, mbpp, mc4, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_eurlex, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, multidoc2dial, multilingual_librispeech, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openai_humaneval, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, riddle_sense, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, russian_super_glue, s2orc, samsum, sanskrit_classic, saudinewsnet, sberquad, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, sede, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2018_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, story_cloze, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, superb, swag, swahili, swahili_news, swda, swedish_medical_ner, swedish_ner_corpus, swedish_reviews, swiss_judgment_prediction, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, the_pile, the_pile_books3, the_pile_openwebtext2, the_pile_stack_exchange, tilde_model, time_dial, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, vivos, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xcsr, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, metaeval/crowdflower, metaeval/blimp_classification, metaeval/recast, metaeval/ethics, albertvillanova/tmp-tests-zip\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d00391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilde_ds = load_dataset('tilde_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b944a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5808c40c3b1f4d9983837725b2dd6ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9720205c83400eac9bd59f3500b7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tiny_shakespeare (/home/guntis/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('tiny_shakespeare', split='train')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b34f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTHRUS_DIR = f'{DATA_BASEDIR}/ftwc/playthru_data'\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    outfile = f\"{PTHRUS_DIR}/mingpt-{split}.textds\"\n",
    "    with open(outfile, \"w\") as out:\n",
    "        ptdirs = glob.glob(f\"{PTHRUS_DIR}/mingpt-{split}/*/\")\n",
    "        for ptdir in ptdirs:\n",
    "    #         print(\"ptdir=\", ptdir)\n",
    "            gamename = Path(ptdir).name\n",
    "            lines = []\n",
    "            for ptf in sorted(glob.glob(ptdir+'*.pthru')):\n",
    "    #             print(ptf)\n",
    "                with open(ptf, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            lines.append(line)\n",
    "            out.write(f'{{\"game\":\"{gamename}\"')\n",
    "            out.write(',\"text\":\"')\n",
    "            out.write(' <|> '.join(lines))\n",
    "            out.write('\"}')\n",
    "            out.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec9df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 48, 12, 11, 13, 11, 91, 117, 10, 11, 13, 16, 55, 10, 11, 13, 16, 56, 20, 10, 11, 13, 25, 55, 10, 11, 13, 16, 30, 10, 11, 13, 109, 56, 20, 10, 29, 10, 46, 9, 1, 14, 49, 12, 17, 9, 1, 40, 12, 11, 13, 11, 91, 67, 69, 68, 9, 1, 33, 134, 67, 69, 68, 110, 49, 35, 1, 37, 77, 43, 67, 69, 68, 24]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(f'{DATA_BASEDIR}/ftwc/ftwc_tokenizer_new.json')\n",
    "pttext = \"on counter : + raw + sliced tomato , + raw red apple , + raw red hot pepper ,\"+\\\n",
    "\" + raw yellow apple , + raw red onion , + raw green hot pepper , cookbook , knife ;\"+\\\n",
    "\" | on stove : nothing ; | carrying : + raw + sliced block of cheese ;\"+\\\n",
    "\" | >>>[ cook block of cheese with stove ]<<< | you fried the block of cheese .\"\n",
    "\n",
    "encoded_data = tokenizer.encode(pttext)\n",
    "print([tokid for tokid in encoded_data.ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcca33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >counter</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tomato</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >yellow</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >onion</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >green</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cookbook</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >knife</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >nothing</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >carrying</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >>>>[</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cook</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >]<<<</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >you</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >fried</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer)\n",
    "visualizer(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5aeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer2 = PreTrainedTokenizerFast(tokenizer_file=f'{DATA_BASEDIR}/ftwc/ftwc_tokenizer_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bea55ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14, 48, 12, 11, 13, 11, 91, 117, 10, 11, 13, 16, 55, 10, 11, 13, 16, 56, 20, 10, 11, 13, 25, 55, 10, 11, 13, 16, 30, 10, 11, 13, 109, 56, 20, 10, 29, 10, 46, 9, 1, 14, 49, 12, 17, 9, 1, 40, 12, 11, 13, 11, 91, 67, 69, 68, 9, 1, 33, 134, 67, 69, 68, 110, 49, 35, 1, 37, 77, 43, 67, 69, 68, 24], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819a30a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/ssd2tb/ftwc/playthru_data/mingpt-train.textds', 'valid': '/ssd2tb/ftwc/playthru_data/mingpt-valid.textds', 'test': '/ssd2tb/ftwc/playthru_data/mingpt-test.textds'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2c9feeb2b957550f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/guntis/.cache/huggingface/datasets/json/default-2c9feeb2b957550f/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c80924468a47218e2bf90c2716771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a658ccd2af114fc1aef62fcd24984c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/guntis/.cache/huggingface/datasets/json/default-2c9feeb2b957550f/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414615cd8057414f87cff221854747e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 2506\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dsfiles = {split: f\"{PTHRUS_DIR}/mingpt-{split}.textds\" for split in ['train', 'valid', 'test']}\n",
    "print(dsfiles)\n",
    "\n",
    "dataset = load_dataset('json', data_files=dsfiles)\n",
    "print(dataset)\n",
    "# batch = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c2f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5461c706cd104d099032b0ef1271b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e912df1d084efe924189da0d02bd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804a27ebb4c347578c7317ebea005e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_text(data:dict) -> dict:\n",
    "    data['input_ids'] = tokenizer.encode(data['text']).ids\n",
    "    return data\n",
    "\n",
    "# tokenized_ds = dataset.map(tokenize_text)\n",
    "tokenized_ds = dataset.map(lambda data: tokenizer2(data['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142e87a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 2506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6481ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game name: tw-cooking-recipe4+take4+cut+go6-0dLNsp9YtYOJcv5d\n",
      ">>>[ start ]<<< <|> --------- Do : find kitchen , read cookbook , eat meal . <|> -= corridor =- <|> Exits <|> east to unknown ; <|> south to unknown ; <|> >>>[ go south ]<<< <|> You go south. <|> -= livingroom =- <|> ON sofa : nothing ; <|> Exits <|>\n",
      "\n",
      ">>>[ start ]<<< <|> --------- do : find kitchen, read cookbook, eat meal. <|> -= corridor =- <|> exits <|> east to unknown ; <|> south to unknown ; <|> >>>[ go south ]<<< <|> you go south. <|> -= livingroom =- <|> on sofa : nothing ; <|> exits <|> east to unknown ; <|>\n",
      "\n",
      "[33, 114, 35, 8, 19, 27, 12, 75, 28, 10, 60, 29, 10, 61, 22, 24, 8, 36, 81, 34, 8, 41, 8, 50, 18, 26, 9, 8, 53, 18, 26, 9, 8, 33, 54, 53, 35, 8, 37, 54, 53, 24, 8, 36, 86, 34, 8, 14, 136, 12, 17, 9, 8, 41, 8, 50, 18, 26, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "_which_record = 0\n",
    "print(\"game name:\", tokenized_ds['test']['game'][0])\n",
    "print(tokenized_ds['test']['text'][_which_record][:250])\n",
    "print()\n",
    "sample_record = tokenized_ds['test']['input_ids'][_which_record]\n",
    "print(tokenizer2.decode(sample_record[:60]))\n",
    "print()\n",
    "print(sample_record[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: switch to using transformers.tokenizers.PreTrainedTokenizerFast\n",
    "# instead of huggingface.tokenizers.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75beac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_ds[train]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[valid]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[test]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.set_format(type='torch', columns=['input_ids'])\n",
    "for k in tokenized_ds:\n",
    "    print(f\"tokenized_ds[{k}]: {tokenized_ds[k].format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "023b0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids'])\n",
      "Number of rows in batch = 10\n",
      "Shape of row[0]= torch.Size([476])\n",
      "Record lengths= [476, 642, 647, 1895, 839, 1503, 1078, 1808, 709, 1680]\n"
     ]
    }
   ],
   "source": [
    "bat = tokenized_ds['valid'][0:10]\n",
    "print(bat.keys())\n",
    "bat1 = bat['input_ids']\n",
    "print(\"Number of rows in batch =\",len(bat1))\n",
    "print(\"Shape of row[0]=\", bat1[0].shape)\n",
    "print(\"Record lengths=\", [b.shape[0] for b in bat1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e097bb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>>[', 'start', ']<<<', '<|>', '---------', 'Do', ':', 'find', 'kitchen', ',', 'read', 'cookbook', ',', 'eat', 'meal', '.', '<|>', '-=', 'corridor', '=-', '<|>', 'Exits', '<|>', 'east', 'to', 'unknown', ';', '<|>', 'south', 'to', 'unknown', ';', '<|>', '>>>[', 'go', 'south', ']<<<', '<|>', 'You', 'go', 'south.', '<|>', '-=', 'livingroom', '=-', '<|>', 'ON', 'sofa', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', 'to', 'unknown', ';', '<|>', 'north', 'to', 'corridor', ';', '<|>', 'south', 'to', 'unknown', ';', '<|>', '>>>[', 'go', 'south', ']<<<', '<|>', 'You', 'go', 'south.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+closed', 'oven', ':', 'unknown', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', '>>>[', 'open', 'oven', ']<<<', '<|>', 'You', 'open', 'the', 'oven.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', '>>>[', 'read', 'cookbook', ']<<<', '<|>', 'You', 'read', 'the', 'recipe', '---------', 'Acquire', ':', 'green', 'hot', 'pepper', ',', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '---------', 'Do', ':', 'slice', 'green', 'hot', 'pepper', ',', 'roast', 'green', 'hot', 'pepper', ',', 'roast', 'red', 'potato', ',', 'prepare', 'meal', ';', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', '>>>[', 'take', 'green', 'hot', 'pepper', ']<<<', '<|>', 'You', 'take', 'the', 'green', 'hot', 'pepper', 'from', 'the', 'counter.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ';', '<|>', '>>>[', 'take', 'knife', ']<<<', '<|>', 'You', 'take', 'the', 'knife', 'from', 'the', 'counter.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ',', 'knife', ';', '<|>', '>>>[', 'slice', 'green', 'hot', 'pepper', 'with', 'knife', ']<<<', '<|>', 'You', 'slice', 'the', 'green', 'hot', 'pepper.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ';', '<|>', '>>>[', 'take', 'red', 'potato', ']<<<', '<|>', 'You', 'take', 'the', 'red', 'potato', 'from', 'the', 'counter.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '<|>', '>>>[', 'go', 'east', ']<<<', '<|>', 'You', 'go', 'east.', '<|>', '-=', 'pantry', '=-', '<|>', 'ON', 'shelf', ':', 'salt', ';', '<|>', 'Exits', '<|>', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '<|>', '>>>[', 'take', 'salt', ']<<<', '<|>', 'You', 'take', 'the', 'salt', 'from', 'the', 'shelf.', '<|>', '-=', 'pantry', '=-', '<|>', 'ON', 'shelf', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '<|>', '>>>[', 'go', 'west', ']<<<', '<|>', 'You', 'go', 'west.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '<|>', '>>>[', 'take', 'yellow', 'bell', 'pepper', ']<<<', '<|>', 'You', 'take', 'the', 'yellow', 'bell', 'pepper', 'from', 'the', 'fridge.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '<|>', '>>>[', 'prepare', 'meal', ']<<<', '<|>', 'You', 'prepare', 'the', 'meal.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', 'knife', ',', 'meal', ';', '<|>', '>>>[', 'eat', 'meal', ']<<<', '<|>', 'You', 'eat', 'the', 'meal.', '<|>', '-=', 'kitchen', '=-', '<|>', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '<|>', 'IN', '+open', 'oven', ':', 'nothing', ';', '<|>', 'ON', 'table', ':', 'cookbook', ';', '<|>', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '<|>', 'ON', 'stove', ':', 'nothing', ';', '<|>', 'Exits', '<|>', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '<|>', 'north', 'to', 'livingroom', ';', '<|>', 'Carrying', ':', 'knife', ';']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds['test']['text'][0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c09c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
