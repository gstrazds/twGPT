{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab3b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea891aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from datasets import list_datasets, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6092c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\r\n",
      "Version: 0.10.1\r\n",
      "Summary: Fast and Customizable Tokenizers\r\n",
      "Home-page: https://github.com/huggingface/tokenizers\r\n",
      "Author: Anthony MOI\r\n",
      "Author-email: anthony@huggingface.co\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: transformers\r\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca343624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.8.2\r\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\r\n",
      "Author-email: thomas@huggingface.co\r\n",
      "License: Apache\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: pyyaml, tqdm, requests, packaging, huggingface-hub, filelock, sacremoses, tokenizers, numpy, regex\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fedb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, catalonia_independence, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, code_x_glue_cc_clone_detection_big_clone_bench, code_x_glue_cc_clone_detection_poj104, code_x_glue_cc_cloze_testing_all, code_x_glue_cc_cloze_testing_maxmin, code_x_glue_cc_code_completion_line, code_x_glue_cc_code_completion_token, code_x_glue_cc_code_refinement, code_x_glue_cc_code_to_code_trans, code_x_glue_cc_defect_detection, code_x_glue_ct_code_to_text, code_x_glue_tc_nl_code_search_adv, code_x_glue_tc_text_to_code, code_x_glue_tt_text_to_text, com_qa, common_gen, common_voice, commonsense_qa, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, eduge, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hendrycks_test, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jnlpba, journalists_questions, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, klue, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, masakhaner, math_dataset, math_qa, matinf, mc4, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, swag, swahili, swahili_news, swda, swedish_ner_corpus, swedish_reviews, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, tilde_model, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, Abdo1Kamr/Arabic_Nine_Hadiths_Books, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Avishekavi/Avi, Binbin/my_dataset, CAGER/rick, Cropinky/flatearther, Cropinky/rap_lyrics_english, Cropinky/wow_fishing_bobber, Darren/data, EMBO/biolang, EMBO/sd-nlp, Ebtihal/OSCAR_Arabic, Eymen3455/xsum_tr, FRTNX/cosuju, Felix-ML/quoteli3, Firoj/CrisisBench, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, GalacticAI/Noirset, Gwangho/NCBI-Sars-Cov-2, Halilyesilceng/autonlp-data-nameEntityRecognition, HarleyQ/WitcherDialogue, Harveenchadha/Gujarati-Monolingual-Data, HarveyBWest/mybot, Jean-Baptiste/wikiner_fr, KETI-AIR/klue, KETI-AIR/kor_corpora, KETI-AIR/korquad, KETI-AIR/nikl, LIAMF-USP/arc-retrieval-c4, MKK/Dhivehi-English, MarianaSahagun/test, Melinoe/TheLabTexts, NTUYG/RAGTest, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, Ofrit/tmp, QA/abk-eng, Remesita/tagged_reviews, SCourthial/test, SajjadAyoubi/persian_qa, Shreesha/discord-rick-bot, TRoboto/masc, Tatyana/ru_sentiment_dataset, Terry0107/RiSAWOZ, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, XiangXiang/clt, Yves/fhnw_swiss_parliament, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/re_dial, ajmbell/test-dataset, albertvillanova/tests-raw-jsonl, alireza655/alireza655, allenai/c4, anukaver/EstQA, artrsousa/brwac-pt, artrsousa/oscar-pt, ashish-shrivastava/dont-know-dataset, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, caca/zscczs, canwenxu/dogwhistle, ccccccc/hdjw_94ejrjr, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, cheulyop/ksponspeech, clarin-pl/cst-wikinews, clarin-pl/kpwr-ner, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, cnrcastroli/aaaa, cointegrated/ParaNMT-Ru-Leipzig, congpt/dstc23_asr, corypaik/prost, ctl/ConceptualCaptions, dasago78/dasago78dataset, dataset/wikipedia_bn, david-wb/zeshel, deepset/germandpr, deepset/germanquad, dfgvhxfgv/fghghj, dfki-nlp/few-nerd, dgknrsln/Yorumsepeti, dispenst/jhghdghfd, dispix/test-dataset, dk-crazydiv/huggingface-modelhub, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ervis/aaa, ervis/qqq, fatvvs/autonlp-data-entity_model_conll2003, flax-community/german_common_crawl, flax-sentence-embeddings/paws-jsonl, flax-sentence-embeddings/stackexchange_title_body_jsonl, flax-sentence-embeddings/stackexchange_xml, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, fuliucansheng/mininlp, fuliucansheng/pascal_voc, fvillena/cantemist, fvillena/spanish_diagnostics, german-nlp-group/german_common_crawl, gmnlp/TICO19, godzillavskongonlinetv/ergfdg, godzillavskongonlinetv/godzillavskongfullmovie, gpt3mix/rt20, gpt3mix/sst2, gustavecortal/fr_covid_news, hartzeer/kdfjdshfje, hfface/poopi, howardmiddleton382/esuyertusutr, howardmiddleton382/wgweagwege, huggingFaceUser02/air21_grp13_inference_results, huggingFaceUser02/air21_grp13_tokenized_results, huseinzol05/translated-The-Pile, iamshsdf/sssssssssss, iarfmoose/question_generator, imthanhlv/binhvq_news21_raw, jaimin/wav2vec2-large-xlsr-gujarati-demo, jdepoix/junit_test_completion, jglaser/binding_affinity, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jiyoojeong/targetizer, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, julien-c/dummy-dataset-from-colab, julien-c/reactiongif, k-halid/ar, karinev/lanuitdudroit, katoensp/VR-OP, kmyoo/klue-tc-dev, lavis-nlp/german_legal_sentences, lewtun/binary_classification_dummy, lewtun/mnist-preds, lewtun/text_classification_dummy, lhoestq/custom_squad, lhoestq/squad, lhoestq/test, lhoestq/wikipedia_bn, lkiouiou/o9ui7877687, lohanna/testedjkcxkf, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, m3hrdadfi/recipe_nlg_lite, mad/IndonesiaNewsDataset, majod/CleanNaturalQuestionsDataset, makanan/umich, medzaf/test, metalearning/kaggale-nlp-tutorial, mksaad/Arabic_news, mldmm/glass_alloy_composition, mmm-da/rutracker_anime_torrent_titles, mrojas/abbreviation, mrojas/body, mrojas/disease, mrojas/family, mrojas/finding, mrojas/medication, mrojas/procedure, mulcyber/europarl-mono, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, nateraw/cats-and-dogs, nateraw/fairface, nateraw/image-folder, nateraw/test, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, nbroad/few-nerd, neelalex/raft-predictions, nielsr/FUNSD_layoutlmv2, nielsr/XFUN, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/llama_test, osanseviero/test, ought/raft, parivartanayurveda/Malesexproblemsayurvedictreatment, pasinit/xlwic, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, pelican/test_100, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, rays2pix/example, rays2pix/example_dataset, rewardsignal/reddit_writing_prompts, rony/soccer-dialogues, roskoN/dstc8-reddit-corpus, sagnikrayc/mctest, sagnikrayc/quasar, salesken/Paraphrase_category_detection, sdfufygvjh/fgghuviugviu, seamew/ChnSentiCorp, seamew/Hotel, seamew/THUCNews, seamew/Weibo, seamew/amazon_reviews_zh, seamew/weibo_avg, shahrukhx01/questions-vs-statements, sharejing/BiPaR, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/Genjer, somaimanguyat/Koboy, somaimanguyat/Movieonline2021, somaimanguyat/Salome, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/ms_marco_doc2query, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/openwebtext-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, susumu2357/squad_v2_sv, svalabs/all-nli-german-translation-wmt19, svalabs/ms-marco-german-translation-wmt19, tals/test, tanfiona/causenet_wiki, tarudesu/UIT-ViCTSD, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, thomwolf/github-dataset, thomwolf/github-python, tianxing1994/temp, tommy19970714/common_voice, turingbench/TuringBench, uasoyasser/rgfes, uva-irlab/canard_quretec, uva-irlab/trec-cast-2019-multi-turn, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/gsoc-librispeech, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vblagoje/wikipedia_snippets_streamed, vctc92/sdsd, vctc92/test, versae/adobo, vershasaxena91/datasets, vershasaxena91/squad_multitask, w-nicole/childes_data, w-nicole/childes_data_no_tags, w-nicole/childes_data_no_tags_, w-nicole/childes_data_with_tags, w-nicole/childes_data_with_tags_, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, weijieliu/senteval_cn, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, yluisfern/PBU\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d00391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilde_ds = load_dataset('tilde_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b944a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621ff5c09eb84b96b9dc6a1d2c445aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1622.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f1dcad6f71455fb82a2ff869e157ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=908.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tiny_shakespeare/default (download: 1.06 MiB, generated: 1.06 MiB, post-processed: Unknown size, total: 2.13 MiB) to /home/gstrazds/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c400a4cd9e4f369bc4094574706a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435071.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tiny_shakespeare downloaded and prepared to /home/gstrazds/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('tiny_shakespeare', split='train')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = '/work2/gstrazds'\n",
    "#DATA_HOME = '/ssd2tb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b34f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTHRUS_DIR = f'{DATA_HOME}/ftwc/playthru_data'\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    outfile = f\"{PTHRUS_DIR}/mingpt-{split}.textds\"\n",
    "    with open(outfile, \"w\") as out:\n",
    "        ptdirs = glob.glob(f\"{PTHRUS_DIR}/mingpt-{split}/*/\")\n",
    "        for ptdir in ptdirs:\n",
    "    #         print(\"ptdir=\", ptdir)\n",
    "            gamename = Path(ptdir).name\n",
    "            lines = []\n",
    "            for ptf in sorted(glob.glob(ptdir+'*.pthru')):\n",
    "    #             print(ptf)\n",
    "                with open(ptf, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            lines.append(line)\n",
    "            out.write(f'{{\"game\":\"{gamename}\"')\n",
    "            out.write(',\"text\":\"')\n",
    "            out.write(' | '.join(lines))\n",
    "            out.write('\"}')\n",
    "            out.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec9df76",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2335d373136b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'${DATA_HOME}/ftwc/ftwc_tokenizer_new.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpttext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"on counter : + raw + sliced tomato , + raw red apple , + raw red hot pepper ,\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\" + raw yellow apple , + raw red onion , + raw green hot pepper , cookbook , knife ;\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(f'${DATA_HOME}/ftwc/ftwc_tokenizer_new.json')\n",
    "pttext = \"on counter : + raw + sliced tomato , + raw red apple , + raw red hot pepper ,\"+\\\n",
    "\" + raw yellow apple , + raw red onion , + raw green hot pepper , cookbook , knife ;\"+\\\n",
    "\" | on stove : nothing ; | carrying : + raw + sliced block of cheese ;\"+\\\n",
    "\" | >>>[ cook block of cheese with stove ]<<< | you fried the block of cheese .\"\n",
    "\n",
    "encoded_data = tokenizer.encode(pttext)\n",
    "print([tokid for tokid in encoded_data.ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fcca33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >counter</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tomato</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >yellow</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >onion</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >green</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cookbook</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >knife</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >nothing</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >carrying</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >>>>[</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cook</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >]<<<</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >you</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >fried</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer)\n",
    "visualizer(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5aeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer2 = PreTrainedTokenizerFast(tokenizer_file='/ssd2tb/ftwc/ftwc_tokenizer_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bea55ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14, 45, 11, 9, 13, 9, 88, 120, 10, 9, 13, 18, 56, 10, 9, 13, 18, 55, 19, 10, 9, 13, 26, 56, 10, 9, 13, 18, 34, 10, 9, 13, 110, 55, 19, 10, 28, 10, 44, 8, 12, 14, 46, 11, 16, 8, 12, 39, 11, 9, 13, 9, 88, 68, 66, 67, 8, 12, 30, 121, 68, 66, 67, 108, 46, 33, 12, 35, 76, 41, 68, 66, 67, 24], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819a30a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/ssd2tb/ftwc/playthru_data/mingpt-train.textds', 'valid': '/ssd2tb/ftwc/playthru_data/mingpt-valid.textds', 'test': '/ssd2tb/ftwc/playthru_data/mingpt-test.textds'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9fb60e3e55c9afb0\n",
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/guntis/.cache/huggingface/datasets/json/default-9fb60e3e55c9afb0/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n",
      "Dataset json downloaded and prepared to /home/guntis/.cache/huggingface/datasets/json/default-9fb60e3e55c9afb0/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 2506\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['game', 'text'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dsfiles = {split: f\"{PTHRUS_DIR}/mingpt-{split}.textds\" for split in ['train', 'valid', 'test']}\n",
    "print(dsfiles)\n",
    "\n",
    "dataset = load_dataset('json', data_files=dsfiles)\n",
    "print(dataset)\n",
    "# batch = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c2f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.75ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.10ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.71ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(data:dict) -> dict:\n",
    "    data['input_ids'] = tokenizer.encode(data['text']).ids\n",
    "    return data\n",
    "\n",
    "# tokenized_ds = dataset.map(tokenize_text)\n",
    "tokenized_ds = dataset.map(lambda data: tokenizer2(data['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142e87a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 2506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'game', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6481ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game name: tw-cooking-recipe4+take4+cut+go6-0dLNsp9YtYOJcv5d\n",
      ">>>[ start ]<<< | --------- Do : find kitchen , read cookbook , eat meal . | -= corridor =- | Exits | east to unknown ; | south to unknown ; | >>>[ go south ]<<< | You go south. | -= livingroom =- | ON sofa : nothing ; | Exits | east to unknown ; | n\n",
      "\n",
      ">>>[ start ]<<< | --------- do : find kitchen, read cookbook, eat meal. | -= corridor =- | exits | east to unknown ; | south to unknown ; | >>>[ go south ]<<< | you go south. | -= livingroom =- | on sofa : nothing ; | exits | east to unknown ; |\n",
      "\n",
      "[30, 114, 33, 12, 21, 37, 11, 79, 27, 10, 61, 28, 10, 62, 25, 24, 12, 31, 77, 32, 12, 40, 12, 49, 17, 23, 8, 12, 52, 17, 23, 8, 12, 30, 53, 52, 33, 12, 35, 53, 52, 24, 12, 31, 85, 32, 12, 14, 128, 11, 16, 8, 12, 40, 12, 49, 17, 23, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "_which_record = 0\n",
    "print(\"game name:\", tokenized_ds['test']['game'][0])\n",
    "print(tokenized_ds['test']['text'][_which_record][:250])\n",
    "print()\n",
    "sample_record = tokenized_ds['test']['input_ids'][_which_record]\n",
    "print(tokenizer2.decode(sample_record[:60]))\n",
    "print()\n",
    "print(sample_record[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: switch to using transformers.tokenizers.PreTrainedTokenizerFast\n",
    "# instead of huggingface.tokenizers.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75beac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_ds[train]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[valid]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n",
      "tokenized_ds[test]: {'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.set_format(type='torch', columns=['input_ids'])\n",
    "for k in tokenized_ds:\n",
    "    print(f\"tokenized_ds[{k}]: {tokenized_ds[k].format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023b0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids'])\n",
      "Number of rows in batch = 10\n",
      "Shape of row[0]= torch.Size([476])\n",
      "Record lengths= [476, 642, 647, 1895, 839, 1503, 1078, 1808, 709, 1680]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/formatting/formatting.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "/home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:44: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n"
     ]
    }
   ],
   "source": [
    "bat = tokenized_ds['valid'][0:10]\n",
    "print(bat.keys())\n",
    "bat1 = bat['input_ids']\n",
    "print(\"Number of rows in batch =\",len(bat1))\n",
    "print(\"Shape of row[0]=\", bat1[0].shape)\n",
    "print(\"Record lengths=\", [b.shape[0] for b in bat1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e097bb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>>[', 'start', ']<<<', '|', '---------', 'Do', ':', 'find', 'kitchen', ',', 'read', 'cookbook', ',', 'eat', 'meal', '.', '|', '-=', 'corridor', '=-', '|', 'Exits', '|', 'east', 'to', 'unknown', ';', '|', 'south', 'to', 'unknown', ';', '|', '>>>[', 'go', 'south', ']<<<', '|', 'You', 'go', 'south.', '|', '-=', 'livingroom', '=-', '|', 'ON', 'sofa', ':', 'nothing', ';', '|', 'Exits', '|', 'east', 'to', 'unknown', ';', '|', 'north', 'to', 'corridor', ';', '|', 'south', 'to', 'unknown', ';', '|', '>>>[', 'go', 'south', ']<<<', '|', 'You', 'go', 'south.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+closed', 'oven', ':', 'unknown', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'open', 'oven', ']<<<', '|', 'You', 'open', 'the', 'oven.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'read', 'cookbook', ']<<<', '|', 'You', 'read', 'the', 'recipe', '---------', 'Acquire', ':', 'green', 'hot', 'pepper', ',', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '---------', 'Do', ':', 'slice', 'green', 'hot', 'pepper', ',', 'roast', 'green', 'hot', 'pepper', ',', 'roast', 'red', 'potato', ',', 'prepare', 'meal', ';', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'green', 'hot', 'pepper', ',', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', '>>>[', 'take', 'green', 'hot', 'pepper', ']<<<', '|', 'You', 'take', 'the', 'green', 'hot', 'pepper', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ',', 'knife', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ';', '|', '>>>[', 'take', 'knife', ']<<<', '|', 'You', 'take', 'the', 'knife', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', 'green', 'hot', 'pepper', ',', 'knife', ';', '|', '>>>[', 'slice', 'green', 'hot', 'pepper', 'with', 'knife', ']<<<', '|', 'You', 'slice', 'the', 'green', 'hot', 'pepper.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+roasted', 'red', 'potato', ',', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ';', '|', '>>>[', 'take', 'red', 'potato', ']<<<', '|', 'You', 'take', 'the', 'red', 'potato', 'from', 'the', 'counter.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'unknown', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '|', '>>>[', 'go', 'east', ']<<<', '|', 'You', 'go', 'east.', '|', '-=', 'pantry', '=-', '|', 'ON', 'shelf', ':', 'salt', ';', '|', 'Exits', '|', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ';', '|', '>>>[', 'take', 'salt', ']<<<', '|', 'You', 'take', 'the', 'salt', 'from', 'the', 'shelf.', '|', '-=', 'pantry', '=-', '|', 'ON', 'shelf', ':', 'nothing', ';', '|', 'Exits', '|', 'west', '+open', 'plain', 'door', 'to', 'kitchen', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '|', '>>>[', 'go', 'west', ']<<<', '|', 'You', 'go', 'west.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', 'yellow', 'bell', 'pepper', ',', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ';', '|', '>>>[', 'take', 'yellow', 'bell', 'pepper', ']<<<', '|', 'You', 'take', 'the', 'yellow', 'bell', 'pepper', 'from', 'the', 'fridge.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', '+roasted', '+sliced', 'green', 'hot', 'pepper', ',', 'knife', ',', '+roasted', 'red', 'potato', ',', 'salt', ',', 'yellow', 'bell', 'pepper', ';', '|', '>>>[', 'prepare', 'meal', ']<<<', '|', 'You', 'prepare', 'the', 'meal.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', 'knife', ',', 'meal', ';', '|', '>>>[', 'eat', 'meal', ']<<<', '|', 'You', 'eat', 'the', 'meal.', '|', '-=', 'kitchen', '=-', '|', 'IN', '+open', 'fridge', ':', '+raw', 'yellow', 'onion', ',', '+raw', 'red', 'hot', 'pepper', ';', '|', 'IN', '+open', 'oven', ':', 'nothing', ';', '|', 'ON', 'table', ':', 'cookbook', ';', '|', 'ON', 'counter', ':', '+raw', 'orange', 'bell', 'pepper', ',', '+raw', 'red', 'apple', ',', '+raw', 'yellow', 'potato', ';', '|', 'ON', 'stove', ':', 'nothing', ';', '|', 'Exits', '|', 'east', '+open', 'plain', 'door', 'to', 'pantry', ';', '|', 'north', 'to', 'livingroom', ';', '|', 'Carrying', ':', 'knife', ';']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds['test']['text'][0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c09c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
