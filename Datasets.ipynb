{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "diagnostic-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from datasets import list_datasets, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lyric-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\r\n",
      "Version: 0.10.2\r\n",
      "Summary: Fast and Customizable Tokenizers\r\n",
      "Home-page: https://github.com/huggingface/tokenizers\r\n",
      "Author: Anthony MOI\r\n",
      "Author-email: anthony@huggingface.co\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: transformers\r\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.6.0\r\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\r\n",
      "Author-email: thomas@huggingface.co\r\n",
      "License: Apache\r\n",
      "Location: /home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages/transformers-4.6.0-py3.8.egg\r\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, regex, requests, sacremoses, tokenizers, tqdm\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n",
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, catalonia_independence, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, com_qa, common_gen, common_voice, commonsense_qa, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jnlpba, journalists_questions, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, math_dataset, math_qa, matinf, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, swag, swahili, swahili_news, swda, swedish_ner_corpus, swedish_reviews, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, tilde_model, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Avishekavi/Avi, Binbin/my_dataset, EMBO/biolang, EMBO/sd-nlp, Eymen3455/xsum_tr, FRTNX/cosuju, Firoj/CrisisBench, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Halilyesilceng/autonlp-data-nameEntityRecognition, Harveenchadha/Gujarati-Monolingual-Data, Jean-Baptiste/wikiner_fr, MarianaSahagun/test, Melinoe/TheLabTexts, NTUYG/RAGTest, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, Ofrit/tmp, QA/abk-eng, Remesita/tagged_reviews, SajjadAyoubi/persian_qa, Salesforce/QAConv, TRoboto/masc, Terry0107/RiSAWOZ, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, XiangXiang/clt, Yves/fhnw_swiss_parliament, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/re_dial, ajmbell/test-dataset, alireza655/alireza655, allenai/c4, ancs21/viwiki-18042021, anukaver/EstQA, aschvin/fhnw_test, ashish-shrivastava/dont-know-dataset, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, caca/zscczs, canwenxu/dogwhistle, ccccccc/hdjw_94ejrjr, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, clarin-pl/cst-wikinews, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, cnrcastroli/aaaa, congpt/dstc23_asr, dasago78/dasago78dataset, david-wb/zeshel, dfgvhxfgv/fghghj, dispenst/jhghdghfd, dispix/test-dataset, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ervis/aaa, ervis/qqq, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, german-nlp-group/german_common_crawl, godzillavskongonlinetv/ergfdg, godzillavskongonlinetv/godzillavskongfullmovie, gustavecortal/fr_covid_news, hartzeer/kdfjdshfje, hfface/poopi, huseinzol05/translated-The-Pile, iamshsdf/sssssssssss, jaimin/wav2vec2-large-xlsr-gujarati-demo, jdepoix/junit_test_completion, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, joelito/ler, joelito/sem_eval_2010_task_8, julien-c/dummy-dataset-from-colab, k-halid/ar, katoensp/VR-OP, lavis-nlp/german_legal_sentences, lhoestq/custom_squad, lhoestq/squad, lhoestq/test, lhoestq/wikipedia_bn, lkiouiou/o9ui7877687, lohanna/testedjkcxkf, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, majod/CleanNaturalQuestionsDataset, makanan/umich, medzaf/test, metalearning/kaggale-nlp-tutorial, mksaad/Arabic_news, mmm-da/rutracker_anime_torrent_titles, mohsenfayyaz/toxicity-classification-datasets, mulcyber/europarl-mono, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/llama_test, parivartanayurveda/Malesexproblemsayurvedictreatment, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, projectaligned/reddit_writingprompts_full, rony/soccer-dialogues, roskoN/dstc8-reddit-corpus, salesken/Paraphrase_category_detection, sdfufygvjh/fgghuviugviu, seamew/Weibo, seamew/amazon_reviews_zh, seamew/weibo_avg, sharejing/BiPaR, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/openwebtext-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stiel/skjdhjkasdhasjkd, susumu2357/squad_v2_sv, tals/test, tommy19970714/common_voice, tommy19970714/jsut_asr, tommy19970714/jsut_asr_hiragana, tommy19970714/jsut_asr_hiragana_small, tommy19970714/laborotvspeech, toriving/agnews, toriving/atis-intent, toriving/cola, toriving/cr, toriving/dbpedia-content, toriving/dbpedia, toriving/imdb, toriving/mpqa, toriving/mr, toriving/rotten-tomato, toriving/snips-intent, toriving/sst2, toriving/sst5, toriving/subj, toriving/trec6, toriving/yahoo-answers, toriving/yelp, turingbench/TuringBench, uasoyasser/rgfes, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, versae/adobo, vershasaxena91/datasets, vershasaxena91/squad_multitask, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, yluisfern/PBU\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unnecessary-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tiny_shakespeare (/home/guntis/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('tiny_shakespeare', split='train')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italic-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTHRUS_DIR = '/ssd2tb/ftwc/playthru_data'\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    outfile = f\"{PTHRUS_DIR}/mingpt-{split}.textds\"\n",
    "    with open(outfile, \"w\") as out:\n",
    "        ptdirs = glob.glob(f\"{PTHRUS_DIR}/mingpt-{split}/*/\")\n",
    "        for ptdir in ptdirs:\n",
    "    #         print(\"ptdir=\", ptdir)\n",
    "            lines = []\n",
    "            for ptf in sorted(glob.glob(ptdir+'*.pthru')):\n",
    "    #             print(ptf)\n",
    "                with open(ptf, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            lines.append(line)\n",
    "            out.write(' | '.join(lines))\n",
    "            out.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expired-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 45, 11, 9, 13, 9, 88, 120, 10, 9, 13, 18, 56, 10, 9, 13, 18, 55, 19, 10, 9, 13, 26, 56, 10, 9, 13, 18, 34, 10, 9, 13, 110, 55, 19, 10, 28, 10, 44, 8, 12, 14, 46, 11, 16, 8, 12, 39, 11, 9, 13, 9, 88, 68, 66, 67, 8, 12, 30, 121, 68, 66, 67, 108, 46, 33, 12, 35, 76, 41, 68, 66, 67, 24]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file('/ssd2tb/ftwc/ftwc_tokenizer_new.json')\n",
    "pttext = \"on counter : + raw + sliced tomato , + raw red apple , + raw red hot pepper ,\"+\\\n",
    "\" + raw yellow apple , + raw red onion , + raw green hot pepper , cookbook , knife ;\"+\\\n",
    "\" | on stove : nothing ; | carrying : + raw + sliced block of cheese ;\"+\\\n",
    "\" | >>>[ cook block of cheese with stove ]<<< | you fried the block of cheese .\"\n",
    "\n",
    "encoded_data = tokenizer.encode(pttext)\n",
    "print([tokid for tokid in encoded_data.ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "passing-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >counter</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tomato</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >yellow</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >apple</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >red</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >onion</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >green</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >hot</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >pepper</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cookbook</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >knife</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >on</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >nothing</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >carrying</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >:</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >raw</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >+</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >sliced</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >;</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >>>>[</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >cook</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >stove</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >]<<<</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >|</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >you</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >fried</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >block</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >of</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >cheese</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "visualizer = EncodingVisualizer(tokenizer)\n",
    "visualizer(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sunrise-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer2 = PreTrainedTokenizerFast(tokenizer_file='/ssd2tb/ftwc/ftwc_tokenizer_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alone-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14, 45, 11, 9, 13, 9, 88, 120, 10, 9, 13, 18, 56, 10, 9, 13, 18, 55, 19, 10, 9, 13, 26, 56, 10, 9, 13, 18, 34, 10, 9, 13, 110, 55, 19, 10, 28, 10, 44, 8, 12, 14, 46, 11, 16, 8, 12, 39, 11, 9, 13, 9, 88, 68, 66, 67, 8, 12, 30, 121, 68, 66, 67, 108, 46, 33, 12, 35, 76, 41, 68, 66, 67, 24], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(pttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "common-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/ssd2tb/ftwc/playthru_data/mingpt-train.textds', 'valid': '/ssd2tb/ftwc/playthru_data/mingpt-valid.textds', 'test': '/ssd2tb/ftwc/playthru_data/mingpt-test.textds'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-621faf0f23301563\n",
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/guntis/.cache/huggingface/datasets/text/default-621faf0f23301563/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "Dataset text downloaded and prepared to /home/guntis/.cache/huggingface/datasets/text/default-621faf0f23301563/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2506\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dsfiles = {split: f\"{PTHRUS_DIR}/mingpt-{split}.textds\" for split in ['train', 'valid', 'test']}\n",
    "print(dsfiles)\n",
    "\n",
    "dataset = load_dataset('text', data_files=dsfiles)\n",
    "print(dataset)\n",
    "# batch = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hungry-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  3.00ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.87ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.08ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(data:dict) -> dict:\n",
    "    data['input_ids'] = tokenizer.encode(data['text']).ids\n",
    "    return data\n",
    "\n",
    "# tokenized_ds = dataset.map(tokenize_text)\n",
    "tokenized_ds = dataset.map(lambda data: tokenizer2(data['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "simplified-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 2506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "absolute-raleigh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>>>[ start ]<<< | --------- do : find kitchen, read cookbook, eat meal. | -= corridor =- | exits | east to unknown ; | south to unknown ; | >>>[ go south ]<<< | you go south. | -= livingroom =- | on sofa : nothing ; | exits | east to unknown ; | north to corridor ; | south to unknown ; | >>>[ go south ]<<< | you go south. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + closed oven : unknown ; | on table : cookbook ; | on counter : + roasted green hot pepper, + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato, knife ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | >>>[ open oven ]<<< | you open the oven. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + roasted green hot pepper, + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato, knife ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | >>>[ read cookbook ]<<< | you read the recipe --------- acquire : green hot pepper, red potato, salt, yellow bell pepper ; --------- do : slice green hot pepper, roast green hot pepper, roast red potato, prepare meal ; | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + roasted green hot pepper, + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato, knife ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | >>>[ take green hot pepper ]<<< | you take the green hot pepper from the counter. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato, knife ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | carrying : + roasted green hot pepper ; | >>>[ take knife ]<<< | you take the knife from the counter. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | carrying : + roasted green hot pepper, knife ; | >>>[ slice green hot pepper with knife ]<<< | you slice the green hot pepper. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + roasted red potato, + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | carrying : + roasted + sliced green hot pepper, knife ; | >>>[ take red potato ]<<< | you take the red potato from the counter. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to unknown ; | north to livingroom ; | carrying : + roasted + sliced green hot pepper, knife, + roasted red potato ; | >>>[ go east ]<<< | you go east. | -= pantry =- | on shelf : salt ; | exits | west + open plain door to kitchen ; | carrying : + roasted + sliced green hot pepper, knife, + roasted red potato ; | >>>[ take salt ]<<< | you take the salt from the shelf. | -= pantry =- | on shelf : nothing ; | exits | west + open plain door to kitchen ; | carrying : + roasted + sliced green hot pepper, knife, + roasted red potato, salt ; | >>>[ go west ]<<< | you go west. | -= kitchen =- | in + open fridge : yellow bell pepper, + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to pantry ; | north to livingroom ; | carrying : + roasted + sliced green hot pepper, knife, + roasted red potato, salt ; | >>>[ take yellow bell pepper ]<<< | you take the yellow bell pepper from the fridge. | -= kitchen =- | in + open fridge : + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to pantry ; | north to livingroom ; | carrying : + roasted + sliced green hot pepper, knife, + roasted red potato, salt, yellow bell pepper ; | >>>[ prepare meal ]<<< | you prepare the meal. | -= kitchen =- | in + open fridge : + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to pantry ; | north to livingroom ; | carrying : knife, meal ; | >>>[ eat meal ]<<< | you eat the meal. | -= kitchen =- | in + open fridge : + raw yellow onion, + raw red hot pepper ; | in + open oven : nothing ; | on table : cookbook ; | on counter : + raw orange bell pepper, + raw red apple, + raw yellow potato ; | on stove : nothing ; | exits | east + open plain door to pantry ; | north to livingroom ; | carrying : knife ;'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(tokenized_ds['test'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lightweight-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: switch to using transformers.tokenizers.PreTrainedTokenizerFast\n",
    "# instead of huggingface.tokenizers.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "disturbed-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tokenizers:\n",
      "\n",
      "NAME\n",
      "    tokenizers\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    decoders (package)\n",
      "    implementations (package)\n",
      "    models (package)\n",
      "    normalizers (package)\n",
      "    pre_tokenizers (package)\n",
      "    processors (package)\n",
      "    tokenizers\n",
      "    tools (package)\n",
      "    trainers (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        AddedToken\n",
      "        Encoding\n",
      "        NormalizedString\n",
      "        PreTokenizedString\n",
      "        Regex\n",
      "        Token\n",
      "        Tokenizer\n",
      "    enum.Enum(builtins.object)\n",
      "        OffsetReferential\n",
      "        OffsetType\n",
      "        SplitDelimiterBehavior\n",
      "    \n",
      "    class AddedToken(builtins.object)\n",
      "     |  AddedToken(self, content, single_word=False, lstrip=False, rstrip=False, normalized=True)\n",
      "     |  \n",
      "     |  Represents a token that can be be added to a :class:`~tokenizers.Tokenizer`.\n",
      "     |  It can have special options that defines the way it should behave.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      content (:obj:`str`): The content of the token\n",
      "     |  \n",
      "     |      single_word (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |          Defines whether this token should only match single words. If :obj:`True`, this\n",
      "     |          token will never match inside of a word. For example the token ``ing`` would match\n",
      "     |          on ``tokenizing`` if this option is :obj:`False`, but not if it is :obj:`True`.\n",
      "     |          The notion of \"`inside of a word`\" is defined by the word boundaries pattern in\n",
      "     |          regular expressions (ie. the token should start and end with word boundaries).\n",
      "     |  \n",
      "     |      lstrip (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |          Defines whether this token should strip all potential whitespaces on its left side.\n",
      "     |          If :obj:`True`, this token will greedily match any whitespace on its left. For\n",
      "     |          example if we try to match the token ``[MASK]`` with ``lstrip=True``, in the text\n",
      "     |          ``\"I saw a [MASK]\"``, we would match on ``\" [MASK]\"``. (Note the space on the left).\n",
      "     |  \n",
      "     |      rstrip (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |          Defines whether this token should strip all potential whitespaces on its right\n",
      "     |          side. If :obj:`True`, this token will greedily match any whitespace on its right.\n",
      "     |          It works just like :obj:`lstrip` but on the right.\n",
      "     |  \n",
      "     |      normalized (:obj:`bool`, defaults to :obj:`True` with :meth:`~tokenizers.Tokenizer.add_tokens` and :obj:`False` with :meth:`~tokenizers.Tokenizer.add_special_tokens`):\n",
      "     |          Defines whether this token should match against the normalized version of the input\n",
      "     |          text. For example, with the added token ``\"yesterday\"``, and a normalizer in charge of\n",
      "     |          lowercasing the text, the token could be extract from the input ``\"I saw a lion\n",
      "     |          Yesterday\"``.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  content\n",
      "     |      Get the content of this :obj:`AddedToken`\n",
      "     |  \n",
      "     |  lstrip\n",
      "     |      Get the value of the :obj:`lstrip` option\n",
      "     |  \n",
      "     |  normalized\n",
      "     |      Get the value of the :obj:`normalized` option\n",
      "     |  \n",
      "     |  rstrip\n",
      "     |      Get the value of the :obj:`rstrip` option\n",
      "     |  \n",
      "     |  single_word\n",
      "     |      Get the value of the :obj:`single_word` option\n",
      "    \n",
      "    class Encoding(builtins.object)\n",
      "     |  The :class:`~tokenizers.Encoding` represents the output of a :class:`~tokenizers.Tokenizer`.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  char_to_token(self, char_pos, sequence_index=0)\n",
      "     |      Get the token that contains the char at the given position in the input sequence.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          char_pos (:obj:`int`):\n",
      "     |              The position of a char in the input string\n",
      "     |          sequence_index (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The index of the sequence that contains the target char\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The index of the token that contains this char in the encoded sequence\n",
      "     |  \n",
      "     |  char_to_word(self, char_pos, sequence_index=0)\n",
      "     |      Get the word that contains the char at the given position in the input sequence.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          char_pos (:obj:`int`):\n",
      "     |              The position of a char in the input string\n",
      "     |          sequence_index (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The index of the sequence that contains the target char\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The index of the word that contains this char in the input sequence\n",
      "     |  \n",
      "     |  pad(self, length, direction='right', pad_id=0, pad_type_id=0, pad_token='[PAD]')\n",
      "     |      Pad the :class:`~tokenizers.Encoding` at the given length\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          length (:obj:`int`):\n",
      "     |              The desired length\n",
      "     |      \n",
      "     |          direction: (:obj:`str`, defaults to :obj:`right`):\n",
      "     |              The expected padding direction. Can be either :obj:`right` or :obj:`left`\n",
      "     |      \n",
      "     |          pad_id (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The ID corresponding to the padding token\n",
      "     |      \n",
      "     |          pad_type_id (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The type ID corresponding to the padding token\n",
      "     |      \n",
      "     |          pad_token (:obj:`str`, defaults to `[PAD]`):\n",
      "     |              The pad token to use\n",
      "     |  \n",
      "     |  set_sequence_id(self, sequence_id)\n",
      "     |      Set the given sequence index\n",
      "     |      \n",
      "     |      Set the given sequence index for the whole range of tokens contained in this\n",
      "     |      :class:`~tokenizers.Encoding`.\n",
      "     |  \n",
      "     |  token_to_chars(self, token_index)\n",
      "     |      Get the offsets of the token at the given index.\n",
      "     |      \n",
      "     |      The returned offsets are related to the input sequence that contains the\n",
      "     |      token.  In order to determine in which input sequence it belongs, you\n",
      "     |      must call :meth:`~tokenizers.Encoding.token_to_sequence()`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_index (:obj:`int`):\n",
      "     |              The index of a token in the encoded sequence.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Tuple[int, int]`: The token offsets :obj:`(first, last + 1)`\n",
      "     |  \n",
      "     |  token_to_sequence(self, token_index)\n",
      "     |      Get the index of the sequence represented by the given token.\n",
      "     |      \n",
      "     |      In the general use case, this method returns :obj:`0` for a single sequence or\n",
      "     |      the first sequence of a pair, and :obj:`1` for the second sequence of a pair\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_index (:obj:`int`):\n",
      "     |              The index of a token in the encoded sequence.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The sequence id of the given token\n",
      "     |  \n",
      "     |  token_to_word(self, token_index)\n",
      "     |      Get the index of the word that contains the token in one of the input sequences.\n",
      "     |      \n",
      "     |      The returned word index is related to the input sequence that contains\n",
      "     |      the token.  In order to determine in which input sequence it belongs, you\n",
      "     |      must call :meth:`~tokenizers.Encoding.token_to_sequence()`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_index (:obj:`int`):\n",
      "     |              The index of a token in the encoded sequence.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The index of the word in the relevant input sequence.\n",
      "     |  \n",
      "     |  truncate(self, max_length, stride=0)\n",
      "     |      Truncate the :class:`~tokenizers.Encoding` at the given length\n",
      "     |      \n",
      "     |      If this :class:`~tokenizers.Encoding` represents multiple sequences, when truncating\n",
      "     |      this information is lost. It will be considered as representing a single sequence.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          max_length (:obj:`int`):\n",
      "     |              The desired length\n",
      "     |      \n",
      "     |          stride (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The length of previous content to be included in each overflowing piece\n",
      "     |  \n",
      "     |  word_to_chars(self, word_index, sequence_index=0)\n",
      "     |      Get the offsets of the word at the given index in one of the input sequences.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          word_index (:obj:`int`):\n",
      "     |              The index of a word in one of the input sequences.\n",
      "     |          sequence_index (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The index of the sequence that contains the target word\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Tuple[int, int]`: The range of characters (span) :obj:`(first, last + 1)`\n",
      "     |  \n",
      "     |  word_to_tokens(self, word_index, sequence_index=0)\n",
      "     |      Get the encoded tokens corresponding to the word at the given index\n",
      "     |      in one of the input sequences.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          word_index (:obj:`int`):\n",
      "     |              The index of a word in one of the input sequences.\n",
      "     |          sequence_index (:obj:`int`, defaults to :obj:`0`):\n",
      "     |              The index of the sequence that contains the target word\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Tuple[int, int]`: The range of tokens: :obj:`(first, last + 1)`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  merge(encodings, growing_offsets=True)\n",
      "     |      Merge the list of encodings into one final :class:`~tokenizers.Encoding`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          encodings (A :obj:`List` of :class:`~tokenizers.Encoding`):\n",
      "     |              The list of encodings that should be merged in one\n",
      "     |      \n",
      "     |          growing_offsets (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether the offsets should accumulate while merging\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Encoding`: The resulting Encoding\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  attention_mask\n",
      "     |      The attention mask\n",
      "     |      \n",
      "     |      This indicates to the LM which tokens should be attended to, and which should not.\n",
      "     |      This is especially important when batching sequences, where we need to applying\n",
      "     |      padding.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |         :obj:`List[int]`: The attention mask\n",
      "     |  \n",
      "     |  ids\n",
      "     |      The generated IDs\n",
      "     |      \n",
      "     |      The IDs are the main input to a Language Model. They are the token indices,\n",
      "     |      the numerical representations that a LM understands.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`List[int]`: The list of IDs\n",
      "     |  \n",
      "     |  n_sequences\n",
      "     |      The number of sequences represented\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The number of sequences in this :class:`~tokenizers.Encoding`\n",
      "     |  \n",
      "     |  offsets\n",
      "     |      The offsets associated to each token\n",
      "     |      \n",
      "     |      These offsets let's you slice the input string, and thus retrieve the original\n",
      "     |      part that led to producing the corresponding token.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A :obj:`List` of :obj:`Tuple[int, int]`: The list of offsets\n",
      "     |  \n",
      "     |  overflowing\n",
      "     |      A :obj:`List` of overflowing :class:`~tokenizers.Encoding`\n",
      "     |      \n",
      "     |      When using truncation, the :class:`~tokenizers.Tokenizer` takes care of splitting\n",
      "     |      the output into as many pieces as required to match the specified maximum length.\n",
      "     |      This field lets you retrieve all the subsequent pieces.\n",
      "     |      \n",
      "     |      When you use pairs of sequences, the overflowing pieces will contain enough\n",
      "     |      variations to cover all the possible combinations, while respecting the provided\n",
      "     |      maximum length.\n",
      "     |  \n",
      "     |  sequence_ids\n",
      "     |      The generated sequence indices.\n",
      "     |      \n",
      "     |      They represent the index of the input sequence associated to each token.\n",
      "     |      The sequence id can be None if the token is not related to any input sequence,\n",
      "     |      like for example with special tokens.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A :obj:`List` of :obj:`Optional[int]`: A list of optional sequence index.\n",
      "     |  \n",
      "     |  special_tokens_mask\n",
      "     |      The special token mask\n",
      "     |      \n",
      "     |      This indicates which tokens are special tokens, and which are not.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`List[int]`: The special tokens mask\n",
      "     |  \n",
      "     |  tokens\n",
      "     |      The generated tokens\n",
      "     |      \n",
      "     |      They are the string representation of the IDs.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`List[str]`: The list of tokens\n",
      "     |  \n",
      "     |  type_ids\n",
      "     |      The generated type IDs\n",
      "     |      \n",
      "     |      Generally used for tasks like sequence classification or question answering,\n",
      "     |      these tokens let the LM know which input sequence corresponds to each tokens.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`List[int]`: The list of type ids\n",
      "     |  \n",
      "     |  word_ids\n",
      "     |      The generated word indices.\n",
      "     |      \n",
      "     |      They represent the index of the word associated to each token.\n",
      "     |      When the input is pre-tokenized, they correspond to the ID of the given input label,\n",
      "     |      otherwise they correspond to the words indices as defined by the\n",
      "     |      :class:`~tokenizers.pre_tokenizers.PreTokenizer` that was used.\n",
      "     |      \n",
      "     |      For special tokens and such (any token that was generated from something that was\n",
      "     |      not part of the input), the output is :obj:`None`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A :obj:`List` of :obj:`Optional[int]`: A list of optional word index.\n",
      "     |  \n",
      "     |  words\n",
      "     |      The generated word indices.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          This is deprecated and will be removed in a future version.\n",
      "     |          Please use :obj:`~tokenizers.Encoding.word_ids` instead.\n",
      "     |      \n",
      "     |      They represent the index of the word associated to each token.\n",
      "     |      When the input is pre-tokenized, they correspond to the ID of the given input label,\n",
      "     |      otherwise they correspond to the words indices as defined by the\n",
      "     |      :class:`~tokenizers.pre_tokenizers.PreTokenizer` that was used.\n",
      "     |      \n",
      "     |      For special tokens and such (any token that was generated from something that was\n",
      "     |      not part of the input), the output is :obj:`None`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A :obj:`List` of :obj:`Optional[int]`: A list of optional word index.\n",
      "    \n",
      "    class NormalizedString(builtins.object)\n",
      "     |  NormalizedString\n",
      "     |  \n",
      "     |  A NormalizedString takes care of modifying an \"original\" string, to obtain a \"normalized\" one.\n",
      "     |  While making all the requested modifications, it keeps track of the alignment information\n",
      "     |  between the two versions of the string.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequence: str:\n",
      "     |          The string sequence used to initialize this NormalizedString\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  append(self, s)\n",
      "     |      Append the given sequence to the string\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clears the string\n",
      "     |  \n",
      "     |  filter(self, func)\n",
      "     |      Filter each character of the string using the given func\n",
      "     |  \n",
      "     |  for_each(self, func)\n",
      "     |      Calls the given function for each character of the string\n",
      "     |  \n",
      "     |  lowercase(self)\n",
      "     |      Lowercase the string\n",
      "     |  \n",
      "     |  lstrip(self)\n",
      "     |      Strip the left of the string\n",
      "     |  \n",
      "     |  map(self, func)\n",
      "     |      Calls the given function for each character of the string\n",
      "     |      \n",
      "     |      Replaces each character of the string using the returned value. Each\n",
      "     |      returned value **must** be a str of length 1 (ie a character).\n",
      "     |  \n",
      "     |  nfc(self)\n",
      "     |      Runs the NFC normalization\n",
      "     |  \n",
      "     |  nfd(self)\n",
      "     |      Runs the NFD normalization\n",
      "     |  \n",
      "     |  nfkc(self)\n",
      "     |      Runs the NFKC normalization\n",
      "     |  \n",
      "     |  nfkd(self)\n",
      "     |      Runs the NFKD normalization\n",
      "     |  \n",
      "     |  prepend(self, s)\n",
      "     |      Prepend the given sequence to the string\n",
      "     |  \n",
      "     |  replace(self, pattern, content)\n",
      "     |      Replace the content of the given pattern with the provided content\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          pattern: Pattern:\n",
      "     |              A pattern used to match the string. Usually a string or a Regex\n",
      "     |      \n",
      "     |          content: str:\n",
      "     |              The content to be used as replacement\n",
      "     |  \n",
      "     |  rstrip(self)\n",
      "     |      Strip the right of the string\n",
      "     |  \n",
      "     |  slice(self, range)\n",
      "     |      Slice the string using the given range\n",
      "     |  \n",
      "     |  split(self, pattern, behavior)\n",
      "     |      Split the NormalizedString using the given pattern and the specified behavior\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          pattern: Pattern:\n",
      "     |              A pattern used to split the string. Usually a string or a Regex\n",
      "     |      \n",
      "     |          behavior: SplitDelimiterBehavior:\n",
      "     |              The behavior to use when splitting.\n",
      "     |              Choices: \"removed\", \"isolated\", \"merged_with_previous\", \"merged_with_next\",\n",
      "     |              \"contiguous\"\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of NormalizedString, representing each split\n",
      "     |  \n",
      "     |  strip(self)\n",
      "     |      Strip both ends of the string\n",
      "     |  \n",
      "     |  uppercase(self)\n",
      "     |      Uppercase the string\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  normalized\n",
      "     |      The normalized part of the string\n",
      "     |  \n",
      "     |  original\n",
      "    \n",
      "    class OffsetReferential(enum.Enum)\n",
      "     |  OffsetReferential(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  An enumeration.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OffsetReferential\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  NORMALIZED = <OffsetReferential.NORMALIZED: 'normalized'>\n",
      "     |  \n",
      "     |  ORIGINAL = <OffsetReferential.ORIGINAL: 'original'>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    class OffsetType(enum.Enum)\n",
      "     |  OffsetType(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  An enumeration.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OffsetType\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  BYTE = <OffsetType.BYTE: 'byte'>\n",
      "     |  \n",
      "     |  CHAR = <OffsetType.CHAR: 'char'>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    class PreTokenizedString(builtins.object)\n",
      "     |  PreTokenizedString(self, sequence)\n",
      "     |  \n",
      "     |  PreTokenizedString\n",
      "     |  \n",
      "     |  Wrapper over a string, that provides a way to normalize, pre-tokenize, tokenize the\n",
      "     |  underlying string, while keeping track of the alignment information (offsets).\n",
      "     |  \n",
      "     |  The PreTokenizedString manages what we call `splits`. Each split represents a substring\n",
      "     |  which is a subpart of the original string, with the relevant offsets and tokens.\n",
      "     |  \n",
      "     |  When calling one of the methods used to modify the PreTokenizedString (namely one of\n",
      "     |  `split`, `normalize` or `tokenize), only the `splits` that don't have any associated\n",
      "     |  tokens will get modified.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequence: str:\n",
      "     |          The string sequence used to initialize this PreTokenizedString\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_splits(self, offset_referential='original', offset_type='char')\n",
      "     |      Get the splits currently managed by the PreTokenizedString\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          offset_referential: :obj:`str`\n",
      "     |              Whether the returned splits should have offsets expressed relative\n",
      "     |              to the original string, or the normalized one. choices: \"original\", \"normalized\".\n",
      "     |      \n",
      "     |          offset_type: :obj:`str`\n",
      "     |              Whether the returned splits should have offsets expressed in bytes or chars.\n",
      "     |              When slicing an str, we usually want to use chars, which is the default value.\n",
      "     |              Now in some cases it might be interesting to get these offsets expressed in bytes,\n",
      "     |              so it is possible to change this here.\n",
      "     |              choices: \"char\", \"bytes\"\n",
      "     |      \n",
      "     |      Returns\n",
      "     |          A list of splits\n",
      "     |  \n",
      "     |  normalize(self, func)\n",
      "     |      Normalize each split of the `PreTokenizedString` using the given `func`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          func: Callable[[NormalizedString], None]:\n",
      "     |              The function used to normalize each underlying split. This function\n",
      "     |              does not need to return anything, just calling the methods on the provided\n",
      "     |              NormalizedString allow its modification.\n",
      "     |  \n",
      "     |  split(self, func)\n",
      "     |      Split the PreTokenizedString using the given `func`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          func: Callable[[index, NormalizedString], List[NormalizedString]]:\n",
      "     |              The function used to split each underlying split.\n",
      "     |              It is expected to return a list of `NormalizedString`, that represent the new\n",
      "     |              splits. If the given `NormalizedString` does not need any splitting, we can\n",
      "     |              just return it directly.\n",
      "     |              In order for the offsets to be tracked accurately, any returned `NormalizedString`\n",
      "     |              should come from calling either `.split` or `.slice` on the received one.\n",
      "     |  \n",
      "     |  to_encoding(self, type_id=0, word_idx=None)\n",
      "     |      Return an Encoding generated from this PreTokenizedString\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          type_id: int = 0:\n",
      "     |              The type_id to be used on the generated Encoding.\n",
      "     |      \n",
      "     |          word_idx: Optional[int] = None:\n",
      "     |              An optional word index to be used for each token of this Encoding. If provided,\n",
      "     |              all the word indices in the generated Encoding will use this value, instead\n",
      "     |              of the one automatically tracked during pre-tokenization.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An Encoding\n",
      "     |  \n",
      "     |  tokenize(self, func)\n",
      "     |      Tokenize each split of the `PreTokenizedString` using the given `func`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          func: Callable[[str], List[Token]]:\n",
      "     |              The function used to tokenize each underlying split. This function must return\n",
      "     |              a list of Token generated from the input str.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Regex(builtins.object)\n",
      "     |  Regex(self, pattern)\n",
      "     |  \n",
      "     |  Instantiate a new Regex with the given pattern\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SplitDelimiterBehavior(enum.Enum)\n",
      "     |  SplitDelimiterBehavior(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  An enumeration.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SplitDelimiterBehavior\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CONTIGUOUS = <SplitDelimiterBehavior.CONTIGUOUS: 'contiguous'>\n",
      "     |  \n",
      "     |  ISOLATED = <SplitDelimiterBehavior.ISOLATED: 'isolated'>\n",
      "     |  \n",
      "     |  MERGED_WITH_NEXT = <SplitDelimiterBehavior.MERGED_WITH_NEXT: 'merged_w...\n",
      "     |  \n",
      "     |  MERGED_WITH_PREVIOUS = <SplitDelimiterBehavior.MERGED_WITH_PREVIOUS: '...\n",
      "     |  \n",
      "     |  REMOVED = <SplitDelimiterBehavior.REMOVED: 'removed'>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    class Token(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  as_tuple(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  id\n",
      "     |  \n",
      "     |  offsets\n",
      "     |  \n",
      "     |  value\n",
      "    \n",
      "    class Tokenizer(builtins.object)\n",
      "     |  Tokenizer(self, model)\n",
      "     |  \n",
      "     |  A :obj:`Tokenizer` works as a pipeline. It processes some raw text as input\n",
      "     |  and outputs an :class:`~tokenizers.Encoding`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      model (:class:`~tokenizers.models.Model`):\n",
      "     |          The core algorithm that this :obj:`Tokenizer` should be using.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getnewargs__(...)\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  add_special_tokens(self, tokens)\n",
      "     |      Add the given special tokens to the Tokenizer.\n",
      "     |      \n",
      "     |      If these tokens are already part of the vocabulary, it just let the Tokenizer know about\n",
      "     |      them. If they don't exist, the Tokenizer creates them, giving them a new id.\n",
      "     |      \n",
      "     |      These special tokens will never be processed by the model (ie won't be split into\n",
      "     |      multiple tokens), and they can be removed from the output when decoding.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
      "     |              The list of special tokens we want to add to the vocabulary. Each token can either\n",
      "     |              be a string or an instance of :class:`~tokenizers.AddedToken` for more\n",
      "     |              customization.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The number of tokens that were created in the vocabulary\n",
      "     |  \n",
      "     |  add_tokens(self, tokens)\n",
      "     |      Add the given tokens to the vocabulary\n",
      "     |      \n",
      "     |      The given tokens are added only if they don't already exist in the vocabulary.\n",
      "     |      Each token then gets a new attributed id.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
      "     |              The list of tokens we want to add to the vocabulary. Each token can be either a\n",
      "     |              string or an instance of :class:`~tokenizers.AddedToken` for more customization.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The number of tokens that were created in the vocabulary\n",
      "     |  \n",
      "     |  decode(self, ids, skip_special_tokens=True)\n",
      "     |      Decode the given list of ids back to a string\n",
      "     |      \n",
      "     |      This is used to decode anything coming back from a Language Model\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          ids (A :obj:`List/Tuple` of :obj:`int`):\n",
      "     |              The list of ids that we want to decode\n",
      "     |      \n",
      "     |          skip_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether the special tokens should be removed from the decoded string\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`str`: The decoded string\n",
      "     |  \n",
      "     |  decode_batch(self, sequences, skip_special_tokens=True)\n",
      "     |      Decode a batch of ids back to their corresponding string\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          sequences (:obj:`List` of :obj:`List[int]`):\n",
      "     |              The batch of sequences we want to decode\n",
      "     |      \n",
      "     |          skip_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether the special tokens should be removed from the decoded strings\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`List[str]`: A list of decoded strings\n",
      "     |  \n",
      "     |  enable_padding(self, direction='right', pad_id=0, pad_type_id=0, pad_token='[PAD]', length=None, pad_to_multiple_of=None)\n",
      "     |      Enable the padding\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          direction (:obj:`str`, `optional`, defaults to :obj:`right`):\n",
      "     |              The direction in which to pad. Can be either ``right`` or ``left``\n",
      "     |      \n",
      "     |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      "     |              If specified, the padding length should always snap to the next multiple of the\n",
      "     |              given value. For example if we were going to pad witha length of 250 but\n",
      "     |              ``pad_to_multiple_of=8`` then we will pad to 256.\n",
      "     |      \n",
      "     |          pad_id (:obj:`int`, defaults to 0):\n",
      "     |              The id to be used when padding\n",
      "     |      \n",
      "     |          pad_type_id (:obj:`int`, defaults to 0):\n",
      "     |              The type id to be used when padding\n",
      "     |      \n",
      "     |          pad_token (:obj:`str`, defaults to :obj:`[PAD]`):\n",
      "     |              The pad token to be used when padding\n",
      "     |      \n",
      "     |          length (:obj:`int`, `optional`):\n",
      "     |              If specified, the length at which to pad. If not specified we pad using the size of\n",
      "     |              the longest sequence in a batch.\n",
      "     |  \n",
      "     |  enable_truncation(self, max_length, stride=0, strategy='longest_first')\n",
      "     |      Enable truncation\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          max_length (:obj:`int`):\n",
      "     |              The max length at which to truncate\n",
      "     |      \n",
      "     |          stride (:obj:`int`, `optional`):\n",
      "     |              The length of the previous first sequence to be included in the overflowing\n",
      "     |              sequence\n",
      "     |      \n",
      "     |          strategy (:obj:`str`, `optional`, defaults to :obj:`longest_first`):\n",
      "     |              The strategy used to truncation. Can be one of ``longest_first``, ``only_first`` or\n",
      "     |              ``only_second``.\n",
      "     |  \n",
      "     |  encode(self, sequence, pair=None, is_pretokenized=False, add_special_tokens=True)\n",
      "     |      Encode the given sequence and pair. This method can process raw text sequences\n",
      "     |      as well as already pre-tokenized sequences.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          Here are some examples of the inputs that are accepted::\n",
      "     |      \n",
      "     |              encode(\"A single sequence\")`\n",
      "     |              encode(\"A sequence\", \"And its pair\")`\n",
      "     |              encode([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], is_pretokenized=True)`\n",
      "     |              encode(\n",
      "     |                  [ \"A\", \"pre\", \"tokenized\", \"sequence\" ], [ \"And\", \"its\", \"pair\" ],\n",
      "     |                  is_pretokenized=True\n",
      "     |              )\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          sequence (:obj:`~tokenizers.InputSequence`):\n",
      "     |              The main input sequence we want to encode. This sequence can be either raw\n",
      "     |              text or pre-tokenized, according to the ``is_pretokenized`` argument:\n",
      "     |      \n",
      "     |              - If ``is_pretokenized=False``: :class:`~tokenizers.TextInputSequence`\n",
      "     |              - If ``is_pretokenized=True``: :class:`~tokenizers.PreTokenizedInputSequence`\n",
      "     |      \n",
      "     |          pair (:obj:`~tokenizers.InputSequence`, `optional`):\n",
      "     |              An optional input sequence. The expected format is the same that for ``sequence``.\n",
      "     |      \n",
      "     |          is_pretokenized (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |              Whether the input is already pre-tokenized\n",
      "     |      \n",
      "     |          add_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether to add the special tokens\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Encoding`: The encoded result\n",
      "     |  \n",
      "     |  encode_batch(self, input, is_pretokenized=False, add_special_tokens=True)\n",
      "     |      Encode the given batch of inputs. This method accept both raw text sequences\n",
      "     |      as well as already pre-tokenized sequences.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          Here are some examples of the inputs that are accepted::\n",
      "     |      \n",
      "     |              encode_batch([\n",
      "     |                  \"A single sequence\",\n",
      "     |                  (\"A tuple with a sequence\", \"And its pair\"),\n",
      "     |                  [ \"A\", \"pre\", \"tokenized\", \"sequence\" ],\n",
      "     |                  ([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], \"And its pair\")\n",
      "     |              ])\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input (A :obj:`List`/:obj:`Tuple` of :obj:`~tokenizers.EncodeInput`):\n",
      "     |              A list of single sequences or pair sequences to encode. Each sequence\n",
      "     |              can be either raw text or pre-tokenized, according to the ``is_pretokenized``\n",
      "     |              argument:\n",
      "     |      \n",
      "     |              - If ``is_pretokenized=False``: :class:`~tokenizers.TextEncodeInput`\n",
      "     |              - If ``is_pretokenized=True``: :class:`~tokenizers.PreTokenizedEncodeInput`\n",
      "     |      \n",
      "     |          is_pretokenized (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |              Whether the input is already pre-tokenized\n",
      "     |      \n",
      "     |          add_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether to add the special tokens\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A :obj:`List` of :class:`~tokenizers.Encoding`: The encoded batch\n",
      "     |  \n",
      "     |  get_vocab(self, with_added_tokens=True)\n",
      "     |      Get the underlying vocabulary\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          with_added_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether to include the added tokens\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Dict[str, int]`: The vocabulary\n",
      "     |  \n",
      "     |  get_vocab_size(self, with_added_tokens=True)\n",
      "     |      Get the size of the underlying vocabulary\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          with_added_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      "     |              Whether to include the added tokens\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`int`: The size of the vocabulary\n",
      "     |  \n",
      "     |  id_to_token(self, id)\n",
      "     |      Convert the given id to its corresponding token if it exists\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          id (:obj:`int`):\n",
      "     |              The id to convert\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Optional[str]`: An optional token, :obj:`None` if out of vocabulary\n",
      "     |  \n",
      "     |  no_padding(self)\n",
      "     |      Disable padding\n",
      "     |  \n",
      "     |  no_truncation(self)\n",
      "     |      Disable truncation\n",
      "     |  \n",
      "     |  num_special_tokens_to_add(self, is_pair)\n",
      "     |      Return the number of special tokens that would be added for single/pair sentences.\n",
      "     |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  post_process(self, encoding, pair=None, add_special_tokens=True)\n",
      "     |      Apply all the post-processing steps to the given encodings.\n",
      "     |      \n",
      "     |      The various steps are:\n",
      "     |      \n",
      "     |          1. Truncate according to the set truncation params (provided with\n",
      "     |             :meth:`~tokenizers.Tokenizer.enable_truncation`)\n",
      "     |          2. Apply the :class:`~tokenizers.processors.PostProcessor`\n",
      "     |          3. Pad according to the set padding params (provided with\n",
      "     |             :meth:`~tokenizers.Tokenizer.enable_padding`)\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          encoding (:class:`~tokenizers.Encoding`):\n",
      "     |              The :class:`~tokenizers.Encoding` corresponding to the main sequence.\n",
      "     |      \n",
      "     |          pair (:class:`~tokenizers.Encoding`, `optional`):\n",
      "     |              An optional :class:`~tokenizers.Encoding` corresponding to the pair sequence.\n",
      "     |      \n",
      "     |          add_special_tokens (:obj:`bool`):\n",
      "     |              Whether to add the special tokens\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Encoding`: The final post-processed encoding\n",
      "     |  \n",
      "     |  save(self, pretty=False)\n",
      "     |      Save the :class:`~tokenizers.Tokenizer` to the file at the given path.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          path (:obj:`str`):\n",
      "     |              A path to a file in which to save the serialized tokenizer.\n",
      "     |      \n",
      "     |          pretty (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |              Whether the JSON file should be pretty formatted.\n",
      "     |  \n",
      "     |  to_str(self, pretty=False)\n",
      "     |      Gets a serialized string representing this :class:`~tokenizers.Tokenizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          pretty (:obj:`bool`, defaults to :obj:`False`):\n",
      "     |              Whether the JSON string should be pretty formatted.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`str`: A string representing the serialized Tokenizer\n",
      "     |  \n",
      "     |  token_to_id(self, token)\n",
      "     |      Convert the given token to its corresponding id if it exists\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token (:obj:`str`):\n",
      "     |              The token to convert\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :obj:`Optional[int]`: An optional id, :obj:`None` if out of vocabulary\n",
      "     |  \n",
      "     |  train(self, files, trainer=None)\n",
      "     |      Train the Tokenizer using the given files.\n",
      "     |      \n",
      "     |      Reads the files line by line, while keeping all the whitespace, even new lines.\n",
      "     |      If you want to train from data store in-memory, you can check\n",
      "     |      :meth:`~tokenizers.Tokenizer.train_from_iterator`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          files (:obj:`List[str]`):\n",
      "     |              A list of path to the files that we should use for training\n",
      "     |      \n",
      "     |          trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      "     |              An optional trainer that should be used to train our Model\n",
      "     |  \n",
      "     |  train_from_iterator(self, iterator, trainer=None, length=None)\n",
      "     |      Train the Tokenizer using the provided iterator.\n",
      "     |      \n",
      "     |      You can provide anything that is a Python Iterator\n",
      "     |      \n",
      "     |          * A list of sequences :obj:`List[str]`\n",
      "     |          * A generator that yields :obj:`str` or :obj:`List[str]`\n",
      "     |          * A Numpy array of strings\n",
      "     |          * ...\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          iterator (:obj:`Iterator`):\n",
      "     |              Any iterator over strings or list of strings\n",
      "     |      \n",
      "     |          trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      "     |              An optional trainer that should be used to train our Model\n",
      "     |      \n",
      "     |          length (:obj:`int`, `optional`):\n",
      "     |              The total number of sequences in the iterator. This is used to\n",
      "     |              provide meaningful progress tracking\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  from_buffer(buffer)\n",
      "     |      Instantiate a new :class:`~tokenizers.Tokenizer` from the given buffer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          buffer (:obj:`bytes`):\n",
      "     |              A buffer containing a previously serialized :class:`~tokenizers.Tokenizer`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      "     |  \n",
      "     |  from_file(path)\n",
      "     |      Instantiate a new :class:`~tokenizers.Tokenizer` from the file at the given path.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          path (:obj:`str`):\n",
      "     |              A path to a local JSON file representing a previously serialized\n",
      "     |              :class:`~tokenizers.Tokenizer`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      "     |  \n",
      "     |  from_str(json)\n",
      "     |      Instantiate a new :class:`~tokenizers.Tokenizer` from the given JSON string.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          json (:obj:`str`):\n",
      "     |              A valid JSON string representing a previously serialized\n",
      "     |              :class:`~tokenizers.Tokenizer`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  decoder\n",
      "     |      The `optional` :class:`~tokenizers.decoders.Decoder` in use by the Tokenizer\n",
      "     |  \n",
      "     |  model\n",
      "     |      The :class:`~tokenizers.models.Model` in use by the Tokenizer\n",
      "     |  \n",
      "     |  normalizer\n",
      "     |      The `optional` :class:`~tokenizers.normalizers.Normalizer` in use by the Tokenizer\n",
      "     |  \n",
      "     |  padding\n",
      "     |      Get the current padding parameters\n",
      "     |      \n",
      "     |      `Cannot be set, use` :meth:`~tokenizers.Tokenizer.enable_padding` `instead`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:obj:`dict`, `optional`):\n",
      "     |              A dict with the current padding parameters if padding is enabled\n",
      "     |  \n",
      "     |  post_processor\n",
      "     |      The `optional` :class:`~tokenizers.processors.PostProcessor` in use by the Tokenizer\n",
      "     |  \n",
      "     |  pre_tokenizer\n",
      "     |      The `optional` :class:`~tokenizers.pre_tokenizers.PreTokenizer` in use by the Tokenizer\n",
      "     |  \n",
      "     |  truncation\n",
      "     |      Get the currently set truncation parameters\n",
      "     |      \n",
      "     |      `Cannot set, use` :meth:`~tokenizers.Tokenizer.enable_truncation` `instead`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:obj:`dict`, `optional`):\n",
      "     |              A dict with the current truncation parameters if truncation is enabled\n",
      "\n",
      "DATA\n",
      "    EncodeInput = typing.Union[str, typing.Tuple[str, str], typing...ping....\n",
      "    InputSequence = typing.Union[str, typing.List[str], typing.Tuple[str]]\n",
      "    List = typing.List\n",
      "    Offsets = typing.Tuple[int, int]\n",
      "    PreTokenizedEncodeInput = typing.Union[typing.List[str], typing.Tuple[...\n",
      "    PreTokenizedInputSequence = typing.Union[typing.List[str], typing.Tupl...\n",
      "    TextEncodeInput = typing.Union[str, typing.Tuple[str, str], typing.Lis...\n",
      "    Tuple = typing.Tuple\n",
      "    Union = typing.Union\n",
      "\n",
      "VERSION\n",
      "    0.10.2\n",
      "\n",
      "FILE\n",
      "    /home/guntis/anaconda3/envs/tw131/lib/python3.8/site-packages/tokenizers/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-multiple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tw131': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd07c8357969da571df80afc132a07f2ae2c06ea8e29ade683f4578cc0e429fe076"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
