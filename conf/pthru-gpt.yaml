# @package _global_
# @package _global_

defaults:
# could specify default config groups / values in external dirs or files listed here
# - groupname: filename
# - def_vals_filename
  - _self_
  - experiment: null
  - overrides: null   # can select (on cmd line) from config files in conf/overrides/ to override values

cwd_path: "."   # gets expanded to an absolute path to the original current directory (not the Hydra/runtime working dir)

train_ftwc: true
use_wandb: false
use_framework: mingpt   # mingpt, xf (xFormers), hf (huggingface), lml (labml)

resume_from_checkpoint: null  #/home/guntis/work/0_magd3/tw13/minGPT/saved_models/dec18-startofepoch2.ckpt

gpus:  1 # a single int n to auto-select n gpus from those available, or else a list of gpu indices e.g. [0,1,2,3]
#  - 0  # yaml format for a list
#  - 1
#  - 2
#  - 3

general:
  random_seed: 42

data:
  dataset_dir: /ssd2tb/twdata/combined
  # data_file: ${cwd_path}/tw-cooking-recipe3+take3+cut+go6-Z7L8CvEPsO53iKDg.pthru  #
  data_file: ${cwd_path}/mingpt-training-all.pthru   #gata-train-all.pthru  #
  val_file:  ${cwd_path}/mingpt-valid-all.pthru  # gata|mingpt-valid-all.pthru
  test_file: ${cwd_path}/mingpt-test-all.pthru # ${cwd_path}/gata-test-all.pthru
  tokenizer_file: ${.dataset_dir}/../ftwc/ftwc_tokenizer_new.json   #ftwc_tokenizer_new.json
  num_workers: 0  # > 0 when using ddp and multiple gpus
  train_filtering: cmd_prompts  # cmd_tokens, cmd_prompts
  eval_filtering: cmd_prompts  #${data.train_filtering}
  ignore_kg: false

model:
  arch: gpt
  block_size: 128  # aka seq_len or 'attention window size'
  attention_type: scaled_dot_product  # for xFormers (xf) framework
  n_layers: 7
  n_heads: 8
  d_embd: 512
  hidden_layer_multiplier: 4          # used directly by xFormers framework
  d_ff: null   # 2048 = ${.hidden_layer_multiplier}*${.d_embd}  # FF layer of transformer block
  dropout: 0.1
  embd_pdrop: ${.dropout}
  resid_pdrop: ${.dropout}
  attn_pdrop: ${.dropout}
  mlp_pdrop: ${.dropout}      # used by xFormers framework
  vocab_size: 0     # gets filled in after we load the vocabulary
#  dt_type: reward_conditioned    # Decision Transformer: reward_conditioned or naive
#  max_timestep: None    # Decision Transformer: max num steps for an episode (for global pos embeddings)

trainer:
  batch_size: 192
  max_epochs: 3
  learning_rate: 6e-4
  val_check_interval: 0.5
  patience: 25
  save_top_k: 7
  limit_val_batches: 10000  # this should cover all of our current validation set (but maybe not the test set?)
  betas:
    - 0.9
    - 0.95
  grad_norm_clip: 1.0
  lr_decay: true
  weight_decay: 0.1 # only applied on matmul weights
  warmup_tokens: 20480  #10240 = 512*20
  decay_multiplier: 8
  decay_tokens: null #{trainer.decay_multiplier}*{model.block_size}*${len(dataset)}  # set by caller after loading training data
  eval_predict: true

eval:
  chkpt_dir: null
  chkpt_glob: null
  results_dir: null
#  checkpoint: ${cwd_path}/saved_models/instrgpt-epoch=4-step=1314-train_loss_step=0.125-val_loss=0.231.ckpt
#  checkpoint: ${cwd_path}/saved_models/12-31/mingpt-epoch=1-step=1469-train_loss_step=0.197-val_loss=0.662.ckpt  # 84.2 % (win 27/124)
#  checkpoint: ${cwd_path}/saved_models/2021-01-04_16-24-29/instrgpt-epoch=4-step=1655-train_loss_step=0.129-val_loss=0.258.ckpt  #
#  checkpoint: ${cwd_path}/saved_models/2022-05-11/mingpt:gpt:pthru-epoch=1-step=331-cmd_acc=0.669-val_loss=0.772.ckpt
  checkpoint: ${cwd_path}/saved_models/2022-05-18/mingpt:gpt:pthru-epoch=2-step=463-val_acc=0.659-val_loss=0.877.ckpt #TOK_ACC = 75.57 CMD_ACC = 68.14

  which_set: valid    # valid or test
  play_games: false
  which_games: gata   # gata or ftwc
  root_dir: /ssd2tb/twdata/${eval.which_games}  # /ssd2tb/ftwc
  pthru_data_dir: ${.root_dir}/playthru_data/${.which_games}_${.which_set}  #mingpt-${eval.which_set}
  games_dir: ${.root_dir}/games_${.which_games}/${.which_set}/
  show_samples: false   # ${trainer.eval_predict}

wandb:
  proj: tw-${use_framework}-${model.arch}
  experiment: null

