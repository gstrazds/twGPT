# @package _global_
# overrides for main config from config.yaml

gpus:  # set to a single int n to auto-select n gpus from all available, or to a list of gpu indices e.g. [0,1,2,3]
#  - 0  # yaml format for a list
  - 1
#  - 2
#  - 3

data:
  dataset_dir: /work2/gstrazds/twdata/combined
#  tokenizer_file: ${.dataset_dir}/combined_tokenizer.json   #ftwc_tokenizer_new.json
#  num_workers: 0  # > 0 when using ddp and multiple gpus
#  train_filtering: cmd_prompts  # cmd_tokens, cmd_prompts

model:
#  block_size: 128  # aka seq_len or 'attention window size'
  block_size: 384  # spatial extent of the model for its context
#  n_layers: 8
#  n_heads: 8
  d_embd: 256

trainer:
  batch_size: 64  # 192
#  max_epochs: 10
#  learning_rate: 6e-4
#  patience: 20
#  save_top_k: 20
#  val_check_interval: 0.2
#  limit_val_batches: 10000  # this should cover all of our current validation set (but maybe not the test set?)

eval:
  root_dir: /work2/gstrazds/ftwc
  checkpoint: ${cwd_path}/saved_models/instrgpt-epoch=4-step=0-train_loss_step=0.016-val_loss=0.651.ckpt #(91.67%) 90.9%
  # checkpoint: ${cwd_path}/saved_models/instrgpt-22-23-25-ep2-train0.024-val0.574.ckpt # 90.22 %
  #  ${cwd_path}/saved_models/instrgpt-epoch=2-step=0-train_loss_step=0.024-val_loss=0.574.ckpt  # 91.97%

