# @package _global_
# overrides for main config from config.yaml

gpus:  # set to a single int n to auto-select n gpus from all available, or to a list of gpu indices e.g. [0,1,2,3]
  - 0  # yaml format for a list
  - 1
  - 2
  - 3

data:
  dataset_dir: /work2/gstrazds/twdata/alt_combined   # dataset_dir: /work2/gstrazds/twdata/data_combined
#  tokenizer_file: ${.dataset_dir}/combined_tokenizer.json   #ftwc_tokenizer_new.json
#  num_workers: 0  # > 0 when using ddp and multiple gpus
#  train_filtering: cmd_prompts  # cmd_tokens, cmd_prompts
#  max_pthru_steps: 30
  filter_out_skills:  # a list of skill types to exclude from the dataset for training / eval
    - drop
  use_internal_names: true

model:
#  block_size: 128  # aka seq_len or 'attention window size'
  block_size: 1024  # max input sequence length of the model
#  block_size: 1024  # spatial extent of the model for its context
  n_layers: 15
#  n_layers: 8
  n_heads: 8
  d_embd: 512

trainer:
  batch_size: 8
  max_epochs: 20
  learning_rate: 3e-4
  patience: 30
  save_top_k: 25
  val_check_interval: 0.1
#  limit_val_batches: 10000  # this should cover all of our current validation set (but maybe not the test set?)
  decay_multiplier: 5

eval:
  play_games: false
  which_games: gata   # gata or ftwc
  which_set: valid    # valid or test
  root_dir: /work2/gstrazds/twdata/${eval.which_games}  # /ssd2tb/twdata
  ds_filename: gata_${.which_set}  #defaults to which_set (which is correct for ftwc, but not for gata)
  #checkpoint: ${cwd_path}/saved_models/2023-04-10-20-32-0/mingpt:gpt:pthru-epoch=5-step=6374-val_acc=0.844-val_loss=0.227.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-11-X/mingpt:gpt:pthru-epoch=7-step=8182-val_acc=0.674-val_loss=0.220.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-11-X/mingpt:gpt:pthru-epoch=4-step=5205-val_acc=0.660-val_loss=0.212.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-11-20-00-Y/mingpt:gpt:pthru-epoch=4-step=4569-val_acc=0.758-val_loss=0.230.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-13-11-43/mingpt:gpt:pthru-epoch=6-step=6801-val_acc=0.842-val_loss=0.242.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-19-18-10-0/mingpt:gpt:pthru-epoch=4-step=5205-val_acc=0.810-val_loss=0.170.ckpt
  #checkpoint: ${cwd_path}/saved_models/2023-04-20-11-12/mingpt:gpt:pthru-epoch=8-step=8609-val_acc=0.850-val_loss=0.234.ckpt #Apr20b [best-ftwc-test]
  checkpoint: ${cwd_path}/saved_models/2023-04-20-11-12/mingpt:gpt:pthru-epoch=9-step=9990-val_acc=0.851-val_loss=0.250.ckpt #Apr20 [best-so-far]
  #(Apr20c)checkpoint: ${cwd_path}/saved_models/2023-04-20-16-29/mingpt:gpt:pthru-epoch=8-step=9139-val_acc=0.869-val_loss=0.254.ckpt
  #(Apr20c2)checkpoint: ${cwd_path}/saved_models/2023-04-20-16-29/mingpt:gpt:pthru-epoch=8-step=9457-val_acc=0.868-val_loss=0.270.ckpt
  #(Apr20d)checkpoint: ${cwd_path}/saved_models/2023-04-20-22-53/mingpt:gpt:pthru-epoch=5-step=7163-val_acc=0.821-val_loss=0.253.ckpt
