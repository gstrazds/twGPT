{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b074ff",
   "metadata": {
    "id": "varying-atlas"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df576b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "#from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.pre_tokenizers import Punctuation\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, Strip, Replace, Sequence\n",
    "from tokenizers.trainers import UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a3609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b5afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILDE_DATA = './data/tilde'\n",
    "!mkdir -p $TILDE_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4e4faf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALC2-FQxnZ1l",
    "outputId": "3e4d0bbd-8b0d-4b9d-884a-eb88c0d3c556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sacrebleu\r\n",
      "Version: 1.5.1\r\n",
      "Summary: Hassle-free computation of shareable, comparable, and reproducible BLEU, chrF, and TER scores\r\n",
      "Home-page: https://github.com/mjpost/sacrebleu\r\n",
      "Author: Matt Post\r\n",
      "Author-email: post@cs.jhu.edu\r\n",
      "License: Apache License 2.0\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: portalocker\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install sacrebleu\n",
    "!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60714bc0",
   "metadata": {
    "id": "Ex11yfCVaueI"
   },
   "source": [
    "#Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d78f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE_ops=10000 vocab_size=5500 joint_vocab_size=11000\r\n"
     ]
    }
   ],
   "source": [
    "num_bpe_merges = 10000\n",
    "vocab_size = 5500\n",
    "joint_vocab_size = 2*vocab_size\n",
    "\n",
    "!echo BPE_ops=$num_bpe_merges vocab_size=$vocab_size joint_vocab_size=$joint_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c17f07de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTfvBWhIaHTh",
    "outputId": "e75b4122-7e6e-4abc-827a-134aec8cf165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: subword-nmt\r\n",
      "Version: 0.3.7\r\n",
      "Summary: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation\r\n",
      "Home-page: https://github.com/rsennrich/subword-nmt\r\n",
      "Author: Rico Sennrich\r\n",
      "Author-email: None\r\n",
      "License: MIT\r\n",
      "Location: /home/gstrazds/anaconda3/envs/tw131/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install subword-nmt\n",
    "!pip show subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4a6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tilde/combined.lv.tok.txt ./data/tilde/combined.en.tok.txt\r\n"
     ]
    }
   ],
   "source": [
    "TILDE_ALL_EN = f'{TILDE_DATA}/all.norm2.en'\n",
    "TILDE_ALL_LV = f'{TILDE_DATA}/all.norm2.lv'\n",
    "\n",
    "TILDE_TOK_EN = f'{TILDE_DATA}/combined.en.tok.txt'\n",
    "TILDE_TOK_LV = f'{TILDE_DATA}/combined.lv.tok.txt'\n",
    "\n",
    "!echo $TILDE_DATA/combined.lv.tok.txt $TILDE_TOK_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee4a46f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzgEuTRkV4kK",
    "outputId": "6ac1bf6e-9a2a-4cf7-c6cc-e3c76ec72522"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/moses-smt/mosesdecoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d756ba",
   "metadata": {
    "id": "historic-thermal"
   },
   "outputs": [],
   "source": [
    "# # Read texts from previously saved files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172464d1",
   "metadata": {
    "id": "wcc2v_fHe81_"
   },
   "outputs": [],
   "source": [
    "with open(f'{TILDE_DATA}/combined.lv.BPE.txt', 'r') as f:\n",
    "    text_input = f.read().splitlines()\n",
    "\n",
    "with open(f'{TILDE_DATA}/combined.en.BPE.txt', 'r') as f:\n",
    "    text_output = f.read().splitlines()\n",
    "    \n",
    "\n",
    "with open('data/tilde/train2.lv', 'r') as f:\n",
    "    train_input = f.read().splitlines()\n",
    "\n",
    "with open('data/tilde/test2.lv', 'r') as f:\n",
    "    test_input = f.read().splitlines()\n",
    "\n",
    "with open('data/tilde/valid2.lv', 'r') as f:\n",
    "    valid_input = f.read().splitlines()\n",
    "\n",
    "with open('data/tilde/train2.en', 'r') as f:\n",
    "    train_output = f.read().splitlines()\n",
    "\n",
    "with open('data/tilde/test2.en', 'r') as f:\n",
    "    test_output = f.read().splitlines()\n",
    "\n",
    "with open('data/tilde/valid2.en', 'r') as f:\n",
    "    valid_output = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f111906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\t 1613611 1613611\n",
      "train:\t 1210208 1210208\n",
      "valid:\t  161361  161361\n",
      "test:\t  242042  242042\n"
     ]
    }
   ],
   "source": [
    "print(\"total:\\t\", len(text_input), len(text_output))\n",
    "print(f\"train:\\t {len(train_input)} {len(train_output)}\")\n",
    "print(f\"valid:\\t  {len(valid_input)}  {len(valid_output)}\")\n",
    "print(f\"test:\\t  {len(test_input)}  {len(test_output)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dbe1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999 99999\n"
     ]
    }
   ],
   "source": [
    "# _100k = 100000\n",
    "# test_inp_100k = test_input[:_100k-1]\n",
    "# test_outp_100k = test_output[:_100k-1]\n",
    "# print(len(test_inp_100k), len(test_outp_100k))\n",
    "\n",
    "# with open(\"data/tilde/test2_100000.lv\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(test_inp_100k)+\"\\n\")\n",
    "# with open(\"data/tilde/test2_100000.en\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(test_outp_100k)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b432b1dd",
   "metadata": {
    "id": "30_qUBkgSNG1"
   },
   "outputs": [],
   "source": [
    "def build_vocab(freq_file):\n",
    "    vocab = Counter()  #['<unk>', '<pad>', '<eos>'])\n",
    "    with open(freq_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            token, num_occurs = line.split()\n",
    "            vocab[token] += int(num_occurs)\n",
    "    return vocab #[:vocab_size]\n",
    "\n",
    "en_vocab = build_vocab(f'{TILDE_DATA}/bpe/vocab.en')\n",
    "lv_vocab = build_vocab(f'{TILDE_DATA}/bpe/vocab.lv')\n",
    "joint_vocab = build_vocab(f'{TILDE_DATA}/bpe/vocab.lven')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2448ce",
   "metadata": {
    "id": "CtzY4LjYbSiJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab: 10099 lv_vocab: 6477 joint_vocab 10519\n"
     ]
    }
   ],
   "source": [
    "print(\"en_vocab:\", len(en_vocab), \"lv_vocab:\", len(lv_vocab), \"joint_vocab\", len(joint_vocab))\n",
    "# en_vocab: 10099 lv_vocab: 6477 joint_vocab 10519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b364b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    special_tokens = ['<unk>', '<pad>', '<eos>', '<sep>'] #, '<S>', '</S>', '<bos>', '<eos>', '<sep>', '<NONE>', '<|>']\n",
    "                  \n",
    "    normalizer = normalizers.Sequence([Strip(), Lowercase()])\n",
    "    pre_tokenizer = Whitespace()\n",
    "\n",
    "    model = tokenizers.models.WordLevel(unk_token='<unk>')\n",
    "    # model = tokenizers.models.WordPiece()\n",
    "    tokenizer = tokenizers.Tokenizer(model=model)\n",
    "\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"test/*.pthru\"))\n",
    "    # filelist.extend( glob.glob(PTHRU_DIR+\"train/*.pthru\"))\n",
    "\n",
    "\n",
    "    # token_strs = [tok for (tok, span) in pre_tokenizer.pre_tokenize_str(str1)]\n",
    "    # print(token_strs)\n",
    "\n",
    "    # filelist = glob.glob(PTHRU_DIR+\"valid/*.pthru\")\n",
    "\n",
    "    filelist = glob.glob(f\"{TILDE_DATA}/combined.*.BPE.txt\")\n",
    "\n",
    "    filelist = sorted(filelist)\n",
    "    print(len(filelist), filelist[:10])\n",
    "\n",
    "\n",
    "    # unigram_trainer = tokenizers.trainers.UnigramTrainer()\n",
    "    # trainer = tokenizers.trainers.WordPieceTrainer(vocab_size=vocab_size)\n",
    "    trainer = tokenizers.trainers.WordLevelTrainer(vocab_size=joint_vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer.train(files=filelist, trainer=trainer)\n",
    "\n",
    "    vocab_dict = tokenizer.get_vocab(with_added_tokens=False)\n",
    "    print(\"ACTUAL VOCAB SIZE =\", len(vocab_dict))\n",
    "    print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad811e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL VOCAB SIZE = 9048  #? Why is this not == len(joint_vocab) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af7fa9",
   "metadata": {
    "id": "PAHdOeNhYJ8v"
   },
   "source": [
    "#MinGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e378e9",
   "metadata": {
    "id": "OcFt-zMBfqDt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def calculate_attention_token(attention, top_k, model):\n",
    "    logits = model.head(attention)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "    ix = torch.multinomial(probs, num_samples=top_k)\n",
    "\n",
    "    return ix[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None,\n",
    "           output_attention=False, stop_tokidx=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    attention_state = [[] for _ in model.blocks]\n",
    "\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "        if output_attention:\n",
    "            b, t = x.size()\n",
    "\n",
    "            for block_id in range(len(model.blocks)):\n",
    "                att = model.blocks[block_id].attn.att\n",
    "                attention_state[block_id].append(att)\n",
    "\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if stop_tokidx is not None and ix == stop_tokidx:\n",
    "            break\n",
    "\n",
    "    if output_attention:\n",
    "        return x, attention_state\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d6e11b0",
   "metadata": {
    "id": "cZGEd6UCfeou"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT model:\n",
    "- the initial stem consists of a combination of token encoding and a positional encoding\n",
    "- the meat of it is a uniform sequence of Transformer blocks\n",
    "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
    "    - all blocks feed into a central residual pathway similar to resnets\n",
    "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.att = None\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "\n",
    "        self.att = att\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74cffa89",
   "metadata": {
    "id": "0s5SO83OXVK6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import sacrebleu\n",
    "import math\n",
    "import logging\n",
    "from random import choice\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, valid_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self, postfix=''):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        checkpoint_path = self.config.ckpt_path + postfix + '.pt'\n",
    "        logger.info(\"saving %s\", checkpoint_path)\n",
    "        torch.save(raw_model.state_dict(), checkpoint_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset\n",
    "            if split == 'test':\n",
    "                data = self.test_dataset\n",
    "            elif split == 'valid':\n",
    "                data = self.valid_dataset\n",
    "                model.eval()\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size, # if is_train else 8,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "#             predicted_tokids = None\n",
    "            context_list = []\n",
    "            translation_results = []\n",
    "            eval_results = []\n",
    "            x_total = None\n",
    "            y_total = None\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    if split == 'valid':\n",
    "                        intent = (x == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True) #[0]\n",
    "                        #print(valid_dataset.tokenizer_input.encode(['<eos>']))\n",
    "                        #print(intent)\n",
    "                        #print(x.shape, y.shape, logits.shape)\n",
    "                        #for i in range(len(intent[0])):\n",
    "                        #    print(x[i][intent[1][i]], end=\", \")\n",
    "                        #print()\n",
    "\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        #print(probs.shape)\n",
    "                        for i in range(len(probs)):\n",
    "                            # sample from the distribution or take the most likely\n",
    "                            _, predicted = torch.topk(probs[i], k=1, dim=-1)\n",
    "                            if len(predicted.shape) > 1:\n",
    "                                # print(\"PREDICTED:\", predicted.shape, predicted)\n",
    "                                predicted = predicted.squeeze()\n",
    "                                if len(predicted.shape) > 1:\n",
    "                                    print(\"AFTER predicted.squeeze(1):\", predicted.shape)\n",
    "                            sep = intent[1][i]\n",
    "                            # print(\"sep=\", sep)\n",
    "                            #print(\"***CONTEXT\")\n",
    "                            context = clean_tokens(data.tokenizer_input.decode(x[i][:sep - 1], True))\n",
    "                            #print(context)\n",
    "                            #print(\"***COMPLETION\")\n",
    "                            completion = clean_tokens(data.tokenizer_output.decode(predicted[sep:], True))\n",
    "                            #print(completion)\n",
    "                            #print(\"***REAL\")\n",
    "                            real = clean_tokens(data.tokenizer_output.decode(y[i][sep:], True))\n",
    "                            #print(real)\n",
    "                            context_list.append(context)\n",
    "                            translation_results.append(completion)\n",
    "                            eval_results.append(real)\n",
    "\n",
    "#                         probs = F.softmax(logits, dim=-1)\n",
    "#                         # sample from the distribution or take the most likely\n",
    "#                         _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "#                         if predicted_tokids is None:\n",
    "#                             predicted_tokids = [predicted]\n",
    "#                             x_total = x\n",
    "#                             y_total = y\n",
    "#                         else:\n",
    "#                             predicted_tokids.append(predicted)\n",
    "#                             x_total = torch.cat((x_total, x), dim=0)\n",
    "#                             y_total = torch.cat((y_total, y), dim=0)\n",
    "                        \n",
    "\n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. mean loss: {float(np.mean(losses)):.5f}. lr {lr:e}\")\n",
    "\n",
    "            if split == 'train':\n",
    "                train_loss = float(np.mean(losses))\n",
    "                print(f\"train loss: {train_loss}\")\n",
    "                return train_loss\n",
    "\n",
    "            if split == 'test':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"test loss: {test_loss}\")\n",
    "                return test_loss\n",
    "\n",
    "            if split == 'valid':\n",
    "                test_loss = float(np.mean(losses))\n",
    "                print(f\"valid loss: {test_loss}\")\n",
    "\n",
    "#                 eval_results = []\n",
    "#                 translation_results = []\n",
    "#                 context_list = []\n",
    "\n",
    "#                 for idx in range(len(logits_total)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     probs = F.softmax(logits_total[idx], dim=-1)\n",
    "#                     # sample from the distribution or take the most likely\n",
    "#                     _, predicted = torch.topk(probs, k=1, dim=-1)\n",
    "#                 for idx in range(len(predicted_tokids)):\n",
    "#                     intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0][0]\n",
    "#                     predicted = predicted_tokids[idx]\n",
    "#                     print(\"***CONTEXT\")\n",
    "#                     context = clean_tokens(data.tokenizer_input.decode(x_total[idx][:intent - 1], True))\n",
    "#                     print(\"***COMPLETION\")\n",
    "#                     completion = clean_tokens(data.tokenizer_output.decode(predicted[intent:], True))\n",
    "#                     print(\"***REAL\")\n",
    "#                     real = clean_tokens(data.tokenizer_output.decode(y_total[idx][intent:], True))\n",
    "\n",
    "#                     context_list.append(context)\n",
    "#                     translation_results.append(completion)\n",
    "#                     eval_results.append(real)\n",
    "                \n",
    "                with open('valid.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "                with open('eval.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(eval_results))\n",
    "\n",
    "                with open('context.txt', 'w') as f:\n",
    "                    f.write(\"\\n\".join(context_list))\n",
    "\n",
    "\n",
    "                !cat valid.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > valid.detok.txt\n",
    "                !cat eval.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > eval.detok.txt\n",
    "                !cat context.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > context.detok.txt\n",
    "\n",
    "                with open('eval.detok.txt', 'r') as f:\n",
    "                    eval_results = [l.strip() for l in f.readlines()]\n",
    "                with open('valid.detok.txt', 'r') as f:\n",
    "                    translation_results = [l.strip() for l in f.readlines()]\n",
    "                with open('context.detok.txt', 'r') as f:\n",
    "                    context_list = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#                 idx = choice(range(len(context_list)))\n",
    "                valid_sentences = ['the driver wore a cap and his face was thin and very tanned.',\n",
    "                                   'outside it was getting dark.',\n",
    "                                   'the two girls were asleep.',\n",
    "                                   'I would like to have had the uniform off although I did not care much about the outward forms.',\n",
    "                                   'I watched the flashes on San Gabriele.',\n",
    "                                   'I asked.',\n",
    "                                   '\"no.']\n",
    "\n",
    "                idx_list = [i for i, sentence in enumerate(eval_results) if sentence in valid_sentences]\n",
    "                \n",
    "                for idx in idx_list:\n",
    "                    print(f'Input:            {context_list[idx]}')\n",
    "                    print(f'Predicted output: {translation_results[idx]}')\n",
    "                    print(f'Real output:      {eval_results[idx]}')\n",
    "                    print('--------------------------------------------------')\n",
    "\n",
    "                refs = [eval_results]\n",
    "                sys = translation_results\n",
    "                bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "                print(f'BLEU: {bleu.score}')\n",
    "                print('##############################################################')\n",
    "\n",
    "                return test_loss, bleu.score\n",
    "\n",
    "        train_loss_list = []\n",
    "        test_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        valid_bleu_list = []\n",
    "        best_loss = float('inf')\n",
    "        best_bleu = 0.0\n",
    "        bleu_score = -1.0\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train')\n",
    "            train_loss_list.append(train_loss)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "                test_loss_list.append(test_loss)\n",
    "\n",
    "            if self.valid_dataset is not None:\n",
    "                valid_loss, bleu_score = run_epoch('valid')\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                valid_bleu_list.append(bleu_score)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            # good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            good_model = self.valid_dataset is None or bleu_score > best_bleu\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                best_bleu = bleu_score\n",
    "                self.save_checkpoint(\"_best\")\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.save_checkpoint(f\"_{epoch}\")\n",
    "\n",
    "            self.save_checkpoint(\"_last\")\n",
    "\n",
    "        return train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59805a",
   "metadata": {
    "id": "jKCnnx4-XTg1"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe123188",
   "metadata": {
    "id": "Ds0Rr1hrbRDq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size, vocab):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = set(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "        if self.vocab_size != vocab_size:\n",
    "            logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
    "        print(f\"Tokenizer vocab_size={vocab_size} len(vocab)={len(self.vocab)}\")\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "    \n",
    "    def tokenize(self, data, block_size):\n",
    "        tokenized_text = data.split()\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if tokenized in self.vocab:\n",
    "                result.append(tokenized)\n",
    "            else:\n",
    "                logger.warn(f\"Tokenizer UNKNOWN TOKEN: |{tokenized}|\")\n",
    "                result.append('<unk>')\n",
    "\n",
    "        # in case the sentence is longer, than block_size, we trim the sentence\n",
    "        return result[:block_size]\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data, clean_paddings=False):\n",
    "        if hasattr(data, \"shape\") and len(data.shape) > 1:\n",
    "            print(data.shape)\n",
    "            print(data)\n",
    "        text = ' '.join([self.itos[int(i)] for i in data if int(i) >= 0])\n",
    "\n",
    "        if not clean_paddings:\n",
    "            return text\n",
    "        return text.replace('<pad>', '').replace('  ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86aff021",
   "metadata": {
    "id": "advised-shelter"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-24751e8d4969>:7: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(f\"Tokenizer len(vocab) != vocab_size: {len(self.vocab)} {vocab_size}\")\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n",
      "Tokenizer len(vocab) != vocab_size: 10519 5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size=5500 len(vocab)=10519\n",
      "Tokenizer vocab_size=5500 len(vocab)=10519\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 10000\n",
    "\n",
    "# vocab_input = None\n",
    "# if os.path.exists('vocab_input.pkl'):\n",
    "#     with open('vocab_input.pkl', 'rb') as f:\n",
    "#         vocab_input = pickle.load(f)\n",
    "        \n",
    "# vocab_output = None\n",
    "# if os.path.exists('vocab_output.pkl'):\n",
    "#     with open('vocab_output.pkl', 'rb') as f:\n",
    "#         vocab_output = pickle.load(f)\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
    "tokenizer_input = Tokenizer(text_input, vocab_size, list(joint_vocab))\n",
    "tokenizer_output = Tokenizer(text_output, vocab_size, list(joint_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28aee3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28 gadus vec@@ s pa@@ vār@@ s atra@@ sts mir@@ is S@@ an@@ f@@ ran@@ c@@ isko liel@@ veik@@ al@@ ā', '28 gadus vec@@ s pa@@ vār@@ s , kurš nesen pār@@ cē@@ l@@ ies uz S@@ an@@ f@@ ran@@ c@@ isko , š@@ on@@ e@@ dēļ tika atra@@ sts mir@@ is kāda liel@@ veik@@ al@@ a kāp@@ ņ@@ u@@ telp@@ ā .']\n",
      "1613611\n"
     ]
    }
   ],
   "source": [
    "print(text_input[:2])\n",
    "print(f\"{len(text_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f6a179",
   "metadata": {
    "id": "floral-ridge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210208 161361 242042\n"
     ]
    }
   ],
   "source": [
    "# def separate_lines(text, train_idxs, valid_idxs, test_idxs):\n",
    "#     text_lines = text.splitlines()\n",
    "#     train_lines = [text_lines[idx] for idx in train_idxs]\n",
    "#     valid_lines = [text_lines[idx] for idx in valid_idxs]\n",
    "#     test_lines = [text_lines[idx] for idx in test_idxs]\n",
    "#     return train_lines, valid_lines, test_lines\n",
    "\n",
    "# train_input, valid_input, test_input = separate_lines(text_input, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "# train_output, valid_output, test_output = separate_lines(text_output, train_idxs, valid_idxs, test_idxs)\n",
    "\n",
    "print(len(train_input), len(valid_input), len(test_input))\n",
    "assert len(train_input) == len(train_output)\n",
    "assert len(valid_input) == len(valid_output)\n",
    "assert len(test_input) == len(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d2e77b9",
   "metadata": {
    "id": "oZUwkrJeb0p4"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, output_text, input_text, tokenizer_output, tokenizer_input, block_size):\n",
    "        self.tokenizer_output = tokenizer_output\n",
    "        self.tokenizer_input = tokenizer_input\n",
    "\n",
    "        self.block_size = block_size * 2 + 1\n",
    "        self.output_text = [tokenizer_output.tokenize(t, block_size) for t in output_text]\n",
    "        self.input_text = [tokenizer_input.tokenize(t, block_size) for t in input_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The idea is to get the input sentence\n",
    "        and translate it to output sentence (sentences could be on any language).\n",
    "\n",
    "        In the init method we already split a sentence into tokens and filled with spaces,\n",
    "        to have an equal sentence size. In this method we just encode the tokens to\n",
    "        ids (a list of numbers), and we're trying to map ids sequences\n",
    "        \"\"\"\n",
    "\n",
    "        tokenized_input_text = self.tokenizer_input.encode(self.input_text[idx])\n",
    "        tokenized_output_text = self.tokenizer_output.encode(self.output_text[idx])\n",
    "\n",
    "        dix = tokenized_input_text + self.tokenizer_output.encode(['<eos>']) + tokenized_output_text\n",
    "        if len(dix) < self.block_size:\n",
    "            dix += self.tokenizer_output.encode(['<pad>']) * (self.block_size - len(dix))\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        y[:len(tokenized_input_text) - 1] = -100\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f349d43",
   "metadata": {
    "id": "fitted-resident"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ encode Datasets - Start time: 2021-07-22 09:55:49.742884\n",
      "================ encode Datasets - Finished : 2021-07-22 09:56:31.426362 -- elapsed: 0:00:41.683478\n"
     ]
    }
   ],
   "source": [
    "block_size = 100  # the estimate how long lines the text could be (token count)\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Start time: {start_time}\")\n",
    "\n",
    "# for faster debuging of Out of Memory during validation\n",
    "_train_limit = len(train_output)   # 10000  # len(train_output)\n",
    "_eval_limit = 10000   # -1    # 5000\n",
    "\n",
    "train_dataset = WordDataset(train_output[:_train_limit], train_input[:_train_limit],\n",
    "                            tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "if _eval_limit > 0:\n",
    "    test_dataset = WordDataset(test_output[:_eval_limit], test_input[:_eval_limit],\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output[:_eval_limit], valid_input[:_eval_limit],\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "else:\n",
    "    test_dataset = WordDataset(test_output, test_input,\n",
    "                               tokenizer_output, tokenizer_input, block_size)\n",
    "    valid_dataset = WordDataset(valid_output, valid_input,\n",
    "                                tokenizer_output, tokenizer_input, block_size)\n",
    "\n",
    "finish_time = datetime.datetime.now()\n",
    "print(f\"================ encode Datasets - Finished : {finish_time} -- elapsed: {finish_time-start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "778e1f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1210208"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: fixed, no longer shows UNKNOWN TOKEN\n",
    "\n",
    "# joint_vocab -s 10000\n",
    "# UNKNOWN TOKEN\n",
    "\n",
    "# |;@@| (2040)  # I &@@ apos@@ ;@@ m\n",
    "# |q@@| (148)\n",
    "# |R| (40)\n",
    "# |v| (409)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "467c7f6b",
   "metadata": {
    "id": "ruled-astronomy"
   },
   "outputs": [],
   "source": [
    "number_of_heads = 8\n",
    "number_of_layers = 6\n",
    "\n",
    "# from mingpt.model import GPT, GPTConfig\n",
    "embd_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "\n",
    "max_vocab = max(tokenizer_input.vocab_size, tokenizer_output.vocab_size)\n",
    "mconf = GPTConfig(max_vocab, train_dataset.block_size,\n",
    "                  n_layer=number_of_layers, n_head=number_of_heads, n_embd=512,\n",
    "                  embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop)\n",
    "\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f4bd055",
   "metadata": {
    "id": "bUgb5MxODXBc"
   },
   "outputs": [],
   "source": [
    "# from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# tokens_per_epoch = len(train_dataset) * block_size\n",
    "# train_epochs = 100\n",
    "# _batch_size = 128\n",
    "\n",
    "# # initialize a trainer instance and kick off training\n",
    "# tconf = TrainerConfig(max_epochs=train_epochs, \n",
    "#                       batch_size=_batch_size, learning_rate=3e-4,\n",
    "#                       lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "#                       ckpt_path='minGPT-Tilde-LV-EN-translator_model',\n",
    "#                       num_workers=1, weight_decay=0.0001, betas=(0.9, 0.98))\n",
    "# trainer = Trainer(model, train_dataset, test_dataset, valid_dataset, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83caae79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPH3nCe9JB63",
    "outputId": "382f135d-e133-4e0b-9bb5-2f7b01269cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters count: 29789696\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(f'Parameters count: {param_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18199e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters count: 29789696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0177ca95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG17yBsSf3uc",
    "outputId": "ca906c3a-1793-4ab9-84c3-3f6c5f59c37d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71598830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "EHFUYpv8P9lr",
    "outputId": "77da5c3a-eeaa-4132-abe1-2de95323a3e7"
   },
   "outputs": [],
   "source": [
    "# epochs = range(len(test_loss_list))\n",
    "# # plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "# fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(20, 10))\n",
    "# axs[0].plot(epochs, train_loss_list)\n",
    "# axs[0].set_title('Train loss')\n",
    "# axs[0].set_xlabel('Epochs')\n",
    "# axs[0].set_ylabel('Loss')\n",
    "\n",
    "# axs[0].plot(epochs, test_loss_list)\n",
    "# axs[0].set_title('Test loss')\n",
    "# axs[0].set_xlabel('Epochs')\n",
    "# axs[0].set_ylabel('Loss')\n",
    "\n",
    "# axs[1].plot(epochs, valid_loss_list)\n",
    "# axs[1].set_title('Validation loss')\n",
    "# axs[1].set_xlabel('Epochs')\n",
    "# axs[1].set_ylabel('Loss')\n",
    "\n",
    "# axs[2].plot(epochs, valid_bleu_list)\n",
    "# axs[2].set_title('Validation BLEU')\n",
    "# axs[2].set_xlabel('Epochs')\n",
    "# axs[2].set_ylabel('BLEU')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158bcc9",
   "metadata": {
    "id": "1Og-0W-MtKoM"
   },
   "source": [
    "#Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "631c00db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d029ed9c-9170-435a-805f-416774999287",
    "outputId": "78ebc7df-beec-45fa-eddf-cc1829c07374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda1 = torch.device('cuda:1')\n",
    "checkpoint = torch.load('minGPT-Tilde-LV-EN-translator_model_best.pt', map_location=cuda1)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e053ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (tok_emb): Embedding(10519, 512)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=512, out_features=10519, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(10519, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=10519, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(model)\n",
    "model.to(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "447bf188",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "greek-travel",
    "outputId": "8b5ee427-7a71-46ab-e62c-a8730ac37bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:            ar cilvēktiesībām saistītās ES darbības starptautisk@@ os for@@ umos\n",
      "\n",
      "Predicted output: EU actions in international for@@ a related to human rights \n",
      "\n",
      "Real output:      EU activities in the area of human rights in international for@@ a\n",
      "--------------------------------------------------\n",
      "Input:            lai gan ES ir vad@@ ošā loma vairākās pētniecības jomās , tā joprojām pilnībā ne@@ izmanto savu pētniecības un attīstības potenciālu , ne@@ īsteno programmu , kas tehnoloģiju no@@ dotu tālāk un veid@@ otu Eiropas krit@@ isko ma@@ su , kura ir nepieciešama , lai konkur@@ ētu pasaules mērogā un veicinātu jaunu fir@@ mas at@@ z@@ ar@@ u ra@@ šanos , kas uzlab@@ os tehnoloģijas / inovāciju izplat@@ ību .\n",
      "\n",
      "Predicted output: while the EU has a leading role in a number of research fields , it is still not making full use of its R &amp; D potential , is fail@@ ing to implement a programme that will technology transfer and build up a European critical mass that is necessary to compet@@ e global@@ ly and to encourage the emer@@ gence of new companies that will improve the diff@@ us@@ ion of technology / innovation . \n",
      "\n",
      "Real output:      the EU , while a lead@@ er in many research fields , is far from fully explo@@ iting the economic potential of its R &amp; D , the implementation of such technology transfer programme would allow to build the European critical mass necessary to compet@@ e on the world stage and contribute to the creation of sp@@ in @-@ off@@ s which will improve the diff@@ us@@ ion of technology / innovation .\n",
      "--------------------------------------------------\n",
      "Input:            piemēram , joprojām ir šaub@@ as par iespējam@@ iem interešu konflik@@ tiem saistībā ar ē@@ st@@ grib@@ as maz@@ ināšanas līdzek@@ li ben@@ fl@@ u@@ o@@ re@@ k@@ su , aģentūras iepirkuma procedūras un kritēriji person@@ āla pieņem@@ šanai darbā arī ir izrais@@ ījuši krit@@ iku .\n",
      "\n",
      "Predicted output: for example , there are still doub@@ ts about the possible conflicts of interest in the pet@@ ro@@ l ben@@ fl@@ u@@ o@@ re@@ x , procurement procedures and criteria for re@@ cru@@ iting staff have also been the subject of critic@@ ism . \n",
      "\n",
      "Real output:      for example , doub@@ ts still remain about possible conflicts of interest in relation to the assessment of the ap@@ pet@@ ite sup@@ pres@@ s@@ ant ben@@ fl@@ u@@ o@@ re@@ x . the agency &apos;s procurement procedures and criteria for re@@ cru@@ iting staff have also attra@@ ct@@ ed critic@@ ism .\n",
      "--------------------------------------------------\n",
      "Input:            pauž bažas par Komisijas nolū@@ ku izstrādāt z@@ aļ@@ ās grāmat@@ as projektu par darba tiesību attīstību , lai tās &quot; vienkārš@@ otu un moderniz@@ ētu &quot; ; uzskata , ka atbilstoš@@ āk būtu , ja z@@ aļ@@ ajā grāmat@@ ā lielāku uzmanību pievēr@@ stu tam , kā labāk aizsargāt strādāj@@ ošo tiesības un novērst darba līgumu dažād@@ ības ( piemēram , darba līgums uz projekta laiku , ar nosacījumiem , pēc vienošanās un uz noteiktu laiku ) ;\n",
      "\n",
      "Predicted output: is concerned about the Commission &apos;s intention to draft a Green Paper on the development of labour law in order to &apos; simpl@@ ify and modern@@ ise &apos; ; considers that a more appropriate approach would be to focus the Green Paper on how to better protect workers &apos; rights and avoid the diversity of employment contracts ( e.g. on the tim@@ ing of the project , on condit@@ ion@@ ality , on the agreement and on a temporary basis ) ; \n",
      "\n",
      "Real output:      expresses its concern about the Commission &apos;s intention to draft a Green Paper on the e@@ vol@@ ution of labour law aim@@ ing at its &quot; simplification and modern@@ isation &quot; ; considers it more appropriate that the Green Paper should focus on how better to protect the rights of workers and to remed@@ y the de@@ standar@@ dis@@ ation of employment contracts ( e.g. project , cont@@ ing@@ ent , pre@@ car@@ ious , f@@ ixed @-@ term employment ) ;\n",
      "--------------------------------------------------\n",
      "Input:            pauž nopietnas bažas par humānās , ekonomiskās un finansi@@ ālās situācijas paslikt@@ inā@@ šanos Rietum@@ kra@@ stā un Gaz@@ ā ,\n",
      "\n",
      "Predicted output: expresses its de@@ ep concern about the deter@@ io@@ r@@ ation of the humanitarian , economic and financial situation in the W@@ est Bank and Gaz@@ a , \n",
      "\n",
      "Real output:      expresses its serious concern over the deter@@ io@@ r@@ ation of the humanitarian , economic and financial situation in the W@@ est Bank and Gaz@@ a ;\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "pad_tokidx = tokenizer_output.encode(['<pad>'])[0]\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = choice(range(len(valid_output)))\n",
    "\n",
    "    context = valid_input[idx]\n",
    "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(cuda1) #trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10,\n",
    "               stop_tokidx=pad_tokidx)[0]\n",
    "\n",
    "    intent = len(encoded_input) + 1\n",
    "\n",
    "    predicted = y[intent:]\n",
    "    completion = tokenizer_output.decode(predicted, True)\n",
    "    print(f'Input:            {context}')\n",
    "    print(f'\\nPredicted output: {completion}')\n",
    "    print(f'\\nReal output:      {valid_output[idx]}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "776245c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pauž nopietnas bažas par humānās , ekonomiskās un finansi@@ ālās situācijas paslikt@@ inā@@ šanos Rietum@@ kra@@ stā un Gaz@@ ā ,\n",
      "[844, 4070, 1052, 7, 4677, 3, 1426, 4, 1023, 289, 2475, 3674, 142, 313, 2923, 4319, 2508, 4, 4675, 34, 3]\n"
     ]
    }
   ],
   "source": [
    "print(context)\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c4b8458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF3lesDzM_UY",
    "outputId": "8d15a610-69b9-47c9-c296-425e51f35e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:            ja ir zināms , ka analīz@@ es laikā veid@@ ojas mas@@ as z@@ ud@@ ums , rezultātu kor@@ iģ@@ ē ; šim nolū@@ kam norāda korek@@ cijas ko@@ ef@@ icien@@ tus .\n",
      "\n",
      "Predicted output: if a mass loss occur@@ s during the analysis , the result cor@@ respond ; correc@@ tion factors shall be indicated for this purpose . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Real output:      if loss in mass is known to occ@@ ur during the analysis , the result shall be correct@@ ed ; correc@@ tion factors for this purpose are given .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx = choice(range(len(valid_output)))\n",
    "\n",
    "context = valid_input[idx]\n",
    "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(cuda1)\n",
    "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10,\n",
    "                            output_attention=True) #, stop_tokidx=pad_tokidx)\n",
    "\n",
    "intent = len(encoded_input) + 1\n",
    "\n",
    "predicted = y[0][intent:]\n",
    "completion = tokenizer_output.decode(predicted,)\n",
    "print(f'Input:            {context}')\n",
    "print(f'\\nPredicted output: {completion}')\n",
    "print(f'\\nReal output:      {valid_output[idx]}')\n",
    "print('--------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89e0a800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "1uY06xs7n5sx",
    "outputId": "0b2993a4-b919-41a6-e99e-d395cde20c1e"
   },
   "outputs": [],
   "source": [
    "# fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
    "\n",
    "# axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
    "\n",
    "# axis_text.append('<eos>')\n",
    "\n",
    "# axis_text += tokenizer_input.decode(predicted, True).split()\n",
    "\n",
    "# limit = len(axis_text)\n",
    "# for bi in range(number_of_layers):\n",
    "#     for hi in range(number_of_heads):\n",
    "#         attetion_plot = torch.zeros(limit, limit)\n",
    "#         for di in range(limit):\n",
    "#             attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
    "\n",
    "#         ax = plots[bi][hi]\n",
    "#         ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
    "\n",
    "#         # Set up axes\n",
    "#         ax.set_xticklabels([''] + axis_text, rotation=90)\n",
    "#         ax.set_yticklabels([''] + axis_text)\n",
    "\n",
    "#         # Show label at every tick\n",
    "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#         ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#         # Set up a title\n",
    "#         ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4114e93",
   "metadata": {
    "id": "CXFro4HhyE8D"
   },
   "outputs": [],
   "source": [
    "# In case the previous cell is not plotting anything, uncomment the code below and execute. After that, the plotting should be fine.\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# x = np.linspace(0, 10, 100)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(x, np.sin(x), '-')\n",
    "# plt.plot(x, np.cos(x), '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa89101",
   "metadata": {
    "id": "bxvZ1nVstR7j"
   },
   "source": [
    "#Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36755525",
   "metadata": {
    "id": "JlVOSUDaNqaz"
   },
   "outputs": [],
   "source": [
    "def clean_tokens(sentence):\n",
    "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88445afc",
   "metadata": {
    "id": "4VAAvPyR4GMv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [8:51:47<00:00,  3.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import time\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# smooth = SmoothingFunction().method7\n",
    "\n",
    "translation_results = []\n",
    "eval_text = []\n",
    "\n",
    "bleu_results = []\n",
    "\n",
    "num_validation_recs = 100000  #len(valid_input) 161,361 -- would take 55 hours!\n",
    "\n",
    "pad_tokidx = tokenizer_output.encode(['<pad>'])[0]\n",
    "\n",
    "for idx, (context,target) in tqdm(enumerate(zip(test_input, test_output)),\n",
    "                                           total=num_validation_recs):\n",
    "#     sys.stdout.write('\\r'+str(idx)+' / '+str(num_validation_recs))\n",
    "#     time.sleep(0.1)\n",
    "##     if (idx+1) % 50 == 0:\n",
    "##         print(idx, end=\" \")\n",
    "##     if (idx+1) % 100 == 0:\n",
    "##         print(idx)\n",
    "    if (idx+1) % (num_validation_recs+1) == 0:\n",
    "        break\n",
    "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
    "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(cuda1)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10,\n",
    "              stop_tokidx=pad_tokidx)[0]\n",
    "\n",
    "    intent = len(encoded_input) + 1\n",
    "    predicted = y[intent:]\n",
    "    completion = clean_tokens(tokenizer_output.decode(predicted, True))\n",
    "    translation_results.append(completion)\n",
    "    eval_text.append(clean_tokens(target))\n",
    "    # bleu = sentence_bleu([eval], completion, smoothing_function=smooth)\n",
    "    # bleu_results.append(bleu)\n",
    "\n",
    "with open('tilde_valid.predicted', 'w') as f:\n",
    "    f.write(\"\\n\".join(translation_results))\n",
    "\n",
    "with open('tilde_valid.gtruth', 'w') as f:\n",
    "    f.write(\"\\n\".join(eval_text))\n",
    "\n",
    "    # print(f\"Averare BLEU: {np.mean(bleu_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45c6d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_recs = 2000\n",
    "# eval_text = []\n",
    "# for idx, (context,target) in tqdm(enumerate(zip(test_input, test_output)),total=max_recs):\n",
    "#     if (idx+1) % (max_recs+1) == 0:\n",
    "#         break\n",
    "#     eval_text.append(clean_tokens(target))\n",
    "# print(len(eval_text))\n",
    "# with open('tilde_valid.gtruth', 'w') as f:\n",
    "#     f.write(\"\\n\".join(eval_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6710e",
   "metadata": {
    "id": "4xQ1sDwfWS9S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15ad134d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8TILVBwWncc",
    "outputId": "771c5462-601d-4e6e-9c44-d1b99918df5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 43.94, 72.3/50.1/37.8/29.6 (BP=0.980, ratio=0.980, hyp_len=2757389, ref_len=2813764)\r\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\r\n"
     ]
    }
   ],
   "source": [
    "!perl mosesdecoder/scripts/generic/multi-bleu.perl tilde_valid.gtruth < tilde_valid.predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ee7c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU = 7.92, 38.4/12.4/4.2/2.0 (BP=1.000, ratio=1.021, hyp_len=9711, ref_len=9509)\n",
    "# joint_vocab -s 10,000\n",
    "# BLEU = 8.61, 44.4/15.1/5.5/2.8 (BP=0.852, ratio=0.862, hyp_len=8198, ref_len=9509)\n",
    "# full joint_vocab\n",
    "# BLEU = 9.18, 41.7/14.1/5.4/2.8 (BP=0.948, ratio=0.950, hyp_len=9030, ref_len=9509)\n",
    "\n",
    "# model_best.pt\n",
    "# BLEU = 13.47, 48.0/19.6/9.4/5.5 (BP=0.908, ratio=0.912, hyp_len=8670, ref_len=9509)\n",
    "\n",
    "# ---- train_input/output[:2000] (epoch 44, validation BLEU: 48.494189)\n",
    "# ---- 49.96, 75.5/55.5/44.2/36.1 (BP=0.982, ratio=0.983, hyp_len=56089, ref_len=57081)\n",
    "\n",
    "# ---- test_input/output[:100000] (epoch >=44, validation BLEU: ? >=48.49)\n",
    "# ---- 43.82, 72.3/49.9/37.7/29.4 (BP=0.980, ratio=0.980, hyp_len=2757982, ref_len=2813764)\n",
    "\n",
    "# --- Epoch 59, validation BLEU:48.80495715291374\n",
    "# ---> 43.94, 72.3/50.1/37.8/29.6 (BP=0.980, ratio=0.980, hyp_len=2757389, ref_len=2813764)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7d26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3be4c7a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0ccdKVEi_51",
    "outputId": "1325a0f7-bbc4-42ce-87c8-bdec30a82a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n",
      "Warning: No built-in rules for language lv.\n",
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: lv\n"
     ]
    }
   ],
   "source": [
    "!cat tilde_valid.predicted | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.predicted\n",
    "!cat tilde_valid.gtruth | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > tilde_valid.detok.gtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dacc2b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ_G2YbxgdnH",
    "outputId": "16446218-4a72-4072-d04d-fad681783dc5"
   },
   "outputs": [],
   "source": [
    "#!pip install sacrebleu\n",
    "#!pip show sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c2d8cf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Z2dQh7gnq2",
    "outputId": "8a4d34d6-f607-4067-ab6c-bb6abb6d70c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 44.06 72.3/50.1/37.9/29.7 (BP = 0.980 ratio = 0.980 hyp_len = 2762448 ref_len = 2817588)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "with open('tilde_valid.detok.gtruth', 'r') as f:\n",
    "    eval_ref = [l.strip() for l in f.readlines()]\n",
    "with open('tilde_valid.detok.predicted', 'r') as f:\n",
    "    translation_results = [l.strip() for l in f.readlines()]\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(translation_results, [eval_ref])\n",
    "print(bleu) #print(bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a15e5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.918993465381516\n",
    "# joint_vocab -s 10000  8.534786641173136\n",
    "\n",
    "# full joint_vocab 9.174070997058795\n",
    "\n",
    "# model_best.pt \n",
    "#13.481896471451254\n",
    "\n",
    "# ---- train_input/output[:2000]  50.07499125333281\n",
    "# ---- test_input/output[:100000] 43.94 72.2/50.0/37.8/29.5 (BP = 0.980 ratio = 0.981 hyp_len = 2762939 ref_len = 2817588)\n",
    "\n",
    "# --- model_best: Epoch 59, validation BLEU:48.80495715291374\n",
    "#BLEU = 44.06 72.3/50.1/37.9/29.7 (BP = 0.980 ratio = 0.980 hyp_len = 2762448 ref_len = 2817588)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba39f96",
   "metadata": {
    "id": "q8b-0-iFkRRA"
   },
   "source": [
    "#Interactive translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc674c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "with valid Proper packages minGPT EN-LV translator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
